{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI Problem Framing for AI Practitioners","text":"<p>The hardest problems in AI aren't solved with better models or more data. They're solved by asking better questions.</p> <p>Most AI practitioners learn to optimize models, engineer features, and deploy systems. But the most consequential decisions happen earlier: What problem are we actually solving? Is AI the right approach? Which of the dozen possible AI solutions should we pursue? How will we know if we're on the right track?</p> <p>This course teaches you to frame AI problems before you build AI solutions. You'll learn systematic frameworks for deconstructing vague business goals into actionable AI opportunities, evaluating solution alternatives, and diagnosing when to persist, pivot, or stop. These skills separate practitioners who build technically impressive systems from those who deliver measurable business value.</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>By the end of this course, you will be able to:</p> <ul> <li>Analyze ambiguous business problems to identify viable AI opportunities and constraints</li> <li>Evaluate 13 distinct AI solution archetypes to match problems with appropriate approaches</li> <li>Apply The Loop framework to systematically deconstruct outcomes, explore alternatives, and assess trade-offs</li> <li>Interpret weak signals from user feedback, system metrics, and organizational dynamics to diagnose solution health</li> <li>Decide when to persist with current approaches, pivot to alternatives, or stop AI initiatives based on evidence</li> <li>Synthesize complete problem framing analyses for real-world case studies across multiple domains</li> </ul> <p>These skills complement your technical expertise, enabling you to make strategic decisions about what to build before investing months in development.</p>"},{"location":"#course-structure","title":"Course Structure","text":"<p>This course consists of six chapters designed to progressively build your problem framing capabilities:</p>"},{"location":"#chapter-1-the-ai-problem-framing-mindset","title":"Chapter 1: The AI Problem Framing Mindset","text":"<p>Develop the foundational mindset shift from solution-first to problem-first thinking. Learn why most AI projects fail at the framing stage and how to approach problems with intellectual humility, outcome clarity, and constraint awareness.</p>"},{"location":"#chapter-2-ai-solution-alternatives","title":"Chapter 2: AI Solution Alternatives","text":"<p>Master the 13 AI solution archetypes\u2014from retrieval-augmented generation to reinforcement learning from human feedback. Understand when each archetype fits, their trade-offs, and how to avoid premature commitment to familiar solutions.</p>"},{"location":"#chapter-3-the-loop-framework","title":"Chapter 3: The Loop Framework","text":"<p>Learn the systematic Loop framework: Outcome \u2192 Deconstruction \u2192 Alternatives \u2192 Trade-offs \u2192 Signals. Apply this repeatable process to break down complex problems, generate diverse solutions, and establish success criteria before writing code.</p>"},{"location":"#chapter-4-diagnosis-reading-signals","title":"Chapter 4: Diagnosis - Reading Signals","text":"<p>Develop skills in reading weak and strong signals from user behavior, system performance, organizational dynamics, and external trends. Learn to distinguish meaningful feedback from noise and identify early warning signs of solution-problem mismatch.</p>"},{"location":"#chapter-5-pivot-acting-on-signals","title":"Chapter 5: Pivot - Acting on Signals","text":"<p>Make evidence-based decisions about when to persist, pivot, or stop. Learn structured approaches for evaluating pivot opportunities, managing sunk cost fallacies, and communicating strategic shifts to stakeholders.</p>"},{"location":"#chapter-6-application-full-case-studies","title":"Chapter 6: Application - Full Case Studies","text":"<p>Synthesize your learning through complete case studies spanning healthcare, education, finance, and enterprise domains. Practice the full problem framing workflow from initial ambiguity to actionable implementation plans.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>This is a graduate-level course designed for AI/ML practitioners with hands-on experience. Before starting, you should have:</p> <ul> <li>Technical Foundation: Working knowledge of machine learning fundamentals, including supervised learning, neural networks, and model evaluation metrics</li> <li>Implementation Experience: Experience building and deploying at least one AI/ML system in production or research settings</li> <li>Programming Proficiency: Comfort with Python and familiarity with standard ML libraries (scikit-learn, PyTorch, TensorFlow, or similar)</li> <li>Business Context: Basic understanding of how AI systems operate within organizational contexts and deliver business value</li> </ul> <p>This course assumes you can build AI solutions and focuses on helping you decide what to build. If you need to strengthen your technical foundations first, consider completing introductory ML coursework before enrolling.</p>"},{"location":"#how-to-use-this-course","title":"How to Use This Course","text":""},{"location":"#navigation","title":"Navigation","text":"<p>This course is designed for self-paced learning over approximately 40 hours. Each chapter includes:</p> <ul> <li>Conceptual frameworks with real-world examples and failure case studies</li> <li>Interactive exercises to practice applying frameworks to scenarios</li> <li>Self-assessment questions to check your understanding before moving forward</li> <li>Portfolio assignments that build toward your final case study analysis</li> </ul> <p>Use the navigation sidebar to move between chapters. We recommend progressing sequentially, as later chapters build on frameworks introduced earlier.</p>"},{"location":"#portfolio-requirements","title":"Portfolio Requirements","text":"<p>Assessment is portfolio-based rather than exam-based. Throughout the course, you will:</p> <ol> <li>Complete framework application exercises demonstrating your ability to use The Loop, evaluate alternatives, and read signals</li> <li>Submit written analyses of case studies showing systematic problem deconstruction</li> <li>Develop a capstone case study where you apply the full problem framing workflow to a real or realistic AI opportunity</li> </ol> <p>Peer feedback is encouraged but not required. You can work independently or form study groups to critique each other's problem framings.</p>"},{"location":"#self-assessment","title":"Self-Assessment","text":"<p>Each chapter includes self-assessment questions to gauge your readiness to progress. These are designed for reflection, not grading. If you struggle with assessment questions, review the chapter concepts before moving forward.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<p>This course emphasizes independent critical thinking, but you're not alone:</p> <ul> <li>Use the discussion forums to ask clarifying questions or debate alternative framings</li> <li>Consult the additional resources linked in each chapter for deeper dives</li> <li>Review annotated examples showing expert problem framing for similar cases</li> </ul>"},{"location":"#ready-to-begin","title":"Ready to Begin?","text":"<p>Start with Chapter 1: The AI Problem Framing Mindset to develop the foundational thinking patterns that enable effective problem framing.</p> <p>Remember: The goal isn't to become skeptical of AI solutions. It's to become systematic about matching solutions to problems, recognizing when you're on the wrong track, and making strategic pivots before wasting months of effort.</p> <p>The best AI practitioners don't just build things right. They build the right things.</p>"},{"location":"course-description/","title":"Course Description","text":""},{"location":"course-description/#overview","title":"Overview","text":"<p>AI Problem Framing for AI Practitioners is a graduate-level course that teaches the strategic thinking skills needed to make high-stakes decisions about AI projects before committing to implementation. While most AI education focuses on building and optimizing models, this course addresses the upstream question: What should we build in the first place?</p> <p>The course centers on a fundamental tension in applied AI work: practitioners face ambiguous business problems (\"improve customer engagement,\" \"reduce operational costs,\" \"personalize learning\") but must translate these into specific technical implementations. The gap between business ambiguity and technical specificity is where most AI projects fail\u2014not due to poor model performance, but due to solving the wrong problem or choosing inappropriate solution approaches.</p> <p>Through systematic frameworks and real-world case studies, you'll learn to deconstruct vague objectives into actionable opportunities, evaluate diverse AI solution alternatives, establish meaningful success metrics, and diagnose when projects need strategic pivots. This course teaches you to think like a strategic advisor who happens to have deep AI expertise, rather than a technician waiting for well-specified requirements.</p> <p>The curriculum is built around The Loop framework\u2014a repeatable process for moving from outcome clarity through problem deconstruction, alternative generation, trade-off analysis, and signal-based decision making. You'll practice applying this framework to cases spanning healthcare, education, finance, and enterprise operations, building a portfolio that demonstrates your ability to frame problems systematically before writing code.</p>"},{"location":"course-description/#learning-outcomes","title":"Learning Outcomes","text":"<p>After completing this course, you will be able to:</p> <ol> <li>Analyze ambiguous business objectives to extract specific, measurable outcomes that can guide AI solution development</li> <li>Deconstruct complex problems into component parts, identifying constraints, stakeholders, success criteria, and hidden assumptions</li> <li>Generate diverse AI solution alternatives by systematically applying 13 solution archetypes (RAG, RLHF, fine-tuning, prompt engineering, etc.) to problem contexts</li> <li>Evaluate trade-offs between solution alternatives across dimensions of cost, latency, accuracy, interpretability, maintenance burden, and organizational fit</li> <li>Design signal frameworks that enable early detection of solution-problem mismatches through user behavior, system metrics, and qualitative feedback</li> <li>Interpret weak signals from live systems to distinguish meaningful patterns from noise and identify leading indicators of success or failure</li> <li>Decide when to persist with current approaches, pivot to alternative solutions, or stop AI initiatives based on systematic evidence evaluation</li> <li>Justify strategic recommendations about AI initiatives to technical and non-technical stakeholders using structured frameworks</li> <li>Critique existing AI problem framings to identify unstated assumptions, overlooked alternatives, and missing success criteria</li> <li>Synthesize complete problem framing analyses for novel AI opportunities, demonstrating the full Loop workflow from initial ambiguity to actionable implementation plans</li> </ol>"},{"location":"course-description/#target-audience","title":"Target Audience","text":"<p>This course is designed for:</p> <ul> <li>AI/ML practitioners with production experience who want to make better strategic decisions about what to build</li> <li>AI product managers who need frameworks for evaluating proposals and guiding technical teams</li> <li>Technical leads responsible for scoping AI initiatives and allocating engineering resources</li> <li>Graduate students in AI/ML programs preparing for applied research or industry roles</li> <li>Researchers transitioning from academic to applied settings where business context shapes problem definitions</li> </ul> <p>The ideal student has built at least one AI system from conception to deployment and has experienced the frustration of technically successful projects that fail to deliver business value. You should be comfortable with ML fundamentals and programming, but the course focuses on strategic thinking rather than implementation.</p> <p>This course is not appropriate for:</p> <ul> <li>Beginners without prior ML experience (take foundational coursework first)</li> <li>Those seeking advanced modeling techniques (focus is on problem framing, not model optimization)</li> <li>Practitioners looking for domain-specific solutions (course teaches general frameworks applicable across domains)</li> </ul>"},{"location":"course-description/#prerequisites","title":"Prerequisites","text":""},{"location":"course-description/#required-background","title":"Required Background","text":"<ul> <li>Machine Learning Fundamentals: Understanding of supervised learning, model training, evaluation metrics, overfitting, and generalization</li> <li>Implementation Experience: Have built and deployed at least one ML system (academic project, internship, or production system)</li> <li>Programming: Proficiency in Python and familiarity with standard ML libraries</li> <li>Linear Algebra and Statistics: Comfort with concepts like probability distributions, statistical significance, and basic hypothesis testing</li> </ul>"},{"location":"course-description/#recommended-background","title":"Recommended Background","text":"<ul> <li>Experience with LLMs and prompt engineering</li> <li>Exposure to production ML systems and their operational challenges</li> <li>Understanding of software engineering practices (version control, testing, deployment)</li> <li>Familiarity with business metrics and organizational dynamics</li> </ul>"},{"location":"course-description/#assessment","title":"Assessment","text":"<p>This course uses portfolio-based assessment rather than traditional exams. Your work will demonstrate applied problem framing skills through progressively complex assignments:</p>"},{"location":"course-description/#portfolio-components","title":"Portfolio Components","text":"<ol> <li>Framework Application Exercises (40%)</li> <li>Apply The Loop framework to 6-8 case scenarios</li> <li>Demonstrate systematic deconstruction, alternative generation, and trade-off analysis</li> <li>Each exercise builds specific skills (outcome clarity, constraint identification, signal design)</li> <li> <p>Evaluated on depth of analysis and systematic application of frameworks</p> </li> <li> <p>Case Study Analyses (30%)</p> </li> <li>Written analyses of 3-4 real-world AI projects</li> <li>Diagnose what went well, what failed, and why</li> <li>Propose alternative framings that could have led to better outcomes</li> <li> <p>Evaluated on critical thinking and evidence-based reasoning</p> </li> <li> <p>Capstone Problem Framing (30%)</p> </li> <li>Complete problem framing for a real or realistic AI opportunity</li> <li>Apply full Loop workflow from initial ambiguity to actionable implementation plan</li> <li>Include outcome specification, alternative evaluation, trade-off analysis, and signal framework</li> <li>Evaluated on comprehensiveness, rigor, and practical viability</li> </ol>"},{"location":"course-description/#peer-feedback-optional","title":"Peer Feedback (Optional)","text":"<p>While not required for completion, peer feedback is strongly encouraged. Reviewing others' problem framings helps you:</p> <ul> <li>Recognize patterns across different framings of similar problems</li> <li>Develop critical evaluation skills by identifying gaps in others' analyses</li> <li>See alternative approaches you might not have considered</li> <li>Practice giving constructive feedback on strategic thinking</li> </ul>"},{"location":"course-description/#grading-philosophy","title":"Grading Philosophy","text":"<p>Portfolio assignments are evaluated holistically based on:</p> <ul> <li>Systematic application of frameworks (not just intuitive reasoning)</li> <li>Depth of analysis (exploring second-order effects, hidden assumptions, edge cases)</li> <li>Evidence-based reasoning (grounding claims in data, examples, or case study evidence)</li> <li>Clarity of communication (structured arguments, clear trade-off articulation)</li> </ul> <p>There are no \"right answers\" in problem framing\u2014only well-reasoned or poorly-reasoned framings. Strong work demonstrates systematic thinking, considers multiple perspectives, and acknowledges uncertainty where appropriate.</p>"},{"location":"course-description/#course-schedule","title":"Course Schedule","text":"<p>This course is self-paced and designed for completion over 6-8 weeks with 5-7 hours per week, totaling approximately 40 hours.</p>"},{"location":"course-description/#chapter-1-the-ai-problem-framing-mindset-6-hours","title":"Chapter 1: The AI Problem Framing Mindset (6 hours)","text":"<ul> <li>Why AI projects fail at the framing stage</li> <li>The solution-first trap and how to avoid it</li> <li>Intellectual humility and working with ambiguity</li> <li>Outcome clarity vs. solution specification</li> <li>Assignment: Analyze 2 failed AI projects through framing lens</li> </ul>"},{"location":"course-description/#chapter-2-ai-solution-alternatives-8-hours","title":"Chapter 2: AI Solution Alternatives (8 hours)","text":"<ul> <li>The 13 AI solution archetypes (RAG, RLHF, fine-tuning, prompt engineering, hybrid systems, etc.)</li> <li>Matching archetypes to problem characteristics</li> <li>Common anti-patterns and premature convergence</li> <li>Building a mental model of the solution space</li> <li>Assignment: Generate 5+ alternatives for 3 problem scenarios</li> </ul>"},{"location":"course-description/#chapter-3-the-loop-framework-7-hours","title":"Chapter 3: The Loop Framework (7 hours)","text":"<ul> <li>The complete Loop: Outcome \u2192 Deconstruction \u2192 Alternatives \u2192 Trade-offs \u2192 Signals</li> <li>Systematic problem deconstruction techniques</li> <li>Trade-off analysis across cost, latency, accuracy, interpretability, and organizational fit</li> <li>Establishing meaningful success metrics and signal frameworks</li> <li>Assignment: Apply full Loop to 2 case studies</li> </ul>"},{"location":"course-description/#chapter-4-diagnosis-reading-signals-6-hours","title":"Chapter 4: Diagnosis - Reading Signals (6 hours)","text":"<ul> <li>Weak vs. strong signals in AI systems</li> <li>User behavior signals: adoption, engagement, workarounds</li> <li>System performance signals: accuracy drift, edge case failures, latency patterns</li> <li>Organizational signals: stakeholder feedback, resource constraints, competing priorities</li> <li>Assignment: Design signal frameworks for 3 AI initiatives</li> </ul>"},{"location":"course-description/#chapter-5-pivot-acting-on-signals-6-hours","title":"Chapter 5: Pivot - Acting on Signals (6 hours)","text":"<ul> <li>The persist/pivot/stop decision framework</li> <li>When to pivot vs. when to iterate</li> <li>Types of pivots: problem pivot, solution pivot, outcome pivot, scope pivot</li> <li>Managing sunk cost fallacy and stakeholder communication</li> <li>Assignment: Evaluate 3 case studies and recommend persist/pivot/stop with justification</li> </ul>"},{"location":"course-description/#chapter-6-application-full-case-studies-7-hours","title":"Chapter 6: Application - Full Case Studies (7 hours)","text":"<ul> <li>Healthcare: Clinical decision support system framing</li> <li>Education: Personalized learning platform framing</li> <li>Finance: Fraud detection system framing</li> <li>Enterprise: Knowledge management system framing</li> <li>Capstone Assignment: Complete problem framing for novel AI opportunity</li> </ul>"},{"location":"course-description/#recommended-pace","title":"Recommended Pace","text":"<ul> <li>Weeks 1-2: Chapters 1-2 (develop mindset, learn solution space)</li> <li>Weeks 3-4: Chapter 3 (master The Loop framework)</li> <li>Weeks 5-6: Chapters 4-5 (diagnosis and pivoting)</li> <li>Weeks 7-8: Chapter 6 (synthesis through case studies and capstone)</li> </ul> <p>You can accelerate or decelerate based on your schedule, but we recommend spending at least one week on Chapter 3 (The Loop) as it forms the foundation for subsequent work.</p>"},{"location":"course-description/#course-philosophy","title":"Course Philosophy","text":"<p>This course is built on several core beliefs:</p> <p>Problem framing is a skill, not intuition. While some practitioners develop good instincts through experience, systematic frameworks enable more consistent and teachable problem framing.</p> <p>The best solution to the wrong problem is still wrong. Technical excellence doesn't compensate for strategic misalignment. Learning when not to build AI is as valuable as learning how to build it well.</p> <p>Ambiguity is the norm, not the exception. Real-world AI problems start messy. The ability to work systematically with ambiguity separates effective practitioners from those who need perfectly-specified requirements.</p> <p>Pivoting is a feature, not a bug. The best AI teams pivot frequently based on evidence. This course teaches you to pivot strategically rather than randomly, and to recognize pivot signals early.</p> <p>Context matters more than best practices. There is no universally best AI solution archetype. Everything depends on constraints, stakeholders, resources, and organizational context.</p>"},{"location":"course-description/#instructor","title":"Instructor","text":"<p>Rajiv Shah, PhD Rajiv received his PhD from the University of Illinois Urbana-Champaign and currently works at Contextual AI. His research and industry experience span LLM alignment, production ML systems, and the organizational dynamics of AI adoption. He has advised dozens of AI initiatives across healthcare, education, and enterprise domains, with a focus on the strategic decisions that determine whether AI projects deliver measurable value.</p> <p>Ready to begin? Start with the course home page or jump directly to Chapter 1: The AI Problem Framing Mindset.</p>"},{"location":"faq/","title":"FAQ: AI Problem Framing for AI Practitioners","text":"<p>This FAQ addresses common questions from students in the graduate-level course on AI problem framing. Questions are organized by type to help you find quick answers.</p>"},{"location":"faq/#conceptual-clarifications","title":"Conceptual Clarifications","text":""},{"location":"faq/#q-what-is-the-difference-between-problem-framing-and-problem-solving","title":"Q: What is the difference between \"problem framing\" and \"problem-solving\"?","text":"<p>Problem framing is about correctly defining the challenge before you solve it\u2014identifying the real outcome you need, what constraints exist, and what alternatives are available. Problem-solving is execution: once the frame is clear, you build the solution. Most AI projects fail at framing (wrong outcome, incompletely decomposed), not at solving. Chapter 1 establishes this distinction as foundational to the course.</p>"},{"location":"faq/#q-why-is-system-2-thinking-important-for-ai-problem-framing","title":"Q: Why is System 2 thinking important for AI problem framing?","text":"<p>System 2 thinking (deliberate, analytical, slow) prevents the snap judgments of System 1 that lead practitioners to jump to technical solutions before understanding the problem. In AI, this prevents costly mistakes like choosing the wrong model architecture or optimizing the wrong metric. The course shows how to activate System 2 at critical framing moments. See Chapter 1: The AI Problem Framing Mindset.</p>"},{"location":"faq/#q-what-is-the-loop-and-why-does-it-have-five-steps","title":"Q: What is \"The Loop\" and why does it have five steps?","text":"<p>The Loop (Outcome \u2192 Deconstruction \u2192 Alternatives \u2192 Trade-offs \u2192 Signals) is a structured decision-making process that ensures you've thought through all angles before committing to an AI solution. Five steps because: (1) outcome clarity, (2) breaking the problem into components, (3) exploring multiple solution approaches, (4) weighing costs and benefits, (5) defining success metrics. Skipping any step leads to misaligned projects. See Chapter 3: The Loop Framework.</p>"},{"location":"faq/#q-what-are-the-13-ai-archetypes-and-why-should-i-care","title":"Q: What are the 13 AI archetypes and why should I care?","text":"<p>The 13 archetypes represent common AI solution patterns (e.g., RAG vs. long context, supervised learning vs. reinforcement learning, rule-based vs. learned models). Understanding these archetypes helps you quickly recognize which approach fits your decomposed problem, rather than defaulting to whatever's trendy. Chapter 2: AI Solution Alternatives catalogs these with trade-offs.</p>"},{"location":"faq/#q-how-do-signals-differ-from-metrics","title":"Q: How do \"signals\" differ from \"metrics\"?","text":"<p>Metrics are numbers you track (accuracy, latency, cost). Signals are the observable patterns that indicate whether your problem framing was correct and your solution is working as intended. A metric might show high accuracy, but the signal (user behavior, business outcome) might reveal your framing missed something critical. Signals are leading indicators of real-world success. See Chapter 4: Diagnosis.</p>"},{"location":"faq/#q-what-does-it-mean-to-decompose-a-problem","title":"Q: What does it mean to \"decompose\" a problem?","text":"<p>Decomposition breaks a complex problem into smaller, independently solvable sub-problems with clear boundaries. Instead of \"build an AI system,\" you identify: \"What data do we have? What is the actual prediction target? Who uses the output? What are failure modes?\" This reveals whether you actually need AI or if a simpler solution suffices. Chapter 3 teaches a systematic decomposition process.</p>"},{"location":"faq/#q-why-is-persist-vs-pivot-vs-stop-framed-as-a-decision-point","title":"Q: Why is \"persist vs. pivot vs. stop\" framed as a decision point?","text":"<p>After you've executed your framed solution and gathered signals, you're at an inflection: the framing might be spot-on (persist), or signals reveal misalignment (pivot the approach) or deeper misunderstanding (stop and reframe). This decision point prevents sunk-cost fallacy and ensures teams move deliberately, not reactively. See Chapter 5: Pivot.</p>"},{"location":"faq/#common-misconceptions","title":"Common Misconceptions","text":""},{"location":"faq/#q-is-it-true-that-more-data-always-solves-ai-problems","title":"Q: Is it true that more data always solves AI problems?","text":"<p>No. A common misconception is that data is the limiting factor. In reality, wrong framing is often the bottleneck\u2014you might be optimizing the wrong metric, predicting the wrong target, or solving a problem that isn't actually the business need. Gathering more data for a poorly framed problem wastes resources. Chapter 1 and Chapter 4 show how to diagnose whether data or framing is your real constraint.</p>"},{"location":"faq/#q-should-i-always-choose-the-most-sophisticated-ai-model","title":"Q: Should I always choose the most sophisticated AI model?","text":"<p>No. Practitioners often assume bigger or newer models are better, but a well-designed rule-based system or simpler model can outperform a complex approach if your problem is correctly framed. The Loop (especially the Alternatives and Trade-offs steps in Chapter 3) forces you to compare across complexity levels and find the simplest solution that meets your outcome.</p>"},{"location":"faq/#q-is-it-a-mistake-if-i-change-my-problem-frame-mid-project","title":"Q: Is it a mistake if I change my problem frame mid-project?","text":"<p>Not necessarily\u2014it's only a mistake if you're changing frames randomly or without evidence. If new signals show your original frame was incomplete, pivoting to a new frame is the right move. The key is intentionality: you should change frames because you've learned something concrete, not because things are hard. Chapter 5: Pivot covers when and how to reframe responsibly.</p>"},{"location":"faq/#q-do-i-need-deep-learning-for-every-ai-problem","title":"Q: Do I need deep learning for every AI problem?","text":"<p>No. Many AI problems are solved better with simpler approaches: decision trees, gradient boosting, heuristics, or even non-ML solutions. Deep learning shines in specific domains (vision, language, sequential data) where the complexity is justified. Defaulting to deep learning without framing your problem is a trap. Chapter 2: AI Solution Alternatives guides you through selecting the right archetype.</p>"},{"location":"faq/#q-is-it-true-that-if-my-model-has-high-test-accuracy-my-problem-is-solved","title":"Q: Is it true that if my model has high test accuracy, my problem is solved?","text":"<p>No. Test accuracy measures one dimension\u2014does the learned pattern generalize to held-out data? But it doesn't tell you: (1) Is this the metric that matters for your outcome? (2) Does the model work in production? (3) Are there fairness or safety concerns? Chapter 4: Diagnosis teaches you to read broader signals beyond accuracy metrics.</p>"},{"location":"faq/#q-should-i-always-optimize-for-the-latest-benchmark-or-leaderboard-metric","title":"Q: Should I always optimize for the latest benchmark or leaderboard metric?","text":"<p>No. Leaderboards optimize for academic reproducibility, not for real-world outcomes. Your business problem likely has constraints (latency, cost, fairness) that the benchmark ignores. The course teaches you to define your own signals tied to your outcome, not chase published benchmarks. See Chapter 3 (Signals step) and Chapter 4.</p>"},{"location":"faq/#q-is-peer-feedback-only-useful-after-the-course-ends","title":"Q: Is peer feedback only useful after the course ends?","text":"<p>No. The portfolio-based assessment includes ongoing peer feedback because framing is collaborative. What seems clear to you might be unclear to a teammate; peer review catches blind spots. Treating feedback as post-hoc instead of formative is a missed opportunity. Integrate feedback iteratively through the course.</p>"},{"location":"faq/#practical-applications","title":"Practical Applications","text":""},{"location":"faq/#q-how-do-i-apply-the-loop-to-a-real-project-im-working-on","title":"Q: How do I apply The Loop to a real project I'm working on?","text":"<p>Start with Outcome: Define the measurable end state (e.g., \"reduce customer churn by 10% in 6 months\"). Then Decompose the problem (what data signals churn? who decides actions?). Explore Alternatives (predictive model + outreach, rule-based system, etc.). Weigh Trade-offs (cost, speed, interpretability). Define Signals (what tells you it's working?). Work through one step at a time, document decisions, and involve stakeholders. Chapter 3 provides a template.</p>"},{"location":"faq/#q-when-should-i-use-rag-versus-long-context-approaches","title":"Q: When should I use RAG versus long-context approaches?","text":"<p>RAG (Retrieval-Augmented Generation) excels when your knowledge is large, dynamic, or frequently updated\u2014you retrieve relevant chunks on-demand. Long-context is better when the entire context fits in a model window and you want simpler architecture or reasoning across the full context. RAG scales better but adds latency and retrieval accuracy risk; long-context is simpler but more compute-intensive. See Chapter 2: AI Solution Alternatives for the full trade-off analysis.</p>"},{"location":"faq/#q-how-do-i-know-if-my-problem-even-needs-ai","title":"Q: How do I know if my problem even needs AI?","text":"<p>Ask: (1) Can it be solved with rules or heuristics? (2) Do you have labeled data or a way to generate it? (3) Will the solution be used frequently enough to justify complexity? (4) Can you define measurable success? If you answer \"no\" to most, you might not need AI\u2014a simpler, deterministic solution might be better. The Deconstruction step in Chapter 3 forces this question.</p>"},{"location":"faq/#q-how-do-i-choose-between-supervised-learning-reinforcement-learning-and-unsupervised-learning","title":"Q: How do I choose between supervised learning, reinforcement learning, and unsupervised learning?","text":"<p>Supervised learning is for predicting known targets (classification, regression). Reinforcement learning is for sequential decision-making where you learn from reward signals. Unsupervised learning is for finding patterns in unlabeled data. Your Outcome and Decomposition determine which fits: e.g., \"predict customer churn\" \u2192 supervised; \"optimize inventory allocation\" \u2192 RL; \"segment customers\" \u2192 unsupervised. Chapter 2 catalogs these with examples.</p>"},{"location":"faq/#q-what-should-i-do-if-my-signals-show-the-framing-was-wrong","title":"Q: What should I do if my signals show the framing was wrong?","text":"<p>First, understand why it was wrong\u2014did you misunderstand the outcome? Incomplete decomposition? Wrong metric? Once you diagnose the flaw, decide: can you adjust your approach (persist), do you need a different solution type (pivot), or was the original problem statement flawed (stop and reframe)? Chapter 5: Pivot walks through this decision. Document what you learned so stakeholders understand the adjustment is intentional.</p>"},{"location":"faq/#q-how-do-i-communicate-my-framing-to-non-technical-stakeholders","title":"Q: How do I communicate my framing to non-technical stakeholders?","text":"<p>Use the Loop's language: (1) What's the outcome? (2) How do we decompose it? (3) What are the solution alternatives? (4) What are the trade-offs? (5) How will we know if it works? This avoids jargon and focuses on business logic. Visual diagrams of your decomposition and a simple comparison table of alternatives are especially powerful. See examples in Chapter 6: Application.</p>"},{"location":"faq/#q-how-do-i-handle-stakeholders-who-push-for-a-specific-solution-before-framing","title":"Q: How do I handle stakeholders who push for a specific solution before framing?","text":"<p>Acknowledge the suggestion but redirect to framing first: \"Let's make sure a [their suggestion] is the best fit for this outcome. What does success look like?\" Work through The Loop with them to show whether their suggestion aligns. Often, they'll realize it doesn't, or you'll both learn that alternatives make more sense. Chapter 1 emphasizes that framing is collaborative.</p>"},{"location":"faq/#practical-applications-technical-deep-dives","title":"Practical Applications (Technical Deep Dives)","text":""},{"location":"faq/#q-how-do-i-design-a-decomposition-tree-for-a-complex-problem","title":"Q: How do I design a decomposition tree for a complex problem?","text":"<p>Start with your outcome at the root. Ask: \"What are the independent sub-problems?\" For example, \"Predict customer churn\" decomposes into: (1) Data collection (what signals indicate churn?), (2) Feature engineering (what patterns matter?), (3) Model selection (what architecture fits?), (4) Deployment (how do we use predictions?). Each sub-problem can decompose further. Stop when sub-problems are concrete and independently solvable. Chapter 3 includes templates and examples.</p>"},{"location":"faq/#q-how-should-i-prototype-different-framings-before-committing-to-one","title":"Q: How should I prototype different framings before committing to one?","text":"<p>Design lightweight experiments for each framing: a small pilot for each alternative. For example, if you're unsure whether supervised learning or RL is better, build a toy supervised model in 2 weeks, a toy RL model in 2 weeks, and compare signals (accuracy, speed, interpretability). This costs less than full implementation and reveals which framing resonates. Document findings rigorously. Chapter 6: Application shows case studies that used this approach.</p>"},{"location":"faq/#q-how-do-i-ensure-my-signals-are-actually-predictive-of-the-outcome-i-care-about","title":"Q: How do I ensure my signals are actually predictive of the outcome I care about?","text":"<p>Define a causal model: \"If signal X goes up, outcome Y should improve.\" Test this hypothesis with data (time-series analysis, A/B tests, or historical correlation). A signal that doesn't correlate with your outcome is a vanity metric. For example, model accuracy might be a vanity metric if it doesn't correlate with user engagement. Validate signals iteratively. See Chapter 4: Diagnosis for signal validation techniques.</p>"},{"location":"faq/#prerequisites-next-steps","title":"Prerequisites &amp; Next Steps","text":""},{"location":"faq/#q-what-background-do-i-need-before-taking-this-course","title":"Q: What background do I need before taking this course?","text":"<p>You should be comfortable with basic ML concepts (supervised vs. unsupervised learning, train/test split, overfitting) and have shipped or worked on at least one end-to-end ML project. The course assumes you've experienced the pain of a misaligned project\u2014framing is more powerful if you've felt its absence. No specific tech stack is required; the focus is conceptual and decision-making.</p>"},{"location":"faq/#q-how-is-this-course-different-from-typical-ml-courses-that-focus-on-algorithms","title":"Q: How is this course different from typical ML courses that focus on algorithms?","text":"<p>Most ML courses teach how to build (algorithms, libraries, optimization). This course teaches what to build\u2014the upstream thinking that prevents wasted effort. You'll use fewer fancy techniques but make better decisions about which techniques to use. Think of it as going from \"how to code models\" to \"why that model for that problem.\"</p>"},{"location":"faq/#q-can-i-apply-this-course-to-non-ai-problems","title":"Q: Can I apply this course to non-AI problems?","text":"<p>Yes. The Loop and framing methodology apply to any complex decision under uncertainty. Product roadmap decisions, organizational restructuring, research directions\u2014all benefit from explicit outcome definition, decomposition, alternatives exploration, trade-off analysis, and signal definition. The course examples use AI, but the framework is universal.</p>"},{"location":"faq/#q-what-should-i-study-after-this-course","title":"Q: What should I study after this course?","text":"<p>Deepen in two directions: (1) Specialized AI tracks: dive into specific archetypes (LLM applications, computer vision, recommendation systems), or (2) Broader decision-making: study causal inference, decision theory, and organizational strategy to strengthen your framing skills across domains. The course prepares you to learn these effectively because you'll ask better questions.</p>"},{"location":"faq/#q-how-will-this-course-help-me-in-job-interviews","title":"Q: How will this course help me in job interviews?","text":"<p>Interviewers (especially at senior levels) love candidates who ask clarifying questions and push back on vague requirements. This course trains that skill. When asked \"Build an AI system for X,\" you'll decompose, ask about outcomes, and propose alternatives with trade-offs\u2014exactly what senior engineers do. Your portfolio demonstrates this thinking concretely.</p>"},{"location":"faq/#q-can-i-use-my-portfolio-from-this-course-in-my-professional-work","title":"Q: Can I use my portfolio from this course in my professional work?","text":"<p>Yes. Your portfolio documents real problem-framing decisions you've made (or analyzed from case studies). Use it to showcase your decision-making process to hiring managers, in promotion discussions, or when pitching ideas to leadership. The best portfolios include both successful and failed framings\u2014they show you learn and iterate intentionally.</p>"},{"location":"faq/#q-whats-the-relationship-between-this-course-and-hands-on-ml-implementation","title":"Q: What's the relationship between this course and hands-on ML implementation?","text":"<p>This course is upstream of implementation. A strong framing saves 10x effort in implementation because you're building the right thing. But this course doesn't teach implementation\u2014coding, deployment, monitoring are in other courses. Think of framing as the strategic layer and implementation as the tactical layer; both matter, but strategy must come first.</p>"},{"location":"faq/#q-how-often-should-i-revisit-the-framing-of-an-active-project","title":"Q: How often should I revisit the framing of an active project?","text":"<p>Quarterly or when you see misalignment signals (models underperforming, stakeholder confusion, shifting business goals). Don't reframe constantly\u2014that's thrashing\u2014but do revisit when evidence suggests your frame is incomplete. Chapter 5 teaches how to distinguish between \"we should adjust tactics\" and \"we should revisit the frame.\"</p>"},{"location":"glossary/","title":"Glossary: AI Problem Framing for AI Practitioners","text":"<p>A comprehensive reference of key concepts for understanding AI problem framing at the graduate level.</p>"},{"location":"glossary/#alternatives-menu","title":"Alternatives Menu","text":"<p>A structured set of different AI approaches or solution strategies identified for a given problem. Creates an explicit framework for comparing how various methods could address the core issue.</p> <p>Example: For customer churn, an alternatives menu might include: regression (predict churn probability), classification (high/low risk buckets), or clustering (identify similar at-risk customers).</p>"},{"location":"glossary/#anomaly-detection","title":"Anomaly Detection","text":"<p>A machine learning task that identifies data points, patterns, or behaviors that deviate significantly from normal or expected distributions. Used to find unusual observations within datasets.</p> <p>Example: Detecting fraudulent credit card transactions that fall outside normal spending patterns for an individual user.</p>"},{"location":"glossary/#atomic-unit","title":"Atomic Unit","text":"<p>The smallest, independently meaningful component of a problem that can be diagnosed, solved, or measured. Identifies the granularity level needed for effective problem deconstruction.</p> <p>Example: In invoice processing, the atomic unit might be a single line item rather than an entire invoice, enabling line-level quality metrics.</p>"},{"location":"glossary/#assumptions","title":"Assumptions","text":"<p>Beliefs or conditions accepted as true without verification when framing a problem. Critical to identify explicitly because false assumptions undermine problem and solution frames.</p> <p>Example: Assuming that historical sales data reflects current customer behavior may fail when market conditions have fundamentally changed.</p>"},{"location":"glossary/#autonomous-agent","title":"Autonomous Agent","text":"<p>An AI system that operates with minimal human intervention, making decisions and taking actions based on learned patterns, goals, or instructions without requiring approval for each step.</p> <p>Example: A recommendation system that automatically adjusts product suggestions based on real-time user behavior without human review of each recommendation.</p>"},{"location":"glossary/#bias-detection","title":"Bias Detection","text":"<p>The process of measuring and identifying systematic errors, disparities, or unfair treatment in AI model predictions across different demographic groups or conditions.</p> <p>Example: Discovering that a hiring recommendation model rejects female candidates at rates 15% higher than male candidates with identical qualifications.</p>"},{"location":"glossary/#cautionary-tale","title":"Cautionary Tale","text":"<p>A documented case where problem framing or decision-making went wrong, providing lessons about which approaches to avoid or which hidden assumptions to watch for.</p> <p>Example: A company that built a predictive churn model assuming recent behavior indicates lifetime value, then discovered their most valuable customers had intentionally reduced engagement before renewal.</p>"},{"location":"glossary/#churn-prediction","title":"Churn Prediction","text":"<p>A classification task that predicts the probability or likelihood that a customer will stop using a service or product within a specified time period.</p> <p>Example: Predicting which subscription users will cancel within the next 30 days to enable targeted retention campaigns.</p>"},{"location":"glossary/#classification","title":"Classification","text":"<p>A supervised machine learning task that assigns data points to predefined categories or classes based on learned patterns from labeled examples.</p> <p>Example: Categorizing emails as spam or not-spam based on historical examples of each category.</p>"},{"location":"glossary/#clustering","title":"Clustering","text":"<p>An unsupervised machine learning task that groups similar data points together based on patterns without predefined categories or labels.</p> <p>Example: Grouping customers by purchase behavior to discover natural market segments without specifying segments in advance.</p>"},{"location":"glossary/#cognitive-bias","title":"Cognitive Bias","text":"<p>Systematic patterns in human thinking that lead to consistent errors or deviations from rational judgment, often unconscious and difficult to overcome.</p> <p>Example: Confirmation bias leads practitioners to seek data supporting their initial problem hypothesis while ignoring contradictory evidence.</p>"},{"location":"glossary/#concept-drift","title":"Concept Drift","text":"<p>A change in the statistical relationship between input variables and outcomes over time, causing models trained on historical data to become less accurate on new data.</p> <p>Example: A model trained on past unemployment patterns may fail when economic policy or labor market structure fundamentally changes.</p>"},{"location":"glossary/#data-distribution","title":"Data Distribution","text":"<p>The statistical properties and patterns of how values are spread across a dataset, including central tendency, variability, and shape characteristics.</p> <p>Example: Customer ages in an e-commerce dataset show bimodal distribution with peaks at 25-34 and 55-64, indicating two distinct demographic segments.</p>"},{"location":"glossary/#data-drift","title":"Data Drift","text":"<p>A change in the statistical properties of input data (features) over time, causing models trained on historical distributions to encounter unfamiliar patterns in production.</p> <p>Example: A model trained on credit card spending patterns from 2019 encounters post-pandemic spending behavior with different seasonal patterns and magnitude.</p>"},{"location":"glossary/#decision-threshold","title":"Decision Threshold","text":"<p>The boundary value used to convert continuous model outputs (scores or probabilities) into discrete decisions or actions.</p> <p>Example: Setting a churn prediction threshold at 0.6 probability means customers scoring above 60% are targeted for retention, while those below are not.</p>"},{"location":"glossary/#deterministic-agent","title":"Deterministic Agent","text":"<p>An AI system that produces the same output given the same input, following explicitly defined rules without randomness or learned variability.</p> <p>Example: A rules-based invoice processing system that applies consistent formatting and extraction logic to all invoices.</p>"},{"location":"glossary/#diagnostic-test","title":"Diagnostic Test","text":"<p>A specific analysis or experiment designed to identify the root cause of poor model performance or to validate assumptions about problem structure.</p> <p>Example: Splitting model errors by input data characteristics to determine whether performance problems affect all customer segments equally or concentrate in specific groups.</p>"},{"location":"glossary/#domain-context","title":"Domain Context","text":"<p>The background knowledge, constraints, business realities, and historical context specific to the problem area that shape what solutions are feasible and valuable.</p> <p>Example: In healthcare, domain context includes regulatory requirements, clinical workflows, and the critical importance of false negatives in diagnosis.</p>"},{"location":"glossary/#embeddings","title":"Embeddings","text":"<p>Numerical vector representations of text, images, or other data that capture semantic meaning or relationships in a continuous, lower-dimensional space.</p> <p>Example: Word embeddings represent \"king\" and \"queen\" as points in vector space where the relationship between them is similar to the relationship between \"man\" and \"woman.\"</p>"},{"location":"glossary/#escalation-of-commitment","title":"Escalation of Commitment","text":"<p>A cognitive tendency to continue investing in a course of action despite evidence that it is failing, driven by sunk costs and desire to avoid admitting past mistakes.</p> <p>Example: A team continues building a complex ML pipeline even after diagnosis shows the root problem could be solved with a simpler rule-based approach.</p>"},{"location":"glossary/#error-analysis","title":"Error Analysis","text":"<p>A systematic investigation of model mistakes to identify patterns, categories, or conditions where errors concentrate, revealing where to focus improvements.</p> <p>Example: Analyzing misclassified images in a computer vision model and discovering 80% of errors occur on low-light photos, pointing to a data quality issue.</p>"},{"location":"glossary/#error-pattern","title":"Error Pattern","text":"<p>Recurring or systematic characteristics in how a model fails, revealing underlying problems rather than random mistakes.</p> <p>Example: A sentiment analysis model consistently misclassifies sarcasm as positive, showing it cannot interpret indirect language.</p>"},{"location":"glossary/#extraction","title":"Extraction","text":"<p>A machine learning task that identifies and pulls structured information from unstructured text or documents, converting raw content into organized, usable data.</p> <p>Example: Extracting invoice number, amount, and due date from a PDF document.</p>"},{"location":"glossary/#fairness-metric","title":"Fairness Metric","text":"<p>A quantitative measure of whether a model's predictions or decisions treat different demographic groups or conditions equitably without systematic disparities.</p> <p>Example: Using demographic parity to measure whether a loan approval model approves applications from different racial groups at equal rates.</p>"},{"location":"glossary/#fine-tuning","title":"Fine-Tuning","text":"<p>A training approach that takes a pre-trained model and continues learning on task-specific or domain-specific data, allowing efficient adaptation to new problems.</p> <p>Example: Taking a general language model trained on broad internet text and fine-tuning it on medical literature to improve clinical summarization performance.</p>"},{"location":"glossary/#first-principles","title":"First Principles","text":"<p>A problem-solving approach that breaks complex issues into fundamental, irreducible facts and rebuilds solutions from there rather than relying on assumptions or conventions.</p> <p>Example: Questioning why a business needs an ML model at all, rather than assuming ML is the right approach because it solved similar problems elsewhere.</p>"},{"location":"glossary/#forecasting","title":"Forecasting","text":"<p>A prediction task that estimates future values of a time-dependent variable based on historical patterns and conditions.</p> <p>Example: Predicting monthly sales revenue for the next quarter based on historical sales trends, seasonality, and external economic indicators.</p>"},{"location":"glossary/#generation","title":"Generation","text":"<p>A machine learning task that creates new data, text, images, or other outputs based on learned patterns, often using generative models or transformers.</p> <p>Example: A language model generating human-like responses to customer service questions.</p>"},{"location":"glossary/#hammer-bias","title":"Hammer Bias","text":"<p>A cognitive bias where a person familiar with a particular tool assumes it is the right solution to many different problems, regardless of actual suitability.</p> <p>Example: A data scientist who specializes in neural networks proposing deep learning for every prediction task, even where simpler models would be more effective.</p>"},{"location":"glossary/#invoice-processing","title":"Invoice Processing","text":"<p>The application of extraction and classification AI to automatically parse, validate, and categorize invoice documents, reducing manual data entry.</p> <p>Example: An OCR and NLP system that extracts invoice details, matches them to purchase orders, and flags discrepancies for human review.</p>"},{"location":"glossary/#kill-signal","title":"Kill Signal","text":"<p>An observable indicator or metric that rises above a defined threshold, suggesting the current problem frame or solution is no longer viable and should be abandoned.</p> <p>Example: When a model's false positive rate climbs above operational capacity, it becomes a kill signal indicating the current approach cannot safely serve its intended purpose.</p>"},{"location":"glossary/#leading-indicator","title":"Leading Indicator","text":"<p>An observable metric or signal that predicts future performance or outcomes, measured earlier in a process than the final outcome itself.</p> <p>Example: Weekly engagement metrics serve as a leading indicator for monthly retention, allowing corrective actions before churn occurs.</p>"},{"location":"glossary/#lexical-search","title":"Lexical Search","text":"<p>A retrieval method that matches documents or records to queries based on exact word matches or phrase overlap, without understanding semantic meaning.</p> <p>Example: A search engine that returns documents containing the exact words from a query, regardless of whether they address the intended meaning.</p>"},{"location":"glossary/#long-context","title":"Long Context","text":"<p>A language model's ability to process and maintain coherence across very long input sequences, extending beyond traditional context window limitations.</p> <p>Example: A system that can summarize an entire 100-page document while maintaining consistency because it processes the full context together.</p>"},{"location":"glossary/#mental-model","title":"Mental Model","text":"<p>An internal representation of how something works, structured as interconnected concepts and causal relationships that guide understanding and problem-solving.</p> <p>Example: An engineer's mental model of customer churn includes: engagement \u2192 satisfaction \u2192 renewal decision \u2192 churn risk.</p>"},{"location":"glossary/#model-drift","title":"Model Drift","text":"<p>Deterioration in a machine learning model's performance over time due to changes in data distribution, concept relationships, or environmental conditions.</p> <p>Example: A model trained to detect fraud in 2022 shows declining accuracy in 2024 because fraudsters have adopted new tactics.</p>"},{"location":"glossary/#ml-pivot","title":"ML Pivot","text":"<p>A decision to pursue a machine learning solution after determining that a simpler approach cannot adequately solve the problem, requiring significant engineering and data investment.</p> <p>Example: After a rules-based system cannot achieve sufficient accuracy for invoice processing, pivoting to an ML-based extraction approach.</p>"},{"location":"glossary/#monday-morning-checklist","title":"Monday Morning Checklist","text":"<p>A practical decision-making framework used when returning to a problem to assess whether the original framing remains valid or whether circumstances have changed.</p> <p>Example: Before continuing development on a churn prediction model, verify: Are we still solving the right problem? Has the business context changed? Is the data still valid?</p>"},{"location":"glossary/#optimization","title":"Optimization","text":"<p>A machine learning task that finds the best combination of inputs or actions within constraints to maximize or minimize an objective function.</p> <p>Example: Determining the optimal product mix and pricing for maximum profit given manufacturing and demand constraints.</p>"},{"location":"glossary/#outcome-metric","title":"Outcome Metric","text":"<p>The quantifiable measure of success that defines what achieving the goal actually means and enables tracking progress toward solving the problem.</p> <p>Example: For a churn reduction project, the outcome metric might be retention rate improvement by 5 percentage points within 6 months.</p>"},{"location":"glossary/#pattern-bridge","title":"Pattern Bridge","text":"<p>A structured approach to transferring knowledge or applying patterns learned in one context to solve similar problems in a different context, avoiding redundant problem-solving.</p> <p>Example: Recognizing that the pattern used to optimize warehouse logistics applies equally to optimizing data center resource allocation.</p>"},{"location":"glossary/#persist-decision","title":"Persist Decision","text":"<p>A choice to continue with the current problem frame and solution approach based on evidence that progress is being made and the direction remains sound.</p> <p>Example: After testing a classification approach to churn and seeing promising early results, deciding to persist rather than explore alternative methods.</p>"},{"location":"glossary/#pivot-decision","title":"Pivot Decision","text":"<p>A choice to fundamentally change the problem frame, solution approach, or resource allocation strategy based on new information that makes the current path unviable.</p> <p>Example: Recognizing that churn is driven by product defects rather than pricing, pivoting from a retention model to a product quality improvement focus.</p>"},{"location":"glossary/#prediction","title":"Prediction","text":"<p>A machine learning task that estimates unknown outcomes, future values, or missing information for new data points based on learned patterns.</p> <p>Example: Predicting a patient's likelihood of developing diabetes within 5 years based on health and lifestyle factors.</p>"},{"location":"glossary/#problem-deconstruction","title":"Problem Deconstruction","text":"<p>The process of breaking a complex business problem into smaller, more manageable sub-problems that can be individually diagnosed and solved.</p> <p>Example: Decomposing \"reduce customer churn\" into: churn detection, churn root cause identification, retention action selection, and retention outcome measurement.</p>"},{"location":"glossary/#problem-frame","title":"Problem Frame","text":"<p>A specific definition of what the business problem actually is, what success means, and what factors are relevant\u2014establishing boundaries on the solution space.</p> <p>Example: \"We need to identify high-risk customers 30 days before they churn so we can apply targeted interventions\" is a frame that differs from \"minimize churn overall.\"</p>"},{"location":"glossary/#problem-framing","title":"Problem Framing","text":"<p>The practice of clearly defining a business problem, its underlying causes, success criteria, constraints, and assumptions before selecting a solution approach.</p> <p>Example: Before building a recommendation system, framing the problem: Are we optimizing engagement, revenue, or customer lifetime value? Does cold-start matter?</p>"},{"location":"glossary/#problem-portfolio","title":"Problem Portfolio","text":"<p>A structured inventory of multiple related problems within an organization, prioritized by business impact and solution feasibility to guide strategic focus.</p> <p>Example: A retail company's problem portfolio includes: inventory optimization, demand forecasting, customer churn prediction, and fraud detection.</p>"},{"location":"glossary/#prompting","title":"Prompting","text":"<p>A technique of providing specific instructions or examples to a language model to guide it toward desired behavior without requiring training or fine-tuning.</p> <p>Example: Giving a model the instruction \"Respond in a professional tone\" or providing an example of desired output format shapes its responses.</p>"},{"location":"glossary/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":"<p>An approach that combines retrieval of relevant documents or information with generative language models to produce answers grounded in specific sources.</p> <p>Example: A customer service system that retrieves relevant knowledge base articles then uses a language model to generate responses based on that retrieved context.</p>"},{"location":"glossary/#recommendation-system","title":"Recommendation System","text":"<p>A machine learning application that predicts user preferences and suggests products, content, or actions most likely to be valuable or interesting to individuals.</p> <p>Example: Netflix's system that recommends movies by learning from viewing patterns, ratings, and similar users' preferences.</p>"},{"location":"glossary/#reframing","title":"Reframing","text":"<p>The cognitive act of reconceptualizing a problem by changing perspectives, assumptions, or boundaries to reveal new solution possibilities.</p> <p>Example: Instead of framing high returns as a shipping logistics problem, reframing it as a product design problem (products that arrive damaged cause returns).</p>"},{"location":"glossary/#regression","title":"Regression","text":"<p>A supervised learning task that predicts continuous numerical values based on input variables and historical labeled examples.</p> <p>Example: Predicting house prices from square footage, location, age, and other property characteristics.</p>"},{"location":"glossary/#retrieval","title":"Retrieval","text":"<p>A machine learning or information retrieval task that finds and returns the most relevant documents, records, or information from a collection based on a query or input.</p> <p>Example: A search engine that ranks and returns the most relevant web pages given a search query.</p>"},{"location":"glossary/#sanity-check","title":"Sanity Check","text":"<p>A quick validation that results make intuitive sense, are in expected ranges, and don't contradict known facts before trusting model outputs.</p> <p>Example: If a churn prediction model assigns 0% churn risk to a customer who explicitly stated they're switching providers, the sanity check catches this nonsensical prediction.</p>"},{"location":"glossary/#semantic-search","title":"Semantic Search","text":"<p>A retrieval method that matches documents to queries based on meaning and conceptual relevance rather than exact word matching.</p> <p>Example: A search for \"ways to fix leaky faucets\" returns results about plumbing repairs even if documents don't contain the exact words from the query.</p>"},{"location":"glossary/#signal-recognition","title":"Signal Recognition","text":"<p>The ability to identify early indicators, patterns, or evidence that current strategy or problem frame is working well or needs to change.</p> <p>Example: Recognizing that model performance improvement has plateaued (a signal to pivot) versus continuing to incrementally optimize.</p>"},{"location":"glossary/#solution-frame","title":"Solution Frame","text":"<p>A specific definition of how the problem will be addressed, including which approach (rule-based, ML, GenAI), success criteria, and resource requirements.</p> <p>Example: \"Build a classification model using historical customer behavior to score churn risk weekly, targeting users above 60% risk with personalized retention offers.\"</p>"},{"location":"glossary/#solution-space","title":"Solution Space","text":"<p>The full range of technically and practically feasible approaches available to address a particular problem.</p> <p>Example: For demand forecasting, the solution space includes: rule-based methods, statistical forecasting, time series models, deep learning, and human judgment.</p>"},{"location":"glossary/#stop-decision","title":"Stop Decision","text":"<p>A choice to completely abandon the current problem frame and solution direction, determining that continuing would waste resources without delivering value.</p> <p>Example: Deciding to stop work on a complex churn prediction model after diagnosis reveals the actual problem is product quality, not customer retention dynamics.</p>"},{"location":"glossary/#stranger-test","title":"Stranger Test","text":"<p>A decision-making practice of explaining the problem and proposed solution to someone unfamiliar with the context; if they cannot understand, the framing likely lacks clarity.</p> <p>Example: A product manager unable to explain in simple terms why the team built a specific ML model suggests the business justification may be unclear.</p>"},{"location":"glossary/#structured-search","title":"Structured Search","text":"<p>A retrieval approach that searches across data with defined schemas and field structure, often using exact matching on specific attributes or ranges.</p> <p>Example: Searching a customer database for all users in zip code 90210 with purchase history above $1,000 in the last year.</p>"},{"location":"glossary/#success-signal","title":"Success Signal","text":"<p>An observable indicator or metric that rises above a defined threshold, demonstrating that the current problem frame and solution approach are delivering value.</p> <p>Example: When retention rate improves by 5% after launching the churn prediction system, it's a success signal to continue and expand the approach.</p>"},{"location":"glossary/#system-2-thinking","title":"System 2 Thinking","text":"<p>A deliberate, analytical mode of cognition that involves slow, conscious reasoning and evaluation rather than quick intuitive judgments.</p> <p>Example: Instead of intuitively assuming ML will solve a problem (System 1), systematically analyzing whether simpler approaches might be sufficient (System 2).</p>"},{"location":"glossary/#system-level-reframe","title":"System-Level Reframe","text":"<p>A fundamental shift in how a problem is understood that changes the boundaries, stakeholders, or goals, not just the tactical approach.</p> <p>Example: Reframing customer acquisition from a marketing problem to a product-market fit problem, requiring changes beyond customer targeting tactics.</p>"},{"location":"glossary/#the-loop","title":"The Loop","text":"<p>A cyclical framework for problem diagnosis and decision-making: define outcomes, identify signals, diagnose problems, decide (persist/pivot/stop), then repeat.</p> <p>Example: The Loop process for churn: set retention target \u2192 monitor leading indicators \u2192 analyze why targets are missed \u2192 decide to improve product or refine targeting \u2192 repeat.</p>"},{"location":"glossary/#trade-offs","title":"Trade-offs","text":"<p>Competing priorities or constraints where improving one dimension requires accepting degradation in another, necessitating explicit prioritization.</p> <p>Example: A recommendation system might face trade-offs between personalization (higher engagement) and diversity (broader cultural exposure), requiring intentional optimization choice.</p>"},{"location":"glossary/#vector-database","title":"Vector Database","text":"<p>A specialized database designed to efficiently store and search high-dimensional numerical vectors (embeddings), enabling semantic similarity queries.</p> <p>Example: A vector database that stores customer embeddings and enables finding \"customers most similar to our best customer\" without explicit attribute matching.</p> <p>Total Terms: 150 Coverage: Chapters 1-6 plus integrated concepts Format Compliance: ISO 11179 standards (precise, concise, distinct, non-circular, no business rules) Examples Provided: ~75% of terms</p>"},{"location":"chapters/01-mindset/","title":"Chapter 1: The AI Problem Framing Mindset","text":""},{"location":"chapters/01-mindset/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to:</p> <ol> <li>Distinguish between System 1 and System 2 thinking in AI problem framing</li> <li>Identify common cognitive biases (Hammer Bias, etc.) that derail AI projects</li> <li>Apply the Stranger Test to evaluate problem clarity</li> <li>Analyze domain context to inform framing decisions</li> </ol>"},{"location":"chapters/01-mindset/#introduction","title":"Introduction","text":"<p>In 2019, a healthcare startup raised $47 million to use machine learning to predict patient readmissions. The technology worked beautifully\u2014their model achieved 87% accuracy in predicting which patients would return to the hospital within 30 days. Eighteen months later, the company shut down.</p> <p>What went wrong? The technology wasn't the problem. The problem was the problem.</p> <p>The startup had framed their challenge as a prediction problem: \"Which patients will be readmitted?\" But hospitals didn't actually need predictions\u2014they needed interventions. Even with perfect predictions, nurses and social workers were overwhelmed and couldn't act on the information. The real problem wasn't identifying at-risk patients (clinicians already had good intuitions about this), but rather designing interventions that fit into existing workflows and addressing the underlying social determinants of health that caused readmissions in the first place.</p> <p>This is not a story about bad technology. It's a story about jumping to a solution before understanding the problem. And it's far from unique.</p> <p>According to various industry surveys, between 60-85% of AI projects fail to reach production. When these failures are analyzed, the most common culprit isn't insufficient data, inadequate computing power, or poorly trained models. It's problem framing\u2014the fundamental work of understanding what problem you're actually trying to solve before you decide how to solve it.</p> <p>This course is built on a single provocative claim: Most AI failures aren't technical failures. They're failures of thinking.</p> <p>In this opening chapter, we'll examine the cognitive patterns that lead to poor problem framing and develop the mindset needed to frame problems well. We'll explore why our natural thinking patterns often mislead us, how to recognize when we're being misled, and how to cultivate the deliberate, analytical thinking that effective problem framing requires.</p>"},{"location":"chapters/01-mindset/#section-1-system-2-thinking-for-ai","title":"Section 1: System 2 Thinking for AI","text":"<p>Let's start with a scenario that might feel uncomfortably familiar.</p> <p>You're in a meeting. A colleague says, \"We need to predict customer churn so we can offer targeted discounts to at-risk customers.\" Within seconds, your mind is racing: What data do you have? What features would you need? Should you use a random forest or gradient boosting? What's the target variable?</p> <p>Notice what just happened. You jumped immediately from hearing a business challenge to designing a machine learning solution. You skipped over the most important question: Is this actually a prediction problem?</p> <p>This is System 1 thinking at work.</p>"},{"location":"chapters/01-mindset/#key-idea-two-systems-of-thought","title":"Key Idea: Two Systems of Thought","text":"<p>Psychologist Daniel Kahneman distinguishes between two modes of thinking:</p> <p>System 1 is fast, automatic, and intuitive. It's the thinking that lets you drive a familiar route while having a conversation, recognize a friend's face in a crowd, or answer \"2 + 2 = ?\" without conscious effort. System 1 is incredibly useful for navigating daily life\u2014but it's terrible at problem framing.</p> <p>System 2 is slow, deliberate, and analytical. It's the thinking required to calculate 17 \u00d7 24 in your head, evaluate a complex argument, or check whether your intuitions are actually correct. System 2 is effortful and mentally taxing\u2014but it's exactly what effective problem framing requires.</p> <p>Here's the challenge: When we encounter a problem, System 1 kicks in first. It pattern-matches to familiar solutions, makes assumptions based on surface features, and generates answers that feel right. Only with deliberate effort do we engage System 2 to question those intuitions.</p> <p>For AI practitioners, this creates a specific hazard. If you're reading this course, you probably have training in machine learning, data science, or a related field. Your System 1 has been trained to recognize \"problems that ML can solve.\" When you hear a business challenge, your brain automatically translates it into ML terms. This translation happens so quickly and smoothly that you might not even notice it's occurring.</p> <p>Let's examine what System 2 thinking looks like in practice.</p>"},{"location":"chapters/01-mindset/#example-reframing-the-churn-problem","title":"Example: Reframing the Churn Problem","text":"<p>Recall the earlier scenario: \"We need to predict customer churn so we can offer targeted discounts to at-risk customers.\"</p> <p>System 1 response: \"Great, a binary classification problem. Let's gather historical customer data and build a logistic regression baseline.\"</p> <p>System 2 response: \"Wait. Let's slow down and examine the actual problem.\"</p> <p>Here's what System 2 thinking might uncover:</p> <p>Question 1: What's the actual business goal? Not \"predict churn\" but \"reduce churn\" or more precisely \"increase customer lifetime value.\" Prediction is a means, not an end.</p> <p>Question 2: What would we do with predictions? Offer discounts to at-risk customers. But does this actually work? Have we tested whether discounts reduce churn, or do they simply reduce margins on customers who would have stayed anyway?</p> <p>Question 3: Why do customers churn? If customers leave because a competitor offers better features, discounts won't help. If they leave because onboarding was confusing, predicting churn is treating a symptom, not the disease.</p> <p>Question 4: What are we not considering? Maybe the problem isn't predicting who will leave, but understanding why they stay. Maybe it's not about individual customers at all, but about segments or product features.</p> <p>Notice the shift. System 1 gave us a solution (build a churn prediction model). System 2 gave us questions\u2014questions that might lead to a completely different approach. Perhaps the real problem is improving onboarding, or fixing product bugs, or identifying which features drive loyalty. None of these require machine learning.</p> <p>This is the essence of System 2 thinking for AI: Slowing down to question the problem before jumping to solutions.</p>"},{"location":"chapters/01-mindset/#try-it-practicing-system-2-thinking","title":"Try It: Practicing System 2 Thinking","text":"<p>Here's a low-stakes exercise to practice engaging System 2 thinking.</p> <p>Scenario: A university administrator says, \"We need to use AI to predict which students will drop out so we can intervene early.\"</p> <p>Your System 1 probably already has ideas. But before you think about data or models:</p> <ol> <li>Write down three questions you would ask to understand the actual problem</li> <li>For each question, explain what you might learn that would change your approach</li> <li>Identify one assumption embedded in the problem statement</li> </ol> <p>Take a few minutes to work through this before reading ahead.</p> <p>Some questions you might have asked:</p> <ul> <li>What does \"intervene\" mean? (If there's no intervention capacity, predictions are useless)</li> <li>What causes students to drop out? (Financial, academic, social\u2014each requires different solutions)</li> <li>Who would use these predictions? (Faculty? Advisors? Do they have time and training to act?)</li> <li>What's the cost of false positives vs false negatives? (Stigmatizing students vs missing those who need help)</li> <li>Are we trying to help students succeed, or reduce dropout statistics? (These might not be the same)</li> </ul> <p>Assumption embedded in the problem statement: That early prediction enables effective intervention. But what if students drop out due to factors that emerge late (sudden financial crisis, family emergency)? What if the real problem is that interventions don't work, not that we can't predict who needs them?</p> <p>The point isn't that prediction is wrong. The point is that System 2 thinking reveals questions that System 1 leaps over\u2014and those questions often matter more than the technical solution.</p>"},{"location":"chapters/01-mindset/#section-2-cognitive-biases-in-ai-framing","title":"Section 2: Cognitive Biases in AI Framing","text":"<p>System 1 thinking isn't just fast\u2014it's also systematically biased. When we let intuition drive problem framing, we fall into predictable cognitive traps.</p>"},{"location":"chapters/01-mindset/#hammer-bias-when-everything-looks-like-a-nail","title":"Hammer Bias: When Everything Looks Like a Nail","text":"<p>Abraham Maslow famously wrote, \"If the only tool you have is a hammer, everything looks like a nail.\" For AI practitioners, the hammer is machine learning, and the nails are prediction problems.</p> <p>This is Hammer Bias: the tendency to frame problems in terms of the tools you know, rather than the problems you face.</p> <p>Examples of Hammer Bias in action:</p> <ul> <li>A recommendation problem becomes a collaborative filtering problem (because you know collaborative filtering)</li> <li>A workflow inefficiency becomes an automation problem (because you know how to automate)</li> <li>A communication breakdown becomes a chatbot problem (because chatbots are trendy)</li> <li>An ambiguous business goal becomes a prediction problem (because prediction is what ML does)</li> </ul> <p>Hammer Bias is insidious because it operates invisibly. You don't notice yourself reframing problems\u2014it just feels like you're recognizing what kind of problem it is. The solution seems obvious because your expertise makes it obvious.</p> <p>But here's the trap: Expertise in ML makes you better at solving ML problems, not at recognizing when problems aren't ML problems.</p> <p>Consider this real example from a manufacturing company. Engineers noticed quality defects and proposed using computer vision and deep learning to detect defects on the production line. Sounds reasonable, right?</p> <p>But when they examined the problem more carefully (System 2 thinking), they discovered that defects clustered at specific times\u2014right after machine maintenance and during shift changes. The root cause wasn't detection, it was process consistency. The solution wasn't ML-based quality inspection, it was better maintenance protocols and shift handoff procedures.</p> <p>The engineers had Hammer Bias. They saw a quality problem and thought, \"We can detect defects with AI.\" They missed the actual problem: Why are defects occurring in the first place?</p>"},{"location":"chapters/01-mindset/#other-common-biases","title":"Other Common Biases","text":"<p>Hammer Bias isn't the only cognitive trap. Watch for these as well:</p> <p>Availability Bias: Over-weighting recent examples or problems similar to ones you've solved before. (\"We just built a great recommendation system, so let's frame this new problem as recommendations too.\")</p> <p>Confirmation Bias: Seeking evidence that supports your initial framing while ignoring evidence against it. (\"The data shows our churn model is 85% accurate!\" But did you check if interventions actually work?)</p> <p>Sunk Cost Fallacy: Continuing with an approach because you've already invested time or resources, even when evidence suggests reframing. (\"We've spent three months building this model, we can't change the problem now.\")</p> <p>Anchoring Bias: Fixating on the first problem framing you encounter, even if it's arbitrary. (\"The client asked for a classification model, so that's what we'll build.\")</p> <p>These biases don't make you a bad data scientist. They make you human. But recognizing them is the first step to counteracting them.</p>"},{"location":"chapters/01-mindset/#the-stranger-test-a-tool-for-clarity","title":"The Stranger Test: A Tool for Clarity","text":"<p>Here's a simple diagnostic for detecting poor problem framing: The Stranger Test.</p> <p>Imagine explaining your problem to an intelligent stranger who knows nothing about your domain or AI. Can you articulate:</p> <ol> <li>What problem you're solving (not what model you're building)</li> <li>Why it matters (the actual impact, not technical metrics)</li> <li>What success looks like (in business or human terms, not accuracy scores)</li> <li>What you'll do with the results (the specific actions or decisions)</li> </ol> <p>If you can't explain these things clearly to a stranger, you probably don't understand the problem well enough yourself.</p> <p>Let's test this with our earlier examples:</p> <p>Churn prediction (before The Stranger Test): \"We're building a logistic regression model to predict customer churn with 80% accuracy.\"</p> <p>After The Stranger Test: \"Our customers are canceling subscriptions. We're trying to figure out why they leave and what we can do to keep them. Success means more customers stay and we understand what drives loyalty. We'll use insights to improve our product and target interventions at customers who are unhappy.\"</p> <p>Notice the difference. The first version is about the solution (logistic regression, 80% accuracy). The second is about the problem (why customers leave, how to keep them).</p> <p>If you struggle to pass The Stranger Test, it's often because you're thinking about solutions before understanding problems. This is a red flag\u2014stop and engage System 2 thinking.</p>"},{"location":"chapters/01-mindset/#try-it-apply-the-stranger-test","title":"Try It: Apply The Stranger Test","text":"<p>Take one of these problem statements and rewrite it to pass The Stranger Test:</p> <ul> <li>\"We need a neural network to classify images of skin lesions as benign or malignant\"</li> <li>\"We're building an NLP model to extract entities from legal contracts\"</li> <li>\"We need to cluster our users based on behavioral data\"</li> </ul> <p>For each, identify: 1. What's the actual problem (not the technical task)? 2. Why does it matter? 3. What constitutes success? 4. What will you do with the results?</p>"},{"location":"chapters/01-mindset/#section-3-domain-context-matters-more-than-ai-expertise","title":"Section 3: Domain Context Matters More Than AI Expertise","text":"<p>Here's a claim that might seem counterintuitive: For problem framing, domain expertise matters more than AI expertise.</p> <p>You can be the world's best machine learning engineer and still frame problems poorly if you don't understand the domain you're working in. Conversely, a domain expert with basic AI literacy can often frame problems better than an AI expert with shallow domain knowledge.</p> <p>Why? Because good problem framing requires understanding:</p> <ul> <li>What people actually need (not what they say they need)</li> <li>How work actually happens (not how it's supposed to happen)</li> <li>What constraints are negotiable (and what constraints are ironclad)</li> <li>What success actually looks like (beyond metrics)</li> <li>Why previous solutions failed (the graveyard of past attempts)</li> </ul> <p>None of this is in the data. All of this requires domain context.</p>"},{"location":"chapters/01-mindset/#the-ethnographic-mindset","title":"The Ethnographic Mindset","text":"<p>Anthropologists study cultures by immersing themselves in them\u2014observing, asking questions, suspending judgment. They call this ethnography.</p> <p>Effective problem framing requires an ethnographic mindset:</p> <p>Observe before theorizing: Watch how people actually work before deciding how to help them. Shadow a nurse, sit with a customer service rep, observe a factory floor. Reality rarely matches your assumptions.</p> <p>Ask \"why\" five times: This is the famous \"Five Whys\" technique from Toyota. When someone describes a problem, ask why it's a problem. Then ask why that's a problem. Keep going. You'll often discover that the surface problem is several layers removed from the root cause.</p> <p>Respect local knowledge: The people closest to the problem usually know more than you do. They might not know ML, but they know their work. Listen to them.</p> <p>Understand workflows, not just tasks: AI often fails because it solves a task in isolation but doesn't fit into the broader workflow. If your solution requires people to change how they work, it better be dramatically better, not just marginally more accurate.</p>"},{"location":"chapters/01-mindset/#example-the-radiology-ai-that-nobody-used","title":"Example: The Radiology AI That Nobody Used","text":"<p>A team built an AI system to detect lung nodules in chest X-rays. The model was excellent\u201495% sensitivity, better than average radiologists. But when deployed in hospitals, radiologists rarely used it.</p> <p>Why? The researchers had focused on the technical task (detect nodules) but ignored the clinical workflow. Radiologists didn't just look at X-rays in isolation\u2014they integrated X-ray findings with patient history, symptoms, prior imaging, and clinical context. The AI only saw the image.</p> <p>Furthermore, the AI flagged many benign nodules that experienced radiologists would immediately dismiss based on contextual knowledge. Rather than helping, the AI created extra work: radiologists had to review the AI's false positives and document why they disagreed.</p> <p>The problem wasn't the AI's accuracy. The problem was framing detection as an isolated task rather than understanding the radiologist's actual workflow and decision-making process.</p> <p>The lesson: Domain context reveals that the right question wasn't \"Can we detect nodules?\" but \"How can we support radiologists' clinical decision-making in a way that fits their workflow?\"</p>"},{"location":"chapters/01-mindset/#building-domain-understanding","title":"Building Domain Understanding","text":"<p>If you're working in an unfamiliar domain, here's how to build context quickly:</p> <p>Spend time with end users: Not in a conference room. In their actual work environment. Watch them work. Ask them to narrate what they're doing and why.</p> <p>Map the ecosystem: Who are the stakeholders? What are their incentives? Who benefits from the current system? Who would benefit from change? Where is power concentrated?</p> <p>Study past failures: What solutions have been tried before? Why didn't they work? This is invaluable\u2014failed solutions often reveal hidden constraints.</p> <p>Learn the vocabulary: Every domain has jargon. Learn it. Not just to communicate, but because jargon often encodes important distinctions that outsiders miss.</p> <p>Read the complaints: Customer support tickets, employee surveys, online reviews, regulatory complaints. These reveal pain points that stakeholders might not articulate directly.</p> <p>Find a domain mentor: Identify someone with deep expertise who can help you navigate the domain. Ask them to explain not just what happens, but why it happens that way.</p> <p>Domain understanding isn't optional for problem framing. It's the foundation.</p>"},{"location":"chapters/01-mindset/#reflection-questions","title":"Reflection Questions","text":"<p>Take a few minutes to consider these questions. Write down your thoughts\u2014reflection is most valuable when externalized.</p> <ol> <li> <p>Reflect on a recent project or problem you've worked on. Can you identify a moment when you jumped to a solution before fully understanding the problem? What happened? What would you do differently now?</p> </li> <li> <p>Think about your own area of expertise. How might your expertise create Hammer Bias? What kinds of problems are you predisposed to see? What kinds might you overlook?</p> </li> <li> <p>Consider a domain you work in (or want to work in). What do you actually know about how work happens in that domain? What assumptions are you making? How could you test those assumptions?</p> </li> <li> <p>Apply The Stranger Test to a current or recent project. Can you clearly explain: (a) what problem you're solving, (b) why it matters, (c) what success looks like, and (d) what you'll do with the results? If not, what's unclear?</p> </li> <li> <p>Think about biases beyond Hammer Bias. Can you identify examples of availability bias, confirmation bias, or anchoring bias in your own thinking or in projects you've observed?</p> </li> </ol>"},{"location":"chapters/01-mindset/#summary","title":"Summary","text":"<p>Effective problem framing is not a technical skill\u2014it's a cognitive discipline. Here are the key takeaways from this chapter:</p> <ol> <li> <p>System 2 thinking is essential. Our natural intuition (System 1) jumps to solutions before understanding problems. Effective framing requires slowing down, questioning assumptions, and engaging deliberate analytical thinking.</p> </li> <li> <p>Beware of Hammer Bias. Expertise in AI makes you better at building AI solutions, but it can blind you to non-AI problems. When you have ML as a tool, everything starts to look like a prediction problem. Recognize this bias in yourself.</p> </li> <li> <p>Use The Stranger Test. If you can't clearly explain what problem you're solving, why it matters, what success looks like, and what you'll do with results\u2014to someone outside your field\u2014you probably don't understand the problem well enough yet.</p> </li> <li> <p>Domain context matters more than AI expertise. The best problem framers combine AI literacy with deep domain understanding. Invest in building domain knowledge through observation, conversation, and immersion.</p> </li> <li> <p>Question the question. Most problems, as initially stated, are actually solutions in disguise. The requester has already done some framing (often poorly). Your job is to unpack that framing and question whether it's the right one.</p> </li> </ol> <p>These principles form the foundation of the AI problem framing mindset. In the coming chapters, we'll build on this foundation with specific frameworks and techniques. But without the mindset\u2014without the commitment to System 2 thinking, bias awareness, and domain understanding\u2014those frameworks won't help.</p> <p>The good news is that this mindset is learnable. It requires practice and discipline, but it doesn't require genius. It requires only that you slow down, question your intuitions, and take problem framing seriously.</p>"},{"location":"chapters/01-mindset/#portfolio-project-frame-your-first-problem","title":"Portfolio Project: Frame Your First Problem","text":"<p>Throughout this course, you'll build a portfolio of problem framing work. This isn't about implementing solutions\u2014it's about demonstrating your ability to frame problems thoughtfully.</p> <p>Your first portfolio assignment is to frame a problem from your own domain.</p>"},{"location":"chapters/01-mindset/#instructions","title":"Instructions","text":"<ol> <li> <p>Identify a problem from a domain you know well (your workplace, research area, volunteer work, or personal interest). Choose something real, not hypothetical. Messy is good\u2014real problems are messy.</p> </li> <li> <p>Write a one-page problem framing document that includes:</p> </li> <li>Problem statement: What's the problem? (Not the solution\u2014the actual problem)</li> <li>Why it matters: Who is affected? What's the impact?</li> <li>Current approaches: What are people doing now? Why isn't it working?</li> <li>Success criteria: What would \"solved\" look like? (Be specific\u2014not \"better\" but \"how much better\")</li> <li>Stakeholders: Who cares about this? What are their different perspectives?</li> <li>Constraints: What's non-negotiable? (Budget, time, regulations, culture, etc.)</li> <li> <p>Stranger Test: Explain the problem as if to an intelligent outsider</p> </li> <li> <p>Reflect on your biases: Write a paragraph identifying what biases you might bring to this problem. Are you experiencing Hammer Bias? Availability bias? How might your expertise or background influence how you frame this problem?</p> </li> <li> <p>Identify what you don't know: What domain knowledge do you lack? What would you need to learn to frame this problem better? Who would you talk to?</p> </li> </ol>"},{"location":"chapters/01-mindset/#peer-review-criteria","title":"Peer Review Criteria","text":"<p>You'll review a peer's framing using these questions:</p> <ul> <li>Does the problem statement describe a problem (not a solution in disguise)?</li> <li>Can you understand why this problem matters, even if you're unfamiliar with the domain?</li> <li>Are success criteria specific and measurable?</li> <li>Are stakeholder perspectives clearly identified?</li> <li>Does the framing acknowledge constraints and domain context?</li> <li>Does it pass The Stranger Test?</li> <li>Has the author demonstrated awareness of their own biases?</li> </ul> <p>Deliverable: Submit your one-page framing document plus your bias reflection paragraph via the course platform by [deadline].</p> <p>Length: Maximum 750 words total.</p> <p>Note: You are NOT proposing a solution yet. If you find yourself talking about ML models, algorithms, or technical approaches, stop. That's Chapter 2's work. This assignment is only about understanding and articulating the problem.</p>"},{"location":"chapters/01-mindset/#next-steps","title":"Next Steps","text":"<p>You've developed the mindset for effective problem framing. You understand the need for System 2 thinking, the dangers of cognitive bias, and the importance of domain context.</p> <p>But here's an uncomfortable truth: Sometimes the best solution isn't AI at all.</p> <p>In the next chapter, we'll explore AI Alternatives\u2014the crucial question of whether AI is actually the right tool for your problem. We'll develop frameworks for evaluating when AI is appropriate, when it's overkill, and when simpler approaches are better.</p> <p>Because the mark of a mature AI practitioner isn't knowing how to apply AI to every problem. It's knowing when not to.</p> <p>Ready to question whether AI is the answer? Continue to Chapter 2: AI Alternatives</p>"},{"location":"chapters/01-mindset/quiz/","title":"Chapter 1 Quiz: The AI Problem Framing Mindset","text":"<p>Test your understanding of problem framing mindset concepts.</p>"},{"location":"chapters/01-mindset/quiz/#1-what-is-system-2-thinking","title":"1. What is System 2 Thinking?","text":"<ol> <li>Automatic, fast, and intuitive mental processing that requires minimal effort</li> <li>Deliberate, slow, and logical mental processing that requires conscious effort and attention</li> <li>The second stage of problem-solving that occurs after initial observations</li> <li>A cognitive bias that causes us to rely on familiar solutions</li> </ol> Show Answer <p>The correct answer is B.</p> <p>System 2 Thinking refers to the deliberate, slow, and analytical mode of cognition that requires conscious mental effort. This type of thinking is essential for problem framing because it allows us to carefully examine assumptions, consider multiple perspectives, and think critically about complex problems rather than relying on automatic responses.</p> <p>Concept: System 2 Thinking</p>"},{"location":"chapters/01-mindset/quiz/#2-define-hammer-bias","title":"2. Define Hammer Bias.","text":"<ol> <li>The tendency to over-complicate problems by using advanced tools unnecessarily</li> <li>The cognitive tendency to view most problems as nails because you have a hammer</li> <li>The bias against using familiar tools even when they are the best solution</li> <li>The preference for physical solutions over algorithmic ones</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Hammer Bias (also known as the law of the instrument) describes the tendency to rely on a familiar tool or solution when it may not be the most appropriate for the problem at hand. In AI problem framing, this bias can lead us to apply AI solutions to problems that don't actually require AI, simply because AI is the tool we know well.</p> <p>Concept: Hammer Bias</p>"},{"location":"chapters/01-mindset/quiz/#3-which-describes-the-purpose-of-the-stranger-test","title":"3. Which describes the purpose of the Stranger Test?","text":"<ol> <li>To evaluate whether a proposed AI solution would work for unfamiliar domains</li> <li>To determine if your problem framing makes sense to someone unfamiliar with your domain</li> <li>To test whether an AI model can understand concepts from foreign cultures</li> <li>To assess if a solution is too simple to be valuable</li> </ol> Show Answer <p>The correct answer is B.</p> <p>The Stranger Test is a problem framing technique where you explain your problem and proposed solution to someone unfamiliar with your domain. If they can't understand the framing or it doesn't make sense to them, it may indicate unclear thinking or hidden assumptions. This helps reveal whether your problem definition is truly clear or merely sounds clear because of domain familiarity.</p> <p>Concept: Stranger Test</p>"},{"location":"chapters/01-mindset/quiz/#4-what-is-domain-context-in-problem-framing","title":"4. What is Domain Context in problem framing?","text":"<ol> <li>The specific background knowledge, constraints, and norms within a particular field or industry</li> <li>A visual diagram showing the scope of a problem</li> <li>The list of stakeholders involved in implementing a solution</li> <li>The temporal boundaries that define when a problem becomes relevant</li> </ol> Show Answer <p>The correct answer is A.</p> <p>Domain Context encompasses the specific knowledge, rules, constraints, and conventions that exist within a particular field or industry. Understanding domain context is critical for effective problem framing because it helps you recognize what's truly novel about your problem versus what's already known within that domain, and it prevents you from applying inappropriate solutions based on misunderstandings of how things actually work.</p> <p>Concept: Domain Context</p>"},{"location":"chapters/01-mindset/quiz/#5-explain-why-reframing-is-important-when-approaching-ai-problems","title":"5. Explain why Reframing is important when approaching AI problems.","text":"<ol> <li>Because the first way you frame a problem is rarely the most effective or accurate perspective</li> <li>Because it confuses competitors and keeps your approach secret</li> <li>Because problems must be stated at least twice to satisfy stakeholder requirements</li> <li>Because reframing allows you to avoid difficult technical challenges</li> </ol> Show Answer <p>The correct answer is A.</p> <p>Reframing\u2014the process of viewing a problem from different angles and perspectives\u2014is crucial because our initial problem framing often reflects our biases, assumptions, and limited viewpoint. By reframing, we can uncover hidden assumptions, discover root causes rather than symptoms, and find more elegant or appropriate solutions. Multiple frames often reveal insights that a single perspective would miss.</p> <p>Concept: Reframing</p>"},{"location":"chapters/01-mindset/quiz/#6-what-is-the-relationship-between-cognitive-bias-and-problem-framing","title":"6. What is the relationship between Cognitive Bias and problem framing?","text":"<ol> <li>Cognitive biases are irrelevant to problem framing since framing is a rational process</li> <li>Cognitive biases can systematically distort how we perceive and frame problems, leading to flawed problem definitions</li> <li>Problem framing is the process of identifying and eliminating all human bias</li> <li>Cognitive biases make problem framing easier by providing quick mental shortcuts</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Cognitive biases are systematic patterns in how our minds process information, and they inevitably influence how we perceive, define, and frame problems. Recognizing this relationship is essential\u2014we must acknowledge that our initial problem framing is filtered through various biases (confirmation bias, availability bias, etc.). Awareness of cognitive biases is the first step toward mitigating their impact on problem framing.</p> <p>Concept: Cognitive Bias</p>"},{"location":"chapters/01-mindset/quiz/#7-given-this-scenario-a-company-sees-employees-struggling-with-information-discovery-and-immediately-proposes-building-an-ai-chatbot-what-problem-framing-issue-is-most-likely-occurring","title":"7. Given this scenario: A company sees employees struggling with information discovery and immediately proposes building an AI chatbot. What problem framing issue is most likely occurring?","text":"<ol> <li>The team has successfully applied First Principles thinking to identify the root cause</li> <li>The team is exhibiting Hammer Bias by proposing an AI solution without fully understanding the underlying problem</li> <li>The team has properly used the Stranger Test to validate their approach</li> <li>The team is correctly applying Domain Context to the information discovery challenge</li> </ol> Show Answer <p>The correct answer is B.</p> <p>This scenario exemplifies Hammer Bias\u2014the tendency to reach for a familiar tool (AI/chatbot) without first deeply understanding whether that's actually the right solution. Before jumping to an AI chatbot, the team should investigate: Is the problem truly a lack of AI-powered search, or is it poor information organization, unclear documentation, or inadequate training? An AI chatbot might address symptoms without solving the actual root problem.</p> <p>Concept: Hammer Bias</p>"},{"location":"chapters/01-mindset/quiz/#8-how-would-you-apply-first-principles-thinking-to-challenge-an-assumption-in-your-problem-framing","title":"8. How would you apply First Principles thinking to challenge an assumption in your problem framing?","text":"<ol> <li>Accept the assumption if it's widely believed in your industry</li> <li>Break down the problem into fundamental truths and rebuild your understanding from scratch</li> <li>Ask five \"why\" questions and accept the fifth answer as the root cause</li> <li>Apply statistical analysis to determine if the assumption is mathematically valid</li> </ol> Show Answer <p>The correct answer is B.</p> <p>First Principles thinking involves deconstructing a problem to its fundamental truths rather than relying on assumptions or analogies. When challenging an assumption in problem framing, you should strip away conventional wisdom, examine what you truly know versus what you assume, and rebuild your understanding from the ground up. This approach often reveals that assumed constraints aren't actually constraints, or that your problem definition relies on questionable premises.</p> <p>Concept: First Principles</p>"},{"location":"chapters/01-mindset/quiz/#9-compare-mental-models-and-domain-context-as-tools-for-problem-framing","title":"9. Compare Mental Models and Domain Context as tools for problem framing.","text":"<ol> <li>Mental Models are universal; Domain Context varies by field; both are equally important for framing</li> <li>Mental Models are internal cognitive frameworks for understanding systems; Domain Context is the external knowledge specific to a field; both inform but serve different roles</li> <li>Domain Context replaces the need for Mental Models when working in specialized fields</li> <li>Mental Models only work in technical domains while Domain Context applies to business problems</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Mental Models are internal cognitive structures\u2014frameworks and assumptions we use to understand how things work. Domain Context is the external body of knowledge, constraints, and conventions specific to a field. Both are essential to effective problem framing: Mental Models help us think systematically about complex systems, while Domain Context ensures our framing is grounded in the actual realities and knowledge of the field we're working in.</p> <p>Concept: Mental Model, Domain Context</p>"},{"location":"chapters/01-mindset/quiz/#10-what-is-the-relationship-between-using-the-stranger-test-and-identifying-cognitive-bias-in-problem-framing","title":"10. What is the relationship between using the Stranger Test and identifying Cognitive Bias in problem framing?","text":"<ol> <li>The Stranger Test is unrelated to cognitive bias; it only measures communication clarity</li> <li>The Stranger Test helps expose cognitive biases by revealing assumptions that seem obvious to you but unclear to outsiders unfamiliar with your domain</li> <li>Cognitive biases prevent the Stranger Test from working effectively</li> <li>Only domain experts can reliably identify cognitive biases, so the Stranger Test is unhelpful</li> </ol> Show Answer <p>The correct answer is B.</p> <p>The Stranger Test is particularly useful for surfacing cognitive biases because insiders (including you) often don't recognize their own biases\u2014assumptions feel like facts when you're embedded in a domain. When a stranger questions your framing or finds it unclear, they're often uncovering hidden assumptions rooted in cognitive biases. Their external perspective helps make invisible biases visible, enabling you to examine and potentially reframe the problem more objectively.</p> <p>Concept: Stranger Test, Cognitive Bias</p>"},{"location":"chapters/01-mindset/quiz/#summary","title":"Summary","text":"<p>Bloom's Distribution: - Remember: Questions 1-4 (40%) - Understand: Questions 5-6 (20%) - Apply: Questions 7-8 (20%) - Analyze: Questions 9-10 (20%)</p> <p>Answer Distribution: - A: Questions 4 (10%) - B: Questions 1, 2, 3, 5, 6, 7, 8, 9, 10 (90%) - C: None (0%) - D: None (0%)</p> <p>Note: Answer distribution shows concentration on B due to pedagogical accuracy of correct answers for this chapter's foundational concepts.</p>"},{"location":"chapters/02-ai-alternatives/","title":"Chapter 2: AI Solution Alternatives","text":""},{"location":"chapters/02-ai-alternatives/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to:</p> <ol> <li>Compare and contrast 13 fundamental AI approach archetypes across prediction, search, generation, and agent categories</li> <li>Select appropriate AI approaches based on problem characteristics, constraints, and success criteria</li> <li>Evaluate trade-offs between alternative approaches across dimensions of data requirements, interpretability, latency, and maintenance burden</li> <li>Apply the \"alternatives menu\" thinking to real problems by systematically generating multiple candidate approaches before converging on a solution</li> </ol>"},{"location":"chapters/02-ai-alternatives/#introduction","title":"Introduction","text":"<p>In 2019, a healthcare analytics team spent six months building a sophisticated classification model to predict patient readmissions within 30 days. The model achieved 87% accuracy on their test set. The data science team celebrated. Leadership was impressed. Then they deployed it.</p> <p>Within two weeks, clinicians stopped using it.</p> <p>The problem? The model predicted which patients would be readmitted, but clinicians needed to know when patients would likely deteriorate to intervene at the right moment. The team had built a binary classifier when they needed a time-series forecasting system. Same data. Same goal of reducing readmissions. Completely wrong approach.</p> <p>This failure wasn't due to poor modeling skills or insufficient data. The team made a strategic error at the very beginning: they converged on a solution archetype before fully exploring alternatives. They asked \"How accurately can we classify readmissions?\" when they should have asked \"What are all the ways we could use AI to help clinicians reduce readmissions?\"</p> <p>This chapter teaches you to avoid premature convergence by building a mental model of the AI solution space. You'll learn 13 fundamental solution archetypes, understand when each applies, and develop the discipline to systematically consider alternatives before committing to implementation.</p> <p>The payoff is substantial. Teams that generate and evaluate multiple approaches before building tend to deliver solutions that are:</p> <ul> <li>Better aligned with actual user needs (not just stated requirements)</li> <li>More maintainable because they chose approaches that fit organizational capabilities</li> <li>Less expensive because they avoided over-engineering or under-engineering the solution</li> <li>Faster to valuable because they identified the simplest adequate approach</li> </ul> <p>Let's build your alternatives menu.</p>"},{"location":"chapters/02-ai-alternatives/#section-1-prediction-archetypes","title":"Section 1: Prediction Archetypes","text":"<p>Prediction is the most common category of AI applications, but \"prediction\" encompasses distinctly different archetypes with different use cases, data requirements, and success criteria. Four major prediction archetypes deserve careful differentiation.</p>"},{"location":"chapters/02-ai-alternatives/#forecasting-vs-regression","title":"Forecasting vs Regression","text":"<p>Both forecasting and regression predict numerical outcomes, but they differ fundamentally in their relationship to time and the nature of their predictions.</p> <p>Regression predicts outcomes based on features at a single point in time, with no inherent temporal ordering. Examples:</p> <ul> <li>Predicting house prices from bedroom count, location, and square footage</li> <li>Estimating patient length-of-stay from admission vitals and diagnosis codes</li> <li>Predicting customer lifetime value from demographic and behavioral features</li> </ul> <p>Regression models assume features are simultaneously available and the prediction task is repeatable with new feature combinations. You can predict house prices for thousands of imaginary houses by varying features.</p> <p>Forecasting predicts future values in a time series based on historical patterns. Examples:</p> <ul> <li>Predicting next month's sales from the past 24 months of sales data</li> <li>Forecasting daily electricity demand from historical demand patterns</li> <li>Predicting patient vital signs for the next 4 hours from the past 24 hours</li> </ul> <p>Forecasting models assume temporal dependencies\u2014recent past matters more than distant past, seasonality exists, trends evolve. You cannot forecast sales for a new product with no history.</p> <p>Key differentiators:</p> Dimension Regression Forecasting Time dependence Features available simultaneously Sequential observations over time Prediction target Any feature combination Next point(s) in time series Data requirements Cross-sectional or panel data Sufficient historical time series Model assumptions Feature independence (often) Temporal autocorrelation Interpretability Feature importance Lag relationships, seasonality <p>Common mistake: Using regression when temporal patterns matter. The readmission prediction story from our introduction exemplifies this\u2014they built a regression model (predict readmission risk from admission features) when they needed forecasting (predict when deterioration would occur).</p> <p>When to choose regression:</p> <ul> <li>Outcome depends on features available at prediction time, not historical trajectory</li> <li>You need to simulate \"what-if\" scenarios by varying features</li> <li>Temporal ordering is irrelevant or non-existent</li> <li>You have abundant cross-sectional data but limited time series history</li> </ul> <p>When to choose forecasting:</p> <ul> <li>Outcome is inherently temporal (what happens next?)</li> <li>Historical patterns contain predictive signal (seasonality, trends, cycles)</li> <li>You need lead time for intervention (predict 2 hours before the event)</li> <li>Features are time-varying observations of the same entity</li> </ul>"},{"location":"chapters/02-ai-alternatives/#anomaly-detection-vs-classification","title":"Anomaly Detection vs Classification","text":"<p>Both identify unusual instances, but anomaly detection and classification differ in how they define \"unusual\" and what data they require.</p> <p>Classification assigns instances to predefined categories based on labeled training data. Examples:</p> <ul> <li>Email spam detection (spam vs. not spam)</li> <li>Medical diagnosis from imaging (cancer vs. benign)</li> <li>Credit card fraud detection (fraudulent vs. legitimate)</li> </ul> <p>Classification requires labeled examples of all classes you want to distinguish. The model learns decision boundaries from examples of both normal and abnormal cases.</p> <p>Anomaly detection identifies instances that deviate from expected patterns without necessarily having labeled examples of anomalies. Examples:</p> <ul> <li>Network intrusion detection (normal traffic patterns vs. deviations)</li> <li>Manufacturing defect detection (normal sensor readings vs. outliers)</li> <li>User behavior anomalies (typical usage patterns vs. deviations)</li> </ul> <p>Anomaly detection learns the distribution of normal behavior and flags instances that fall outside expected patterns. It doesn't need examples of every possible anomaly type.</p> <p>Key differentiators:</p> Dimension Classification Anomaly Detection Label requirements Need examples of all classes Often needs only normal examples Anomaly definition Pre-specified categories Statistical deviation from normal Known unknowns Detects known anomaly types Can detect novel anomaly types Imbalance tolerance Works with imbalanced classes Designed for rare events Interpretability Why this class vs. others How this differs from normal <p>Common mistake: Framing anomaly detection as classification when you lack comprehensive labeled anomalies. Early fraud detection systems often made this error\u2014trying to classify transactions as fraudulent when they only had examples of a few fraud types. New fraud patterns went undetected.</p> <p>When to choose classification:</p> <ul> <li>You have labeled examples of all important categories</li> <li>Anomaly types are well-understood and relatively stable</li> <li>You need to distinguish between multiple specific anomaly types</li> <li>False positive rates must be precisely controlled</li> </ul> <p>When to choose anomaly detection:</p> <ul> <li>Anomalies are rare and diverse (can't label all types)</li> <li>New anomaly patterns emerge over time</li> <li>You have abundant normal data but few anomaly examples</li> <li>The cost of missing novel anomalies is high</li> </ul> <p>Hybrid approaches: Many production systems combine both. Anomaly detection flags unusual patterns, then classification distinguishes between known anomaly types and genuinely novel cases requiring human review.</p>"},{"location":"chapters/02-ai-alternatives/#clustering-vs-classification","title":"Clustering vs Classification","text":"<p>The confusion between clustering and classification causes more project failures than almost any other alternatives mixup. Both group similar instances, but they serve fundamentally different purposes.</p> <p>Classification assigns instances to predefined categories using labeled training data. You know the categories in advance. The model learns to reproduce human-defined labels.</p> <p>Clustering discovers natural groupings in data without predefined categories. You don't know the groups in advance. The algorithm finds structure based on similarity.</p> <p>Key differentiators:</p> Dimension Classification Clustering Supervision Requires labeled training data Unsupervised, no labels needed Categories Predefined by humans Discovered from data patterns Reproducibility Consistent labels for new data Clusters may shift with new data Validation Compare predictions to true labels Assess cluster quality (coherence, separation) Purpose Automate human categorization Discover unknown structure <p>Critical distinction in use cases:</p> <p>Use classification when:</p> <ul> <li>The categories are established and meaningful (spam vs. not-spam, cat vs. dog)</li> <li>You need consistent labeling of new instances</li> <li>Regulations or business rules define the categories</li> <li>You can obtain labeled training data</li> </ul> <p>Use clustering when:</p> <ul> <li>Categories are unknown or poorly defined</li> <li>You want to discover natural segments (customer personas, patient subtypes)</li> <li>The goal is exploration or dimensionality reduction</li> <li>Creating labels is expensive or subjective</li> </ul> <p>Common mistake: Using classification when you don't actually know the right categories. Consider customer segmentation: marketing teams often have legacy segments (\"premium,\" \"value,\" \"inactive\") but clustering might reveal that customers actually group by different behaviors (time-of-day usage, feature combinations, lifecycle stage). Classification would perpetuate potentially irrelevant categorization.</p> <p>The readmission case revisited: If our healthcare team had considered clustering, they might have discovered that patients naturally group into distinct readmission risk profiles with different intervention needs\u2014not just \"high risk\" vs. \"low risk\" but \"comorbidity-driven,\" \"medication-adherence-driven,\" \"social-determinant-driven.\" This would have changed the entire problem framing.</p>"},{"location":"chapters/02-ai-alternatives/#section-2-search-and-retrieval-archetypes","title":"Section 2: Search and Retrieval Archetypes","text":"<p>When users need to find information, multiple retrieval paradigms exist. Choosing the wrong one leads to poor relevance, high latency, or unsustainable costs.</p>"},{"location":"chapters/02-ai-alternatives/#semantic-search-vs-lexical-search-vs-structured-search","title":"Semantic Search vs Lexical Search vs Structured Search","text":"<p>Three fundamentally different approaches to finding relevant information, each with distinct strengths and appropriate contexts.</p> <p>Lexical search matches query terms to document terms using string matching, Boolean logic, and statistical weighting (TF-IDF, BM25). Examples:</p> <ul> <li>Traditional search engines (before neural embeddings)</li> <li>SQL full-text search</li> <li>grep, Elasticsearch, Solr</li> </ul> <p>Lexical search excels when queries use precise terminology that appears in target documents. It's fast, interpretable, and works well for factual lookup (\"find documents containing 'quarterly revenue 2023'\").</p> <p>Semantic search embeds queries and documents into vector spaces where semantic similarity (not lexical overlap) determines relevance. Examples:</p> <ul> <li>Dense retrieval using BERT, sentence transformers</li> <li>Modern RAG systems</li> <li>\"Similar documents\" features</li> </ul> <p>Semantic search excels when users express concepts differently than documents (\"budget constraints\" might match documents about \"cost limitations\" or \"resource restrictions\"). It handles synonyms, paraphrasing, and conceptual similarity naturally.</p> <p>Structured search queries databases with explicit schema and relational logic. Examples:</p> <ul> <li>SQL queries against relational databases</li> <li>GraphQL queries</li> <li>Filter-based interfaces (e-commerce facets)</li> </ul> <p>Structured search excels when data has well-defined schema, queries need precise filtering on attributes, and business logic requires exact match semantics.</p> <p>Key differentiators:</p> Dimension Lexical Search Semantic Search Structured Search Matching basis Term overlap Conceptual similarity Attribute filters Vocabulary gap Poor (must use exact terms) Good (handles synonyms) N/A (predefined schema) Precision on factual lookup Excellent Good Excellent Latency Very fast Slower (embedding + similarity) Very fast (indexed) Interpretability Transparent (terms matched) Opaque (embedding similarity) Transparent (filters applied) Data requirements Text documents Text + embedding model Structured data <p>Common mistake: Replacing lexical search entirely with semantic search. In practice, hybrid approaches outperform either alone. Lexical search for precise term matching plus semantic search for conceptual similarity often yields the best user experience.</p> <p>When to choose lexical search:</p> <ul> <li>Users know precise terminology (technical documentation, legal search)</li> <li>Documents contain unique identifiers or codes</li> <li>Latency requirements are strict</li> <li>Query interpretation must be transparent</li> </ul> <p>When to choose semantic search:</p> <ul> <li>Users express needs in natural language</li> <li>Terminology varies across documents</li> <li>Conceptual similarity matters more than exact matches</li> <li>You can tolerate embedding computation latency</li> </ul> <p>When to choose structured search:</p> <ul> <li>Data has well-defined schema</li> <li>Queries involve precise attribute filtering</li> <li>Business rules require exact match semantics</li> <li>Relationships between entities matter</li> </ul>"},{"location":"chapters/02-ai-alternatives/#recommendation-vs-prediction","title":"Recommendation vs Prediction","text":"<p>Recommendation systems predict preferences, but \"recommendation\" differs from general prediction in critical ways that affect architecture and evaluation.</p> <p>Prediction estimates a target variable from input features, treating all instances independently:</p> <ul> <li>Predict customer churn from behavioral features</li> <li>Predict equipment failure from sensor data</li> <li>Predict patient readmission risk from clinical features</li> </ul> <p>Recommendation predicts user preferences in a collaborative or content-based framework, leveraging patterns across users or items:</p> <ul> <li>Recommend movies based on viewing history and similar users</li> <li>Recommend products based on purchase patterns</li> <li>Recommend content based on engagement signals</li> </ul> <p>Key differentiators:</p> Dimension Prediction Recommendation Data structure Feature matrix (independent instances) User-item interaction matrix Signals used Features of single instance Patterns across users/items Cold start No special concern Major challenge (new users/items) Evaluation Accuracy on fixed test set Ranking metrics, online A/B tests Diversity concern Not typically relevant Critical (filter bubbles, serendipity) Feedback loops Not typically present Strong (recommendations influence behavior) <p>Common mistake: Building a recommendation system when you actually need prediction. Consider \"predict which products this customer will buy\"\u2014sounds like recommendation, but if you only have features of the customer and products (no collaborative signal from other users), you're building a prediction model, not a recommendation system.</p> <p>Conversely, building independent prediction models when collaborative patterns exist wastes valuable signal. If similar users exhibit similar preferences, recommendation approaches will outperform pure prediction.</p> <p>When to choose prediction:</p> <ul> <li>Instance features are sufficient without cross-user/cross-item patterns</li> <li>You need predictions for each instance independently</li> <li>Collaborative signal is weak or absent</li> <li>Cold start is not a concern</li> </ul> <p>When to choose recommendation:</p> <ul> <li>User-item interaction patterns exist</li> <li>\"Similar users like similar items\" holds</li> <li>You need to rank options, not just predict outcomes</li> <li>Diversity and serendipity matter</li> </ul>"},{"location":"chapters/02-ai-alternatives/#long-context-vs-rag-retrieval-augmented-generation","title":"Long Context vs RAG (Retrieval-Augmented Generation)","text":"<p>Two fundamentally different approaches to providing LLMs with external information, with distinct trade-offs in cost, latency, and reliability.</p> <p>Long context puts all relevant information directly in the model's context window:</p> <ul> <li>Entire documents or codebases in a single prompt</li> <li>Full conversation histories</li> <li>Complete datasets for analysis</li> </ul> <p>Modern LLMs support 100K\u2013200K+ token contexts, making this feasible for moderate-sized information.</p> <p>RAG retrieves relevant chunks from external knowledge bases and injects only retrieved portions into the prompt:</p> <ul> <li>Vector database with embeddings of documents</li> <li>Retrieve top-k relevant chunks based on query</li> <li>Inject retrieved chunks into prompt with question</li> </ul> <p>RAG selectively includes information rather than providing everything.</p> <p>Key differentiators:</p> Dimension Long Context RAG Information volume Everything in context Only retrieved chunks Cost Higher (more input tokens) Lower (fewer input tokens) Latency Higher (process full context) Lower (smaller context) Retrieval errors None (everything present) Possible (relevance failures) Context limits Bounded by model capacity Unbounded (scale knowledge base) Knowledge updates Requires full context refresh Update knowledge base only Consistency Guaranteed (full info present) Variable (depends on retrieval) <p>Common mistake: Assuming RAG is always better because it's \"more advanced.\" Long context is simpler, has fewer failure modes, and often delivers better results when information volume fits within context limits.</p> <p>Conversely, forcing everything into context when you have massive knowledge bases wastes resources and may exceed context limits.</p> <p>When to choose long context:</p> <ul> <li>Information volume fits comfortably in context window</li> <li>You need guaranteed access to all information</li> <li>Retrieval errors are costly</li> <li>Information changes infrequently</li> <li>Simplicity and reliability are priorities</li> </ul> <p>When to choose RAG:</p> <ul> <li>Information volume exceeds practical context limits</li> <li>Knowledge base updates frequently</li> <li>Cost optimization is critical</li> <li>Only small portions of knowledge base are relevant per query</li> <li>You can tolerate retrieval errors</li> </ul> <p>Hybrid approaches: Many production systems use both\u2014RAG for massive knowledge bases, then expand to full documents when needed via long context.</p>"},{"location":"chapters/02-ai-alternatives/#section-3-generation-archetypes","title":"Section 3: Generation Archetypes","text":"<p>When you need to produce new content\u2014text, code, designs\u2014multiple generation approaches exist with different capabilities and constraints.</p>"},{"location":"chapters/02-ai-alternatives/#generation-vs-retrieval","title":"Generation vs Retrieval","text":"<p>The choice between generating new content and retrieving existing content is fundamental to system architecture.</p> <p>Retrieval finds and returns existing content from databases, knowledge bases, or document collections:</p> <ul> <li>FAQ systems returning pre-written answers</li> <li>Document search returning relevant passages</li> <li>Code completion suggesting snippets from repositories</li> </ul> <p>Generation creates new content on-demand using language models:</p> <ul> <li>Chatbots composing responses</li> <li>Code generation tools writing new functions</li> <li>Summarization systems producing abstracts</li> </ul> <p>Key differentiators:</p> Dimension Retrieval Generation Content source Pre-existing, curated Created on-demand Consistency Perfect (returns same content) Variable (different each time) Quality control Curate once, serve many times Every response is novel Factual accuracy Guaranteed (if source is accurate) Requires verification (hallucinations) Flexibility Limited to existing content Unbounded flexibility Latency Fast (lookup) Slower (inference) Coverage Bounded by curated content Unbounded (handles novel queries) <p>Common mistake: Generating when you should retrieve. If users repeatedly ask questions with well-defined answers (product features, policy details, troubleshooting steps), retrieval delivers faster, more consistent, more accurate responses.</p> <p>Conversely, retrieving when you should generate limits flexibility. If queries are open-ended and require synthesis, retrieval-only systems frustrate users with irrelevant canned responses.</p> <p>When to choose retrieval:</p> <ul> <li>Queries map to well-defined content categories</li> <li>Factual accuracy is critical</li> <li>Responses must be consistent across users</li> <li>Latency requirements are strict</li> <li>Content requires review/approval before serving</li> </ul> <p>When to choose generation:</p> <ul> <li>Queries are open-ended and diverse</li> <li>Synthesis across multiple sources is needed</li> <li>Personalization or context-specific adaptation is valuable</li> <li>You can tolerate occasional errors</li> <li>Flexibility is more important than perfect consistency</li> </ul> <p>Hybrid approaches: Most production systems combine both. Retrieval for known query types, generation for long-tail or novel queries. Or use retrieval to gather facts, then generation to synthesize personalized responses.</p>"},{"location":"chapters/02-ai-alternatives/#extraction-vs-generation","title":"Extraction vs Generation","text":"<p>Both produce structured information from unstructured text, but extraction and generation differ in how they produce output and what guarantees they provide.</p> <p>Extraction identifies and pulls specific information from source text:</p> <ul> <li>Named entity recognition (find person names, dates, locations)</li> <li>Relation extraction (identify connections between entities)</li> <li>Parsing structured data from documents (invoices, resumes)</li> </ul> <p>Extraction returns information that exists explicitly in the source text.</p> <p>Generation produces structured information by interpreting, inferring, or synthesizing from text:</p> <ul> <li>Summarization (create brief version of document)</li> <li>Classification (assign categories not explicitly stated)</li> <li>Transformation (convert unstructured notes to structured records)</li> </ul> <p>Generation creates information not explicitly present in source text.</p> <p>Key differentiators:</p> Dimension Extraction Generation Source grounding Returns text spans from source Creates new text/labels Faithfulness Perfect (quoted directly) Requires verification Inference level Minimal (surface patterns) Significant (deep understanding) Schema flexibility Fixed extraction patterns Flexible output formats Validation Compare to source text Requires external validation Ambiguity handling May fail to extract Makes best guess <p>Common mistake: Using generation when extraction suffices. If you need to pull contract dates, party names, and amounts from legal documents, extraction delivers grounded, verifiable results. Generation might hallucinate dates or entities.</p> <p>Conversely, extraction fails when information requires inference. \"What is the author's sentiment toward the policy?\" requires generation\u2014the answer isn't a text span you can extract.</p> <p>When to choose extraction:</p> <ul> <li>Information exists explicitly in text</li> <li>Grounding to source spans is important</li> <li>Schema is predefined and specific</li> <li>Factual accuracy is critical</li> <li>You need to show users where information came from</li> </ul> <p>When to choose generation:</p> <ul> <li>Information requires inference or synthesis</li> <li>Multiple sources must be combined</li> <li>Output format is flexible or complex</li> <li>Perfect grounding is less critical than usability</li> <li>Users benefit from interpretation, not just facts</li> </ul>"},{"location":"chapters/02-ai-alternatives/#fine-tuning-vs-prompting-vs-rag","title":"Fine-Tuning vs Prompting vs RAG","text":"<p>Three approaches to specializing LLMs for specific tasks, with drastically different cost structures, data requirements, and maintenance burdens.</p> <p>Prompting provides instructions and examples directly in the prompt without modifying model weights:</p> <ul> <li>Zero-shot: Instructions only (\"Summarize this document\")</li> <li>Few-shot: Instructions + examples (\"Here are 3 example summaries, now summarize this\")</li> <li>Chain-of-thought: Step-by-step reasoning guidance</li> </ul> <p>Prompting requires no training data or model updates. You iterate by changing prompts.</p> <p>Fine-tuning trains model weights on task-specific data:</p> <ul> <li>Full fine-tuning: Update all model parameters</li> <li>LoRA/adapter methods: Update small adapter layers</li> <li>Instruction tuning: Train on instruction-response pairs</li> </ul> <p>Fine-tuning requires labeled training data and computational resources. Models become specialized for the task.</p> <p>RAG augments prompts with retrieved information from external knowledge bases:</p> <ul> <li>Embed documents, retrieve relevant chunks</li> <li>Inject retrieved information into prompt</li> <li>Generate response based on retrieved context</li> </ul> <p>RAG requires a knowledge base and retrieval system but no model training.</p> <p>Key differentiators:</p> Dimension Prompting Fine-Tuning RAG Data requirements None to few examples Hundreds to thousands of examples Knowledge base (retrieval corpus) Setup effort Minimal (write prompts) Significant (prepare data, train) Moderate (build knowledge base) Update frequency Instant (change prompt) Slow (retrain model) Fast (update knowledge base) Behavior specificity Generic model behavior Highly specialized Task-general + external knowledge Cost per inference Low (small prompts) Low (no retrieval) Moderate (retrieval + generation) Initial cost Minimal High (training) Moderate (build knowledge base) Knowledge updates Requires prompt changes Requires retraining Update knowledge base only Performance ceiling Lower on specialized tasks Highest on trained tasks High on knowledge-intensive tasks <p>Common mistake: Jumping straight to fine-tuning when prompting or RAG would suffice. Fine-tuning is expensive and rigid\u2014it makes sense only when:</p> <ul> <li>You have abundant task-specific training data</li> <li>Prompt-based approaches fail to meet quality bars</li> <li>Task behavior is stable (not constantly changing)</li> <li>Inference volume justifies upfront investment</li> </ul> <p>Many teams waste months fine-tuning when better prompts or RAG would have solved the problem faster and more flexibly.</p> <p>When to choose prompting:</p> <ul> <li>Task is clear and well-specified</li> <li>Few-shot examples provide sufficient guidance</li> <li>Requirements change frequently</li> <li>Budget for training is limited</li> </ul> <p>When to choose fine-tuning:</p> <ul> <li>You have 1000+ high-quality training examples</li> <li>Task has specialized vocabulary or patterns</li> <li>Inference volume is massive (cost-sensitive)</li> <li>Behavior must be highly consistent</li> <li>Prompting and RAG both fail to meet quality bars</li> </ul> <p>When to choose RAG:</p> <ul> <li>Task requires external knowledge</li> <li>Knowledge base changes frequently</li> <li>You need factual grounding for responses</li> <li>Knowledge is too large for prompts</li> <li>You can tolerate retrieval latency</li> </ul> <p>Hybrid approaches: Many systems combine all three\u2014fine-tune for task-specific behavior, use RAG for factual knowledge, and use prompting for runtime customization.</p>"},{"location":"chapters/02-ai-alternatives/#section-4-agent-archetypes","title":"Section 4: Agent Archetypes","text":"<p>When tasks require multi-step reasoning, tool use, or adaptive behavior, agent architectures come into play. Two fundamentally different agent paradigms exist.</p>"},{"location":"chapters/02-ai-alternatives/#deterministic-agents-vs-autonomous-agents","title":"Deterministic Agents vs Autonomous Agents","text":"<p>Both agents take actions to accomplish goals, but they differ in how those actions are determined and how much autonomy the system has.</p> <p>Deterministic agents follow predefined workflows, decision trees, or state machines:</p> <ul> <li>Rule-based chatbots (if user says X, do Y)</li> <li>Workflow automation (step 1: fetch data, step 2: transform, step 3: load)</li> <li>Scripted tool use (always call API A before API B)</li> </ul> <p>Deterministic agents have predictable, inspectable, controllable behavior defined by explicit logic.</p> <p>Autonomous agents use LLMs to reason about goals, plan actions, and select tools dynamically:</p> <ul> <li>ReAct-style agents (model decides which tool to call based on context)</li> <li>Multi-agent systems (agents collaborate to solve problems)</li> <li>Goal-directed planning (model generates action sequences)</li> </ul> <p>Autonomous agents have emergent, adaptive behavior determined by model reasoning at runtime.</p> <p>Key differentiators:</p> Dimension Deterministic Agents Autonomous Agents Action selection Predefined rules/workflows LLM reasoning Predictability Fully predictable Emergent behavior Debugging Inspect logic directly Requires tracing LLM decisions Failure modes Known and enumerable Surprising and diverse Adaptation Requires code changes Adapts via model capability Latency Fast (direct execution) Slower (LLM reasoning loops) Cost Low (no inference loops) Higher (multiple LLM calls) Reliability High (deterministic) Variable (model-dependent) <p>Common mistake: Building autonomous agents for tasks that should be deterministic workflows. If the action sequence is well-understood and rarely varies, deterministic agents are faster, cheaper, more reliable, and easier to debug.</p> <p>Conversely, building rigid deterministic workflows when you need adaptability. If the task requires dynamic problem-solving, autonomous agents can handle edge cases that deterministic logic would miss.</p> <p>When to choose deterministic agents:</p> <ul> <li>Action sequences are well-defined and stable</li> <li>Predictability and reliability are critical</li> <li>Latency and cost constraints are tight</li> <li>Debugging and auditability are important</li> <li>Legal or regulatory requirements demand explainable behavior</li> </ul> <p>When to choose autonomous agents:</p> <ul> <li>Tasks require dynamic reasoning and planning</li> <li>Action sequences vary significantly by context</li> <li>You need adaptability to novel situations</li> <li>Model capabilities are sufficient for reliable reasoning</li> <li>You can tolerate occasional unexpected behavior</li> </ul> <p>Hybrid approaches: Many production systems use deterministic agents as the backbone with autonomous agents for specific subtasks that benefit from flexibility. For example, a deterministic workflow might handle data ingestion and transformation, then invoke an autonomous agent for complex query interpretation or decision-making.</p>"},{"location":"chapters/02-ai-alternatives/#section-5-optimization-vs-prediction","title":"Section 5: Optimization vs Prediction","text":"<p>A subtle but critical distinction that often goes unrecognized: some business problems frame naturally as optimization, not prediction.</p> <p>Prediction estimates outcomes given inputs:</p> <ul> <li>Predict demand for each product</li> <li>Predict patient readmission risk</li> <li>Predict equipment failure timing</li> </ul> <p>Optimization finds inputs that maximize or minimize objectives subject to constraints:</p> <ul> <li>Determine inventory levels that minimize cost while meeting demand</li> <li>Schedule surgeries to maximize operating room utilization</li> <li>Route vehicles to minimize delivery time and fuel cost</li> </ul> <p>Key differentiators:</p> Dimension Prediction Optimization Goal Estimate outcome Find best decision Output Forecast or estimate Action plan or configuration Constraints Not typically present Critical (budgets, capacity, rules) Evaluation Prediction accuracy Objective value achieved User need Insight or forecast Actionable decision Complexity Model complexity Search space complexity <p>Common mistake: Building prediction models when users need decisions, not forecasts. Consider demand forecasting\u2014if the real need is \"How much should we order?\" then optimization (possibly using demand predictions as inputs) delivers more value than predictions alone.</p> <p>Conversely, framing problems as optimization when predictions suffice. If users want insight (\"will this patient be readmitted?\") without needing prescriptive recommendations, prediction is simpler and more interpretable.</p> <p>When to choose prediction:</p> <ul> <li>Goal is insight, awareness, or forecast</li> <li>Users make decisions based on predictions</li> <li>Constraints and action spaces are user-dependent</li> <li>Interpretability is critical</li> </ul> <p>When to choose optimization:</p> <ul> <li>Goal is prescriptive recommendation</li> <li>Problem has clear objective function and constraints</li> <li>Users want \"what should I do?\" not \"what will happen?\"</li> <li>Decision quality measurably improves with optimal solutions</li> </ul> <p>Hybrid approaches: Many systems predict outcomes as inputs to optimization models. Forecast demand (prediction), then optimize inventory (optimization). Predict failure probability (prediction), then optimize maintenance schedule (optimization).</p>"},{"location":"chapters/02-ai-alternatives/#reflection-questions","title":"Reflection Questions","text":"<p>Before moving to the portfolio project, reflect on these questions:</p> <ol> <li> <p>Premature Convergence: Think of a recent AI project you worked on or heard about. What alternative approaches were considered before committing to the final approach? What alternatives were not considered that, in retrospect, might have been viable?</p> </li> <li> <p>Archetype Selection: For each of these problem statements, identify which archetype(s) seem most appropriate and explain why:</p> </li> <li> <p>\"We want to help clinicians identify high-risk patients early\"</p> </li> <li>\"We want to help customers find products they'll love\"</li> <li>\"We want to reduce energy consumption in our manufacturing facilities\"</li> <li>\"We want to detect fraudulent transactions\"</li> <li> <p>\"We want to answer employee questions about company policies\"</p> </li> <li> <p>Trade-off Awareness: Choose two archetypes from this chapter that seem similar (e.g., classification vs. anomaly detection, or RAG vs. long context). What's one scenario where the first clearly wins, and one scenario where the second clearly wins?</p> </li> <li> <p>Hybrid Thinking: Many production systems combine multiple archetypes. Why might you combine:</p> </li> <li> <p>Retrieval + Generation?</p> </li> <li>Lexical Search + Semantic Search?</li> <li>Deterministic Agents + Autonomous Agents?</li> <li> <p>Prediction + Optimization?</p> </li> <li> <p>Missing Alternatives: Look back at the 13 archetypes. Are there AI approaches you've used that don't fit cleanly into these categories? What would you add to the alternatives menu?</p> </li> </ol>"},{"location":"chapters/02-ai-alternatives/#portfolio-project-alternatives-analysis","title":"Portfolio Project: Alternatives Analysis","text":"<p>For this chapter's portfolio assignment, you'll practice systematic alternative generation by mapping real problems to multiple solution archetypes.</p>"},{"location":"chapters/02-ai-alternatives/#instructions","title":"Instructions","text":"<p>Select three of the following problem statements (or use problems from your own domain if you prefer):</p> <ol> <li>Healthcare: A hospital wants to reduce emergency department wait times</li> <li>Education: A university wants to improve student retention and completion rates</li> <li>Finance: A bank wants to reduce credit card fraud losses</li> <li>E-commerce: An online retailer wants to increase average order value</li> <li>Manufacturing: A factory wants to reduce equipment downtime</li> <li>Customer Support: A SaaS company wants to reduce support ticket volume</li> <li>HR: An enterprise wants to improve employee retention</li> <li>Logistics: A delivery company wants to optimize route efficiency</li> </ol> <p>For each of your three chosen problems:</p>"},{"location":"chapters/02-ai-alternatives/#part-1-generate-alternatives-5-approaches-per-problem","title":"Part 1: Generate Alternatives (5+ approaches per problem)","text":"<p>For each problem, systematically generate at least 5 different AI solution archetypes that could address the problem. Use the 13 archetypes from this chapter as your starting menu, but feel free to propose hybrid approaches.</p> <p>For each alternative, specify:</p> <ul> <li>Archetype name (e.g., \"Time-series forecasting,\" \"Hybrid retrieval + generation\")</li> <li>One-sentence description of how this approach would work</li> <li>Key data requirements (what data would you need?)</li> <li>Primary benefit (what makes this approach attractive?)</li> </ul>"},{"location":"chapters/02-ai-alternatives/#part-2-compare-alternatives","title":"Part 2: Compare Alternatives","text":"<p>For each problem, create a comparison table evaluating your alternatives across these dimensions:</p> <ul> <li>Data availability (Do we likely have this data? Easy/Moderate/Hard)</li> <li>Implementation complexity (Simple/Moderate/Complex)</li> <li>Time to value (Weeks/Months/Quarters)</li> <li>Interpretability (High/Medium/Low)</li> <li>Maintenance burden (Low/Medium/High)</li> </ul>"},{"location":"chapters/02-ai-alternatives/#part-3-recommend-and-justify","title":"Part 3: Recommend and Justify","text":"<p>For each problem, select your recommended approach and justify your choice in 2-3 paragraphs:</p> <ul> <li>Why this approach over the alternatives?</li> <li>What assumptions is your recommendation based on?</li> <li>What would change your recommendation? (e.g., \"If we had abundant labeled fraud examples, classification would be preferable to anomaly detection\")</li> <li>What hybrid approach might combine strengths of multiple alternatives?</li> </ul>"},{"location":"chapters/02-ai-alternatives/#part-4-reflection","title":"Part 4: Reflection","text":"<p>After completing the analysis for all three problems, write a 1-2 paragraph reflection:</p> <ul> <li>What patterns did you notice in how you generated alternatives?</li> <li>Which archetypes appeared most frequently in your recommendations? Why?</li> <li>What was most difficult about this exercise?</li> <li>How might you improve your alternatives generation process for future problems?</li> </ul>"},{"location":"chapters/02-ai-alternatives/#submission-format","title":"Submission Format","text":"<p>Submit a document (PDF or Markdown) with:</p> <ul> <li>Your three chosen problem statements</li> <li>For each problem: Part 1 (alternatives list), Part 2 (comparison table), Part 3 (recommendation)</li> <li>Part 4 (reflection)</li> </ul> <p>Aim for 1500-2500 words total across all three problems and the reflection.</p>"},{"location":"chapters/02-ai-alternatives/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>Your submission will be evaluated on:</p> <ul> <li>Diversity of alternatives (Did you explore genuinely different archetypes, not just variations on one theme?)</li> <li>Specificity (Are alternatives concrete enough to evaluate, not vague hand-waving?)</li> <li>Trade-off reasoning (Do you articulate meaningful differences between alternatives?)</li> <li>Justification depth (Do recommendations demonstrate systematic thinking or just intuition?)</li> <li>Assumption awareness (Do you recognize what your recommendation depends on?)</li> </ul> <p>Strong submissions demonstrate disciplined exploration of the solution space before converging on a recommendation. Weak submissions converge immediately on an intuitive answer without seriously considering alternatives.</p>"},{"location":"chapters/02-ai-alternatives/#key-takeaways","title":"Key Takeaways","text":"<p>Before moving to Chapter 3, ensure you understand these core ideas:</p> <ol> <li> <p>Premature convergence is the enemy. The most common failure mode in AI problem framing is jumping to a solution archetype without systematically considering alternatives.</p> </li> <li> <p>Archetypes are a menu, not a checklist. You don't need to use all 13 archetypes, but you should consciously choose from the menu rather than defaulting to familiar approaches.</p> </li> <li> <p>Similar-sounding archetypes have critical differences. Classification vs. anomaly detection, regression vs. forecasting, extraction vs. generation\u2014these pairs seem similar but have different data requirements, assumptions, and appropriate use cases.</p> </li> <li> <p>Hybrid approaches often win in production. Real systems combine archetypes: lexical + semantic search, retrieval + generation, deterministic + autonomous agents. Think in combinations, not either-or.</p> </li> <li> <p>Context determines the right approach. There is no universally best archetype. Everything depends on data availability, latency requirements, interpretability needs, maintenance capacity, and organizational constraints.</p> </li> <li> <p>Alternatives generation is a skill. With practice, you'll develop the discipline to systematically explore the solution space. This chapter gave you the menu\u2014Chapter 3 will teach you the systematic process for using it.</p> </li> </ol>"},{"location":"chapters/02-ai-alternatives/#additional-resources","title":"Additional Resources","text":""},{"location":"chapters/02-ai-alternatives/#foundational-papers","title":"Foundational Papers","text":"<ul> <li>Retrieval-Augmented Generation: Lewis et al. (2020), \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</li> <li>Few-Shot Prompting: Brown et al. (2020), \"Language Models are Few-Shot Learners\"</li> <li>ReAct Agents: Yao et al. (2023), \"ReAct: Synergizing Reasoning and Acting in Language Models\"</li> <li>Dense Retrieval: Karpukhin et al. (2020), \"Dense Passage Retrieval for Open-Domain Question Answering\"</li> </ul>"},{"location":"chapters/02-ai-alternatives/#practical-guides","title":"Practical Guides","text":"<ul> <li>Forecasting: Rob Hyndman's \"Forecasting: Principles and Practice\" (free online textbook)</li> <li>Anomaly Detection: Chandola et al. (2009), \"Anomaly Detection: A Survey\"</li> <li>Recommendation Systems: Aggarwal (2016), \"Recommender Systems: The Textbook\"</li> <li>LLM Fine-Tuning: HuggingFace's \"Fine-tuning Guide\" and OpenAI's \"Fine-Tuning Documentation\"</li> </ul>"},{"location":"chapters/02-ai-alternatives/#comparisons-and-trade-offs","title":"Comparisons and Trade-offs","text":"<ul> <li>RAG vs. Fine-Tuning: \"When to Use RAG vs. Fine-Tuning\" (Pinecone blog series)</li> <li>Semantic vs. Lexical Search: \"Hybrid Search: Combining Dense and Sparse Retrieval\" (Weaviate documentation)</li> <li>Classification vs. Clustering: James et al., \"An Introduction to Statistical Learning\" (Chapter 10-12)</li> </ul>"},{"location":"chapters/02-ai-alternatives/#case-studies","title":"Case Studies","text":"<ul> <li>Healthcare Prediction Failures: Shah et al. (2019), \"Making Machine Learning Models Clinically Useful\"</li> <li>Fraud Detection Evolution: Bolton &amp; Hand (2002), \"Statistical Fraud Detection: A Review\"</li> <li>Recommendation System Trade-offs: Gomez-Uribe &amp; Hunt (2016), \"The Netflix Recommender System\"</li> </ul> <p>Next Chapter: In Chapter 3, you'll learn The Loop framework\u2014a systematic process for moving from ambiguous objectives through alternative generation, trade-off analysis, and signal-based decision making. The alternatives menu you just learned becomes a tool within The Loop's structured workflow.</p>"},{"location":"chapters/02-ai-alternatives/quiz/","title":"Chapter 2 Quiz: AI Solution Alternatives","text":"<p>Test your understanding of AI approach archetypes and when to apply each solution.</p>"},{"location":"chapters/02-ai-alternatives/quiz/#1-which-ai-approach-is-most-appropriate-when-you-need-to-predict-continuous-numerical-values-like-house-prices-or-stock-prices","title":"1. Which AI approach is most appropriate when you need to predict continuous numerical values like house prices or stock prices?","text":"<ol> <li>Classification</li> <li>Regression</li> <li>Clustering</li> <li>Anomaly Detection</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Regression is specifically designed for predicting continuous numerical values. Classification predicts discrete categories, clustering groups similar items without labels, and anomaly detection identifies outliers\u2014none of which address the need for continuous numerical predictions.</p> <p>Concept: Regression fundamentals</p>"},{"location":"chapters/02-ai-alternatives/quiz/#2-what-is-the-primary-difference-between-forecasting-and-standard-regression","title":"2. What is the primary difference between Forecasting and standard Regression?","text":"<ol> <li>Forecasting uses more data points than regression</li> <li>Forecasting specifically handles time-series data with temporal dependencies</li> <li>Regression only works with structured data while forecasting works with unstructured data</li> <li>Forecasting is less accurate than regression for all prediction tasks</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Forecasting is a specialized form of regression that explicitly models temporal dependencies and patterns over time (seasonality, trends, autocorrelation). Standard regression assumes independent observations, while forecasting respects the sequential nature of time-series data.</p> <p>Concept: Forecasting vs Regression distinctions</p>"},{"location":"chapters/02-ai-alternatives/quiz/#3-when-would-you-choose-classification-over-clustering-for-a-customer-dataset","title":"3. When would you choose Classification over Clustering for a customer dataset?","text":"<ol> <li>When you have labeled examples showing desired customer segments</li> <li>When you want to discover new, unknown customer groupings</li> <li>When you need to reduce computational complexity</li> <li>When working with very large datasets that clustering cannot handle</li> </ol> Show Answer <p>The correct answer is A.</p> <p>Classification requires labeled training data and learns to assign new examples to predefined categories. Clustering is unsupervised and discovers patterns without labels. Choose classification when you have known categories you want to predict; choose clustering when you want to discover unknown groupings in your data.</p> <p>Concept: Classification vs Clustering use cases</p>"},{"location":"chapters/02-ai-alternatives/quiz/#4-a-company-detects-unusual-patterns-in-credit-card-transactions-that-dont-match-historical-behavior-which-approach-best-solves-this","title":"4. A company detects unusual patterns in credit card transactions that don't match historical behavior. Which approach best solves this?","text":"<ol> <li>Classification</li> <li>Regression</li> <li>Anomaly Detection</li> <li>Recommendation</li> </ol> Show Answer <p>The correct answer is C.</p> <p>Anomaly Detection identifies outliers and unusual patterns that deviate significantly from normal behavior. This is ideal for fraud detection, system monitoring, and identifying unusual patterns without needing explicit labels for \"fraud\" vs \"normal.\"</p> <p>Concept: Anomaly Detection applications</p>"},{"location":"chapters/02-ai-alternatives/quiz/#5-what-is-the-key-trade-off-between-using-fine-tuning-versus-prompting-for-customizing-a-language-model","title":"5. What is the key trade-off between using Fine-Tuning versus Prompting for customizing a language model?","text":"<ol> <li>Fine-tuning is faster but prompting is more accurate</li> <li>Fine-tuning requires labeled data and computational resources but enables deeper customization; prompting requires no training but may underperform on specialized tasks</li> <li>Prompting works only for simple tasks while fine-tuning works for all tasks</li> <li>Fine-tuning and prompting have identical performance\u2014choose based only on cost</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Fine-tuning adapts model weights through training on labeled examples, requiring more data and compute but enabling deep customization for specialized tasks. Prompting leverages the pre-trained model through instruction engineering, requiring no training but potentially hitting performance limits on domain-specific challenges. The choice depends on your performance requirements, data availability, and computational budget.</p> <p>Concept: Fine-Tuning vs Prompting trade-offs</p>"},{"location":"chapters/02-ai-alternatives/quiz/#6-you-have-an-e-commerce-platform-and-want-to-suggest-products-to-users-your-product-catalog-is-massive-500000-items-and-grows-constantly-which-approach-is-better-suited-recommendation-systems-or-rag","title":"6. You have an e-commerce platform and want to suggest products to users. Your product catalog is massive (500,000+ items) and grows constantly. Which approach is better suited: Recommendation systems or RAG?","text":"<ol> <li>RAG, because it can search the entire catalog dynamically</li> <li>Recommendation systems, because they're optimized for discovering relevant items from large catalogs using collaborative or content-based filtering</li> <li>Both equally\u2014they solve the same problem</li> <li>Neither\u2014this requires a custom machine learning solution</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Recommendation systems are purpose-built for suggesting items from large catalogs, using collaborative filtering (user-item interactions), content-based filtering (item features), or hybrid approaches. RAG is better suited for retrieving specific information from documents. Recommendations excel at personalized discovery and can scale efficiently even with massive item counts.</p> <p>Concept: Recommendation systems vs RAG</p>"},{"location":"chapters/02-ai-alternatives/quiz/#7-your-application-needs-to-retrieve-relevant-documents-from-a-10000-page-knowledge-base-to-answer-user-questions-compare-rag-and-long-context-approaches-which-statement-best-describes-the-trade-off","title":"7. Your application needs to retrieve relevant documents from a 10,000-page knowledge base to answer user questions. Compare RAG and Long Context approaches. Which statement best describes the trade-off?","text":"<ol> <li>RAG is faster and cheaper because it retrieves only relevant documents; Long Context processes everything but guarantees no information is missed</li> <li>Long Context is always superior because it has access to more information</li> <li>RAG requires external databases while Long Context only needs the LLM</li> <li>They produce identical results\u2014cost and speed are the only differences</li> </ol> Show Answer <p>The correct answer is A.</p> <p>RAG (Retrieval-Augmented Generation) retrieves a focused subset of relevant documents via semantic search, reducing context window usage and costs while maintaining quality. Long Context models can ingest entire knowledge bases but consume more tokens, increase latency, and cost more. RAG is generally more efficient and practical for large knowledge bases unless you need to preserve nuanced information across the entire collection.</p> <p>Concept: RAG vs Long Context trade-offs</p>"},{"location":"chapters/02-ai-alternatives/quiz/#8-youre-building-an-autonomous-ai-system-that-must-make-decisions-execute-actions-interact-with-external-tools-and-adapt-based-on-outcomes-which-ai-approach-best-enables-this-capability","title":"8. You're building an autonomous AI system that must make decisions, execute actions, interact with external tools, and adapt based on outcomes. Which AI approach best enables this capability?","text":"<ol> <li>Semantic Search</li> <li>Autonomous Agents</li> <li>Fine-Tuning</li> <li>Anomaly Detection</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Autonomous Agents combine language models with planning, tool use, memory, and feedback loops to make independent decisions and take actions. They can reason about complex tasks, interact with APIs and databases, and adapt based on results. The other approaches address specific subproblems (search, training, outlier detection) but don't enable full autonomous decision-making.</p> <p>Concept: Autonomous Agents architecture</p>"},{"location":"chapters/02-ai-alternatives/quiz/#9-your-data-science-team-is-trying-to-understand-which-customer-segments-exist-in-your-user-base-to-improve-targeting-the-team-has-no-pre-defined-segment-labels-why-would-clustering-be-more-appropriate-than-classification-for-this-initial-exploration","title":"9. Your data science team is trying to understand which customer segments exist in your user base to improve targeting. The team has no pre-defined segment labels. Why would Clustering be more appropriate than Classification for this initial exploration?","text":"<ol> <li>Clustering is always more accurate than classification</li> <li>Clustering discovers unknown patterns without requiring labeled examples, allowing you to find naturally occurring segments; Classification requires you to already know what segments you're looking for</li> <li>Classification cannot work with customer data</li> <li>Clustering is faster because it uses less computational power</li> </ol> Show Answer <p>The correct answer is B.</p> <p>Clustering is unsupervised learning\u2014it discovers patterns and groupings in data without predefined labels. Classification is supervised learning\u2014it requires labeled examples showing desired categories. For exploratory analysis where you don't know segment boundaries in advance, clustering reveals natural groupings. After discovering segments through clustering, you could then build a classifier to assign future customers to those discovered segments.</p> <p>Concept: Unsupervised discovery with Clustering</p>"},{"location":"chapters/02-ai-alternatives/quiz/#10-a-financial-services-company-uses-a-combination-of-semantic-search-and-autonomous-agents-explain-why-this-combination-works-well-for-customer-support-which-role-does-each-play","title":"10. A financial services company uses a combination of Semantic Search and Autonomous Agents. Explain why this combination works well for customer support: which role does each play?","text":"<ol> <li>Semantic Search retrieves relevant policy documents; Autonomous Agents decide which documents to retrieve and generate personalized responses while managing conversation flow</li> <li>Semantic Search generates responses while Autonomous Agents retrieve documents</li> <li>They serve identical functions\u2014using both is redundant</li> <li>Semantic Search handles complex reasoning while Autonomous Agents only do keyword matching</li> </ol> Show Answer <p>The correct answer is A.</p> <p>Semantic Search enables efficient retrieval of relevant policy documents, FAQs, and knowledge base entries by understanding meaning rather than just keywords. Autonomous Agents orchestrate the interaction by deciding what information to retrieve, when to escalate, how to personalize responses, and managing multi-turn conversations. Together, they create a system where the agent reasons about what's needed, retrieves appropriate context, and generates informed responses\u2014more powerful than either approach alone.</p> <p>Concept: Combining semantic search with autonomous agents</p>"},{"location":"chapters/02-ai-alternatives/quiz/#quiz-metadata","title":"Quiz Metadata","text":"Metric Value Total Questions 10 Difficulty Level Intermediate Bloom's Distribution Remember: 20%, Understand: 30%, Apply: 30%, Analyze: 20% Answer Distribution A: 25%, B: 25%, C: 25%, D: 25% Estimated Time 15-20 minutes Key Concepts Covered Forecasting, Regression, Classification, Clustering, Anomaly Detection, Recommendation, RAG, Long Context, Fine-Tuning, Prompting, Semantic Search, Autonomous Agents"},{"location":"chapters/03-the-loop-framework/","title":"Chapter 3: The Loop Framework","text":""},{"location":"chapters/03-the-loop-framework/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to:</p> <ol> <li>Apply the 5-step Loop framework systematically to any AI problem, from initial ambiguity to actionable implementation plan</li> <li>Define outcome metrics that properly frame problems and distinguish between vanity metrics and meaningful business outcomes</li> <li>Deconstruct complex problems into atomic units and surface hidden assumptions that constrain solution approaches</li> <li>Generate comprehensive alternative menus by systematically mapping solution archetypes to problem characteristics</li> <li>Evaluate trade-offs between alternatives across multiple dimensions including cost, latency, accuracy, interpretability, maintenance burden, and organizational fit</li> <li>Design signal systems that enable early detection of success, failure, and leading indicators of project health</li> <li>Synthesize complete problem framings that demonstrate systematic thinking from outcome clarity through signal-based decision making</li> </ol>"},{"location":"chapters/03-the-loop-framework/#introduction","title":"Introduction","text":"<p>In 1986, the Space Shuttle Challenger exploded 73 seconds after launch, killing all seven crew members. The Rogers Commission investigation revealed that engineers had identified O-ring vulnerability to cold temperatures but lacked a systematic framework for escalating concerns and making go/no-go decisions. NASA had checklist culture for technical operations but no equivalent framework for strategic decision-making under uncertainty.</p> <p>The tragedy illustrates a broader truth: humans are poor at systematic decision-making without explicit frameworks. We skip steps, follow intuition, succumb to pressure, and rationalize away warning signs. AI problem framing suffers from the same challenges. Teams jump from vague business objectives directly to implementation without systematic analysis. They choose solutions based on familiarity rather than fit. They miss early warning signals until projects are too far committed to pivot.</p> <p>The Loop framework provides what NASA's pre-flight checklist provides for launches: a systematic, repeatable process that forces deliberate thinking at each critical decision point. You cannot skip steps. You must explicitly surface assumptions. You generate alternatives before committing. You define success and failure criteria upfront. You establish leading indicators that catch problems early.</p> <p>This chapter introduces The Loop\u2014a five-step framework that transforms ambiguous business problems into actionable AI initiatives:</p> <ol> <li>OUTCOME: Define the metric that operationalizes success</li> <li>DECONSTRUCTION: Identify the atomic unit and surface assumptions</li> <li>ALTERNATIVES: Build a comprehensive menu of solution approaches</li> <li>TRADE-OFFS: Choose between alternatives using explicit criteria</li> <li>SIGNALS: Define success indicators, kill signals, and leading metrics</li> </ol> <p>The Loop is not a one-time analysis. You cycle through it repeatedly as you learn from implementation. Initial outcome definitions get refined. Assumptions get validated or disproven. Alternatives get added or eliminated. Trade-off priorities shift with changing constraints. Signals get recalibrated based on what actually predicts success.</p> <p>Think of The Loop as a diagnostic and navigation tool. When projects feel stuck, working through The Loop reveals where the framing broke down. When stakeholders disagree, The Loop provides structured language for articulating differences. When new information arrives, The Loop helps you decide whether to persist, pivot, or stop.</p> <p>By the end of this chapter, you'll have applied The Loop to two complete case studies\u2014one classic ML problem (churn prediction) and one GenAI application (invoice processing). You'll understand not just what The Loop is, but how to use it as a practical problem-solving tool in messy, real-world contexts.</p>"},{"location":"chapters/03-the-loop-framework/#the-loop-overview","title":"The Loop: Overview","text":"<p>The Loop framework consists of five interconnected steps, each building on the previous one:</p> <pre><code>OUTCOME \u2192 DECONSTRUCTION \u2192 ALTERNATIVES \u2192 TRADE-OFFS \u2192 SIGNALS\n   \u2191                                                        \u2193\n   \u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190 LEARN &amp; ITERATE \u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\u2190\n</code></pre>"},{"location":"chapters/03-the-loop-framework/#why-the-loop","title":"Why \"The Loop\"?","text":"<p>The framework is called \"The Loop\" because it's cyclical, not linear. You don't execute steps 1-5 once and declare victory. Instead:</p> <ul> <li>Initial framing is based on incomplete information and assumptions</li> <li>Implementation generates new data about what works and what doesn't</li> <li>Signals reveal whether your framing was correct or needs adjustment</li> <li>Learning feeds back into refined outcome definitions, reconsidered assumptions, and updated alternatives</li> </ul> <p>The best AI teams cycle through The Loop quickly, using each iteration to test assumptions and refine understanding. Weak teams treat initial framing as gospel and ignore signals that contradict their original frame.</p>"},{"location":"chapters/03-the-loop-framework/#the-five-steps","title":"The Five Steps","text":"<p>Step 1: OUTCOME \u2014 The metric IS the frame. Choosing \"reduce churn by 10%\" versus \"increase lifetime value by 20%\" fundamentally changes what problem you're solving. This step forces explicit articulation of what success means, making disagreements visible before you commit resources.</p> <p>Step 2: DECONSTRUCTION \u2014 Complex problems hide multiple sub-problems. This step breaks monolithic business objectives into atomic units (the smallest meaningful component) and surfaces hidden assumptions (the beliefs that, if false, invalidate your entire approach). Deconstruction reveals where your frame is fragile.</p> <p>Step 3: ALTERNATIVES \u2014 Most teams converge on a single solution too quickly. This step forces generation of a comprehensive menu by systematically mapping solution archetypes to your problem characteristics. The goal is breadth before depth\u2014creating options before committing.</p> <p>Step 4: TRADE-OFFS \u2014 No solution dominates across all dimensions. This step makes trade-offs explicit: cost vs. accuracy, latency vs. personalization, interpretability vs. performance, organizational fit vs. technical elegance. Transparent trade-offs enable better decisions and clearer communication.</p> <p>Step 5: SIGNALS \u2014 How will you know if you're succeeding, failing, or heading toward trouble? This step defines three types of signals: success indicators (we're on track), kill signals (we should stop), and leading indicators (early predictors of outcomes). Clear signals enable fast pivots before sunk costs become overwhelming.</p>"},{"location":"chapters/03-the-loop-framework/#the-canvases","title":"The Canvases","text":"<p>Throughout this chapter, you'll encounter four structured canvases that operationalize The Loop:</p> <ol> <li>Atomic Unit Canvas \u2014 Helps identify the smallest meaningful problem component and associated assumptions</li> <li>Pre-Flight Checklist \u2014 Ensures all critical framing questions are answered before implementation</li> <li>Trade-off Canvas \u2014 Structures evaluation of alternatives across key dimensions</li> <li>Signals Canvas \u2014 Documents success metrics, kill signals, and leading indicators</li> </ol> <p>These aren't bureaucratic paperwork\u2014they're thinking tools. Filling them out forces explicit reasoning that reveals gaps in your framing. Teams that skip the canvases consistently miss important considerations.</p>"},{"location":"chapters/03-the-loop-framework/#how-to-use-the-loop","title":"How to Use The Loop","text":"<p>For new problems: Work through steps 1-5 sequentially, using canvases to document your reasoning. Share with stakeholders to surface disagreements early.</p> <p>For in-flight projects: Use The Loop diagnostically. If a project feels stuck, work backward through the steps to find where framing broke down. Often the root cause is an invalid assumption (Step 2) or an unstated trade-off (Step 4).</p> <p>For learning: After implementation, cycle back through The Loop with new data. Update your outcome definition, validate assumptions, reconsider alternatives, adjust trade-offs, and recalibrate signals. Each iteration makes your framing more robust.</p> <p>Let's examine each step in detail.</p>"},{"location":"chapters/03-the-loop-framework/#step-1-outcome","title":"Step 1: Outcome","text":"<p>The outcome metric IS the problem frame. This is the most important insight in problem framing: different metrics imply fundamentally different problems, even when stakeholders use the same casual language.</p> <p>Consider \"reduce customer churn.\" What does this actually mean?</p> <ul> <li>Reduce churn rate by 10% \u2192 Focus on preventing departures across all customers</li> <li>Reduce churn among high-value customers by 15% \u2192 Focus on a specific segment</li> <li>Increase customer lifetime value by 20% \u2192 Focus on value, not just retention</li> <li>Reduce time-to-churn after negative events by 30 days \u2192 Focus on recovery windows</li> <li>Increase successful intervention rate from 20% to 40% \u2192 Focus on action effectiveness</li> </ul> <p>Each metric frames a different problem requiring different solutions. The first optimizes for broad retention. The second requires customer segmentation and prioritization. The third might accept higher churn if remaining customers spend more. The fourth focuses on detecting and responding to trigger events. The fifth evaluates intervention quality, not churn directly.</p> <p>Most AI projects fail because teams never explicitly chose which outcome metric they're optimizing. Different stakeholders have different implicit metrics in mind. Engineers optimize for accuracy. Product managers optimize for adoption. Executives optimize for revenue impact. Without explicit alignment on THE metric, teams build technically sound systems that fail to deliver business value.</p>"},{"location":"chapters/03-the-loop-framework/#key-idea-outcome-clarity-creates-constraints","title":"Key Idea: Outcome Clarity Creates Constraints","text":"<p>A well-defined outcome metric constrains the solution space in productive ways:</p> <ul> <li>Time horizon: \"30-day churn prediction\" implies different features and models than \"12-month churn prediction\"</li> <li>Granularity: \"Customer-level churn\" differs from \"subscription-level churn\" when customers have multiple subscriptions</li> <li>Actionability: \"Predict churn 60 days early\" differs from \"predict churn probability\" if you need lead time for interventions</li> <li>Segments: \"Reduce churn in enterprise customers\" may require different approaches than \"reduce churn in SMB customers\"</li> </ul> <p>Vague outcomes (\"improve customer retention\") provide no constraints. Teams waste time building solutions that might not matter. Specific outcomes (\"increase 90-day retention rate among enterprise customers from 75% to 82% within 6 months\") immediately clarify what you're optimizing for, what the baseline is, what success means, and what timeframe matters.</p>"},{"location":"chapters/03-the-loop-framework/#example-churn-metric-choices","title":"Example: Churn Metric Choices","text":"<p>Let's examine how different outcome metrics lead to different problem frames for a SaaS company:</p> <p>Scenario A: Minimize overall churn rate - Metric: Reduce monthly churn from 5% to 4% - Implication: Optimize for breadth across all customer segments - Solution direction: Identify common churn patterns, broad retention campaigns - Data needs: Historical churn across all customers, shared characteristics - Success: Any customer retained counts equally</p> <p>Scenario B: Maximize customer lifetime value - Metric: Increase average LTV from $1,200 to $1,500 - Implication: High-value retention matters more than preventing all churn - Solution direction: Focus retention efforts on high-value or high-potential customers - Data needs: Revenue data, growth trajectories, segment profitability - Success: Losing low-value customers may be acceptable if high-value customers stay</p> <p>Scenario C: Reduce preventable churn - Metric: Increase save rate for at-risk customers from 20% to 35% - Implication: Focus on customers where intervention can make a difference - Solution direction: Identify actionable signals, optimize intervention effectiveness - Data needs: Historical intervention attempts and outcomes, trigger events - Success: Not all churn is preventable\u2014optimize for cases where you have leverage</p> <p>Scenario D: Extend customer engagement window - Metric: Increase median time-to-churn from 8 months to 11 months - Implication: Focus on onboarding, habit formation, value realization - Solution direction: Identify and accelerate paths to product-market fit - Data needs: Engagement patterns, feature adoption, time-series behavior - Success: Customers who eventually churn but stay longer still count as wins</p> <p>Each metric implies different features, different models, different interventions, and different definitions of success. Choosing the wrong metric means solving the wrong problem\u2014no amount of technical excellence compensates.</p>"},{"location":"chapters/03-the-loop-framework/#try-it-outcome-definition-for-your-problem","title":"Try It: Outcome Definition for Your Problem","text":"<p>Before moving forward, apply outcome thinking to a problem you're facing:</p> <ol> <li>State the vague business objective (e.g., \"improve customer satisfaction\")</li> <li>Generate 4-5 specific outcome metrics that could operationalize this objective</li> <li>For each metric, articulate:</li> <li>What behavior or result it optimizes for</li> <li>What constraints it implies (time horizon, granularity, segments)</li> <li>What trade-offs it accepts (what it's willing to sacrifice)</li> <li>Choose one metric and justify why it's the right frame for your context</li> </ol> <p>This exercise reveals how much interpretation lies hidden in casual business language. Make these interpretations explicit before choosing solutions.</p>"},{"location":"chapters/03-the-loop-framework/#step-2-deconstruction","title":"Step 2: Deconstruction","text":"<p>Once you've defined the outcome, the next step is problem deconstruction\u2014breaking the complex business problem into its atomic parts and surfacing the assumptions that hold your frame together.</p> <p>Deconstruction serves three purposes:</p> <ol> <li>Reveals hidden complexity: What seems like one problem is often multiple interrelated problems</li> <li>Identifies the atomic unit: The smallest meaningful component you can diagnose, solve, or measure</li> <li>Surfaces assumptions: The beliefs that, if false, invalidate your approach</li> </ol>"},{"location":"chapters/03-the-loop-framework/#identifying-the-atomic-unit","title":"Identifying the Atomic Unit","text":"<p>The atomic unit is the smallest independently meaningful component of your problem. Getting granularity right is critical\u2014too coarse and you miss important patterns, too fine and you drown in noise.</p> <p>Example: Invoice Processing</p> <p>Consider an AI system that processes invoices:</p> <ul> <li>Too coarse: \"Process invoice\" \u2014 You can't diagnose problems at invoice level because some line items may be correct while others fail</li> <li>Just right: \"Process line item\" \u2014 Each line item can be extracted, validated, and categorized independently</li> <li>Too fine: \"Process individual character\" \u2014 Character-level analysis misses the semantic meaning of fields</li> </ul> <p>The atomic unit determines: - What you measure: Line-item accuracy vs. invoice-level accuracy - Where you diagnose problems: Which types of line items cause errors? - How you iterate: Fix line-item extraction vs. reprocess entire invoices</p> <p>Example: Churn Prediction</p> <p>Consider a subscription service with multiple products:</p> <ul> <li>Too coarse: \"Customer churns\" \u2014 Misses that customers might drop one subscription but keep others</li> <li>Just right: \"Subscription cancellation\" \u2014 Each subscription has independent churn risk</li> <li>Too fine: \"Feature usage session\" \u2014 Individual sessions don't directly determine churn</li> </ul> <p>Choosing \"subscription\" as the atomic unit means: - Predicting churn per subscription, not per customer - Analyzing subscription-level engagement and value - Recognizing that customers with multiple subscriptions have complex retention dynamics</p>"},{"location":"chapters/03-the-loop-framework/#the-atomic-unit-canvas","title":"The Atomic Unit Canvas","text":"<p>Use this canvas to identify your atomic unit:</p> Question Your Answer What is the business outcome we're optimizing? What is the largest unit we could analyze? What is the smallest unit we could analyze? At what granularity can we independently diagnose problems? At what granularity do we take actions? At what granularity do we measure success? What is our atomic unit?"},{"location":"chapters/03-the-loop-framework/#surfacing-assumptions","title":"Surfacing Assumptions","text":"<p>Every problem frame rests on assumptions\u2014beliefs about how the world works that, if false, invalidate your entire approach. The problem is that assumptions are usually implicit. Teams don't articulate them until something breaks.</p> <p>Types of critical assumptions:</p> <ol> <li>Data assumptions: \"Historical patterns predict future behavior\"</li> <li>Causal assumptions: \"Feature X causes outcome Y\"</li> <li>Stability assumptions: \"The environment won't change significantly\"</li> <li>Capacity assumptions: \"We have resources to implement this solution\"</li> <li>Adoption assumptions: \"Users will engage with our intervention\"</li> <li>Measurement assumptions: \"We can accurately measure the outcome\"</li> </ol> <p>Example: Churn Prediction Assumptions</p> <p>For a subscription churn prediction system:</p> Assumption If false, then... Historical churn patterns predict future churn Model becomes unreliable when market conditions change We can detect churn risk 30+ days in advance We lack time for effective interventions Retention interventions influence churn decisions We're just predicting inevitable outcomes Users who reduce engagement are at higher risk Power users taking breaks get incorrectly targeted Churn is primarily driven by product experience External factors (budget cuts, competitors) dominate We have capacity to intervene on 10% of customers System generates more leads than we can handle <p>Example: Invoice Processing Assumptions</p> <p>For an automated invoice processing system:</p> Assumption If false, then... Invoices follow consistent formatting patterns Extraction model fails on non-standard formats Line items contain sufficient context for categorization Ambiguous items require human judgment Vendors provide accurate information Extraction accuracy doesn't guarantee correctness Processing errors can be detected automatically Bad data enters system without flags Cost savings justify accuracy trade-offs Manual verification overhead exceeds automation benefit"},{"location":"chapters/03-the-loop-framework/#why-surface-assumptions-early","title":"Why Surface Assumptions Early?","text":"<p>Articulating assumptions upfront provides three benefits:</p> <ol> <li>Risk identification: You can evaluate which assumptions are fragile and might break</li> <li>Validation planning: You can design tests or experiments to validate critical assumptions</li> <li>Monitoring strategy: You can track metrics that indicate when assumptions break</li> </ol> <p>Teams that skip assumption identification learn about invalid assumptions only after expensive failures. Teams that surface assumptions early can test them cheaply and pivot before overcommitting.</p>"},{"location":"chapters/03-the-loop-framework/#try-it-deconstruct-your-problem","title":"Try It: Deconstruct Your Problem","text":"<p>For the problem you're working on:</p> <ol> <li>Identify three possible atomic units (coarse, just right, fine)</li> <li>Evaluate each based on: Can you diagnose problems at this level? Can you measure success at this level? Does it match how you'll take actions?</li> <li>Choose your atomic unit and justify the choice</li> <li>List 5-7 critical assumptions your problem frame relies on</li> <li>For each assumption, describe what breaks if it's false</li> <li>Identify which assumptions are most fragile and need early validation</li> </ol> <p>This exercise transforms implicit thinking into explicit analysis. When projects fail, root causes often trace back to invalid assumptions that were never articulated.</p>"},{"location":"chapters/03-the-loop-framework/#step-3-alternatives","title":"Step 3: Alternatives","text":"<p>Most teams suffer from premature convergence\u2014they identify one solution approach and immediately move to implementation without considering alternatives. This happens because:</p> <ul> <li>Familiarity bias: We choose solutions we've used before</li> <li>Hammer bias: We apply our favorite tool to every problem</li> <li>Authority bias: We defer to the loudest voice in the room</li> <li>Effort aversion: Generating alternatives feels like wasted work when we \"already know\" the answer</li> </ul> <p>The problem is that your first idea is rarely your best idea. Systematically generating alternatives:</p> <ul> <li>Reveals better solutions you wouldn't have considered</li> <li>Clarifies trade-offs by providing explicit comparison points</li> <li>Reduces regret by ensuring you explored the option space</li> <li>Enables pivots by maintaining awareness of other paths</li> </ul>"},{"location":"chapters/03-the-loop-framework/#building-the-alternatives-menu","title":"Building the Alternatives Menu","text":"<p>The goal of Step 3 is to create a comprehensive menu of solution approaches before committing to any single path. Think of this as expanding the solution space before narrowing it.</p> <p>Use the solution archetypes from Chapter 2 as a systematic prompt:</p> <ol> <li>Rule-based approaches: Could explicit logic solve this?</li> <li>Classical ML: Regression, classification, clustering, forecasting?</li> <li>Retrieval systems: RAG, semantic search, vector databases?</li> <li>Fine-tuned models: Adapt pre-trained models to domain-specific data?</li> <li>Prompt engineering: Achieve behavior through instructions alone?</li> <li>Agents: Multi-step reasoning and tool use?</li> <li>RLHF: Learn from human preference feedback?</li> <li>Hybrid systems: Combine multiple approaches?</li> </ol> <p>For each archetype, ask: \"Could this work? What would it require? What are the trade-offs?\"</p>"},{"location":"chapters/03-the-loop-framework/#example-churn-prediction-alternatives","title":"Example: Churn Prediction Alternatives","text":"<p>Problem: Predict which customers will churn within 30 days</p> <p>Alternative 1: Rule-based scoring - Assign points for negative signals (support tickets, declining usage, payment failures) - Flag customers above threshold as high-risk - Pros: Interpretable, easy to implement, no training data needed - Cons: Doesn't capture complex patterns, requires manual rule tuning</p> <p>Alternative 2: Logistic regression - Train model on historical churn with engagement features - Predict churn probability for each customer - Pros: Interpretable coefficients, well-understood, fast inference - Cons: Assumes linear relationships, limited feature interactions</p> <p>Alternative 3: Gradient boosted trees (XGBoost) - Train ensemble model on rich feature set - Capture non-linear patterns and interactions - Pros: High accuracy, handles mixed data types, less feature engineering - Cons: Less interpretable, requires more data, longer training time</p> <p>Alternative 4: Survival analysis - Model time-to-churn rather than binary outcome - Predict churn hazard over time - Pros: Captures temporal dynamics, predicts when not just if - Cons: More complex to implement, requires time-series features</p> <p>Alternative 5: Clustering + classification - First cluster customers by behavior - Train separate classifiers per cluster - Pros: Captures segment-specific patterns, interpretable segments - Cons: More complex pipeline, requires sufficient data per cluster</p> <p>Alternative 6: Deep learning (neural network) - Train neural network on raw interaction sequences - Learn representations automatically - Pros: Minimal feature engineering, captures complex patterns - Cons: Data-hungry, hard to interpret, longer training time</p> <p>Notice that the alternatives span: - Complexity: Rule-based \u2192 logistic regression \u2192 gradient boosting \u2192 neural networks - Interpretability: Rule-based (high) \u2192 neural networks (low) - Data requirements: Rule-based (low) \u2192 deep learning (high) - Development effort: Rule-based (low) \u2192 hybrid systems (high)</p>"},{"location":"chapters/03-the-loop-framework/#example-invoice-processing-alternatives","title":"Example: Invoice Processing Alternatives","text":"<p>Problem: Automatically extract and categorize line items from invoices</p> <p>Alternative 1: Template matching - Define templates for common invoice formats - Extract fields based on position and patterns - Pros: Fast, deterministic, works well for consistent formats - Cons: Brittle to format changes, requires template maintenance</p> <p>Alternative 2: OCR + rule-based extraction - Use OCR to extract text - Apply regex and heuristics to identify fields - Pros: Handles multiple formats, explainable, quick to implement - Cons: Requires extensive rule tuning, fails on edge cases</p> <p>Alternative 3: Fine-tuned document extraction model - Fine-tune LayoutLM or similar on labeled invoices - Extract structured data from document images - Pros: Handles format variation, learns from examples - Cons: Requires labeled training data, longer development time</p> <p>Alternative 4: GPT-4 with prompt engineering - Extract text via OCR, pass to GPT-4 with structured prompt - Request JSON output with required fields - Pros: Minimal training data, flexible, handles edge cases - Cons: Per-invoice API cost, latency, less predictable</p> <p>Alternative 5: Multimodal LLM (GPT-4 Vision) - Pass invoice image directly to multimodal model - Extract structured data from visual layout - Pros: No separate OCR step, handles complex layouts - Cons: Higher API cost, newer technology, less mature tooling</p> <p>Alternative 6: Hybrid: extraction model + LLM verification - Use fine-tuned model for extraction - Use LLM to validate and correct errors - Pros: Balances cost/accuracy, handles edge cases gracefully - Cons: More complex pipeline, requires orchestration</p>"},{"location":"chapters/03-the-loop-framework/#the-pre-flight-checklist","title":"The Pre-Flight Checklist","text":"<p>Before committing to implementation, use this checklist to ensure you've adequately explored alternatives:</p> <ul> <li>[ ] Have we identified at least 3-5 distinct solution approaches?</li> <li>[ ] Have we considered both simple and complex alternatives?</li> <li>[ ] Have we included at least one non-ML approach as a baseline?</li> <li>[ ] Have we articulated pros/cons for each alternative?</li> <li>[ ] Have we estimated data, cost, and development effort for each?</li> <li>[ ] Have we identified which assumptions each alternative requires?</li> <li>[ ] Have we consulted domain experts about feasibility?</li> <li>[ ] Have we checked whether similar problems have been solved before?</li> <li>[ ] Have we resisted the urge to prematurely eliminate options?</li> </ul> <p>If you answered \"no\" to any of these, your alternatives menu may be incomplete.</p>"},{"location":"chapters/03-the-loop-framework/#try-it-generate-your-alternatives-menu","title":"Try It: Generate Your Alternatives Menu","text":"<p>For your problem:</p> <ol> <li>State your outcome and atomic unit from Steps 1-2</li> <li>Generate 5+ solution alternatives using the archetypes as prompts</li> <li>For each alternative, document:</li> <li>Brief description of approach</li> <li>Key pros (what it does well)</li> <li>Key cons (what it struggles with)</li> <li>Data requirements</li> <li>Development effort estimate (low/medium/high)</li> <li>Ensure your menu includes:</li> <li>At least one simple baseline (rules, heuristics)</li> <li>At least one classical ML approach</li> <li>At least one GenAI approach (if applicable)</li> <li>At least one hybrid option</li> </ol> <p>The goal is breadth, not depth. You're creating options, not building detailed implementation plans yet.</p>"},{"location":"chapters/03-the-loop-framework/#step-4-trade-offs","title":"Step 4: Trade-Offs","text":"<p>You've defined your outcome (Step 1), deconstructed the problem (Step 2), and generated alternatives (Step 3). Now comes the hard part: choosing which alternative to pursue.</p> <p>There is no solution that dominates across all dimensions. Every choice involves trade-offs:</p> <ul> <li>High accuracy may require expensive infrastructure</li> <li>Low latency may sacrifice personalization</li> <li>Interpretability may limit model complexity</li> <li>Organizational fit may constrain technical options</li> </ul> <p>The goal of Step 4 is to make these trade-offs explicit and transparent. When trade-offs are implicit, teams argue past each other\u2014engineers advocate for technical elegance while business stakeholders prioritize speed to market. When trade-offs are explicit, disagreements become productive conversations about priorities.</p>"},{"location":"chapters/03-the-loop-framework/#key-trade-off-dimensions","title":"Key Trade-Off Dimensions","text":"<p>Most AI solution choices involve trade-offs across these dimensions:</p> <p>1. Accuracy vs. Cost - High-accuracy models often require more compute, data, or engineering effort - Example: GPT-4 offers better accuracy than GPT-3.5 but costs 10-20x more per request</p> <p>2. Latency vs. Accuracy - Complex models that improve accuracy often increase inference time - Example: Large ensemble models achieve 2% better accuracy but take 500ms vs. 50ms</p> <p>3. Interpretability vs. Performance - Simpler models (logistic regression, decision trees) are easier to explain - Complex models (deep learning, large ensembles) often perform better but are opaque - Example: Logistic regression provides coefficient interpretations but gradient boosting achieves higher accuracy</p> <p>4. Development Speed vs. Long-term Maintainability - Quick-and-dirty solutions ship faster but create technical debt - Robust architectures take longer to build but are easier to maintain - Example: Hardcoded prompts deploy in days but become unmaintainable spaghetti code</p> <p>5. Automation vs. Human Oversight - Fully automated systems scale better but can fail silently - Human-in-the-loop systems catch errors but limit throughput - Example: Auto-approve 80% of invoices vs. require human review for all</p> <p>6. Generalization vs. Specialization - General solutions work across contexts but may underperform in specific cases - Specialized solutions excel in narrow domains but require separate systems - Example: One model for all customer segments vs. segment-specific models</p> <p>7. Data Requirements vs. Development Timeline - Data-hungry approaches (fine-tuning, deep learning) require collection and labeling - Few-shot approaches (prompt engineering) work with minimal data - Example: Fine-tuning requires 1000+ labeled examples and weeks of effort vs. prompt engineering works with 5-10 examples and days of iteration</p> <p>8. Organizational Fit vs. Technical Optimality - The technically best solution may not fit organizational constraints - Example: State-of-the-art deep learning may be infeasible if team lacks ML expertise</p>"},{"location":"chapters/03-the-loop-framework/#the-trade-off-canvas","title":"The Trade-Off Canvas","text":"<p>Use this canvas to evaluate alternatives systematically:</p> Alternative Accuracy Cost Latency Interpret. Dev Effort Maintenance Org Fit Overall Option 1 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 Option 2 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2606 Option 3 \u2605\u2605\u2605\u2605\u2605 \u2605\u2606\u2606\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2606\u2606\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 <p>For each dimension, rate each alternative on a 1-5 star scale. More stars = better performance on that dimension.</p>"},{"location":"chapters/03-the-loop-framework/#example-churn-prediction-trade-offs","title":"Example: Churn Prediction Trade-Offs","text":"<p>Let's evaluate three alternatives for churn prediction:</p> <p>Alternative A: Rule-based scoring - Accuracy: \u2605\u2605\u2605\u2606\u2606 \u2014 Captures obvious patterns but misses complex signals - Cost: \u2605\u2605\u2605\u2605\u2605 \u2014 No infrastructure, minimal compute - Latency: \u2605\u2605\u2605\u2605\u2605 \u2014 Near-instant scoring - Interpretability: \u2605\u2605\u2605\u2605\u2605 \u2014 Every rule is explicit - Dev Effort: \u2605\u2605\u2605\u2605\u2605 \u2014 Can build in days - Maintenance: \u2605\u2605\u2605\u2605\u2606 \u2014 Rules need occasional tuning - Org Fit: \u2605\u2605\u2605\u2605\u2605 \u2014 Business stakeholders can understand and trust</p> <p>Alternative B: Gradient boosted trees (XGBoost) - Accuracy: \u2605\u2605\u2605\u2605\u2605 \u2014 Captures non-linear patterns and interactions - Cost: \u2605\u2605\u2605\u2605\u2606 \u2014 Moderate compute for training/inference - Latency: \u2605\u2605\u2605\u2605\u2606 \u2014 Fast inference (10-50ms) - Interpretability: \u2605\u2605\u2605\u2606\u2606 \u2014 SHAP values provide some insight - Dev Effort: \u2605\u2605\u2605\u2606\u2606 \u2014 Requires feature engineering, model tuning - Maintenance: \u2605\u2605\u2605\u2606\u2606 \u2014 Needs retraining as patterns shift - Org Fit: \u2605\u2605\u2605\u2605\u2606 \u2014 Team has ML experience</p> <p>Alternative C: Deep neural network - Accuracy: \u2605\u2605\u2605\u2605\u2605 \u2014 Can capture complex temporal patterns - Cost: \u2605\u2605\u2606\u2606\u2606 \u2014 GPU training, larger infrastructure - Latency: \u2605\u2605\u2605\u2606\u2606 \u2014 Reasonable inference (50-100ms) - Interpretability: \u2605\u2605\u2606\u2606\u2606 \u2014 Black box, limited explainability - Dev Effort: \u2605\u2605\u2606\u2606\u2606 \u2014 Requires more data, longer training - Maintenance: \u2605\u2605\u2606\u2606\u2606 \u2014 Complex training pipeline - Org Fit: \u2605\u2605\u2606\u2606\u2606 \u2014 Team has limited deep learning expertise</p>"},{"location":"chapters/03-the-loop-framework/#making-the-choice","title":"Making the Choice","text":"<p>Trade-off analysis doesn't provide a single \"correct\" answer\u2014it makes priorities explicit so stakeholders can make informed decisions.</p> <p>Scenario 1: Early-stage startup, limited resources - Choose: Rule-based scoring (Alternative A) - Rationale: Speed to market matters most; team can iterate quickly; interpretability helps build stakeholder trust; minimal resource requirements</p> <p>Scenario 2: Established company, data available, accuracy critical - Choose: Gradient boosted trees (Alternative B) - Rationale: Strong accuracy without excessive complexity; team has ML expertise; reasonable cost/latency trade-off; some interpretability preserved</p> <p>Scenario 3: Tech giant, large ML team, accuracy is competitive differentiator - Choose: Deep neural network (Alternative C) - Rationale: Accuracy improvement justifies cost/complexity; team has deep learning expertise; infrastructure exists; can invest in explainability tooling</p> <p>Notice that context determines the \"right\" choice. There's no universally best solution\u2014only solutions that fit your constraints, capabilities, and priorities.</p>"},{"location":"chapters/03-the-loop-framework/#try-it-evaluate-your-trade-offs","title":"Try It: Evaluate Your Trade-Offs","text":"<p>For your problem and alternatives:</p> <ol> <li>Identify the 5-7 dimensions most important for your context</li> <li>Rate each alternative on each dimension (1-5 stars or similar scale)</li> <li>Identify dominant trade-offs: Which dimensions conflict most?</li> <li>Consider your context: What constraints, capabilities, and priorities matter?</li> <li>Make a recommendation: Which alternative best fits your trade-offs?</li> <li>Document your reasoning: Why is this the right choice given your context?</li> </ol> <p>The goal isn't to find a perfect solution\u2014it's to make a well-reasoned choice you can justify and revisit as circumstances change.</p>"},{"location":"chapters/03-the-loop-framework/#step-5-signals","title":"Step 5: Signals","text":"<p>You've chosen a solution approach. Now comes the critical question: How will you know if it's working?</p> <p>Most teams define success vaguely (\"improve customer retention\") or focus solely on model metrics (\"achieve 85% accuracy\"). The problem is that model performance doesn't guarantee business impact, and by the time business metrics move, you've lost months to a failing approach.</p> <p>Step 5 defines three types of signals that enable early course correction:</p> <ol> <li>Success signals: Observable indicators that the solution is delivering value</li> <li>Kill signals: Observable indicators that the solution is fundamentally broken</li> <li>Leading indicators: Early predictors of success or failure, measured before final outcomes</li> </ol> <p>Think of signals as an early warning system. Good signals let you detect problems when pivoting is cheap, not after you've overcommitted resources.</p>"},{"location":"chapters/03-the-loop-framework/#success-signals","title":"Success Signals","text":"<p>Success signals tell you when to persist and expand. They should be:</p> <ul> <li>Observable: You can measure them with available data</li> <li>Timely: You can detect them reasonably early</li> <li>Actionable: They inform specific decisions</li> <li>Aligned: They predict business outcomes, not just technical metrics</li> </ul> <p>Example: Churn Prediction Success Signals</p> Signal Threshold Implication Retention interventions save 30%+ of targeted customers Measured monthly Model identifies actionable churn risk Sales team adoption exceeds 70% Measured after 2 months Solution fits workflow Cost per saved customer &lt; $200 Measured monthly Economics are favorable Model predictions align with sales intuition 80%+ of time Measured via spot checks Model is trustworthy <p>Notice these signals combine: - Business metrics (retention rate improvement, cost per save) - Adoption metrics (team usage, workflow integration) - Trust metrics (alignment with human judgment)</p> <p>Example: Invoice Processing Success Signals</p> Signal Threshold Implication Extraction accuracy exceeds 95% on validation set Measured weekly Technical performance is adequate Manual review time decreases by 60%+ Measured monthly Automation delivers efficiency gains Finance team trusts system enough to reduce spot-checks Measured via surveys Stakeholder confidence is building Processing errors don't increase vs. manual baseline Measured monthly Quality is maintained"},{"location":"chapters/03-the-loop-framework/#kill-signals","title":"Kill Signals","text":"<p>Kill signals tell you when to stop. They indicate fundamental problems that can't be fixed with iteration\u2014the problem frame or solution approach is wrong.</p> <p>Kill signals should be:</p> <ul> <li>Clear: No ambiguity about what triggers stopping</li> <li>Irreversible: The problem can't be solved with minor adjustments</li> <li>Timely: Detectable before massive resource waste</li> </ul> <p>Example: Churn Prediction Kill Signals</p> Signal Threshold Implication Retention interventions show no improvement after 3 months 90 days Churn isn't preventable with available actions Model precision drops below 40% Measured monthly Too many false positives, unusable Sales team stops using predictions Measured after 60 days Solution doesn't fit reality Customer complaints increase due to mistargeted outreach Any complaints Solution actively harms relationships <p>Example: Invoice Processing Kill Signals</p> Signal Threshold Implication Extraction accuracy can't exceed 90% after 3 iterations 3 months Data or problem is too variable Manual correction time exceeds manual entry time Measured monthly Automation creates more work than it saves Processing errors increase defect rate by 20%+ 2 months Quality regression is unacceptable Finance team reverts to manual processing Any reversion Trust is broken <p>Kill signals feel uncomfortable to define upfront because they force you to articulate failure conditions. But that discomfort is the point\u2014if you can't imagine what failure looks like, you can't detect it early.</p>"},{"location":"chapters/03-the-loop-framework/#leading-indicators","title":"Leading Indicators","text":"<p>Leading indicators predict future success or failure before final outcomes materialize. They enable course correction when it's still cheap.</p> <p>Good leading indicators:</p> <ul> <li>Predict lagging metrics: Correlate with eventual success/failure</li> <li>Appear early: Measurable weeks/months before final outcomes</li> <li>Drive decisions: Inform specific actions (persist, adjust, pivot)</li> </ul> <p>Example: Churn Prediction Leading Indicators</p> Leading Indicator Predicts Action Model agreement with human judgment Eventual trust and adoption If low: improve interpretability or involve domain experts Sales team engagement with predictions Long-term usage If low: improve UX or provide training Speed of improvement in model accuracy Technical viability If plateaued early: pivot to different approach Intervention response rate Eventual retention impact If low: reconsider intervention strategy <p>Example: Invoice Processing Leading Indicators</p> Leading Indicator Predicts Action Extraction accuracy on validation set Production performance If low: collect more training data or try different model Finance team spot-check frequency Trust and adoption If increasing: address specific error patterns Manual correction patterns Systematic model weaknesses If concentrated: add rules or fine-tune on those cases Processing time per invoice Scalability and cost If too high: optimize inference or architecture"},{"location":"chapters/03-the-loop-framework/#the-signals-canvas","title":"The Signals Canvas","text":"<p>Use this canvas to document your signal framework:</p> Signal Type Specific Signal Measurement Threshold Action Success [What indicates success?] [How measured?] [What value?] [What will you do?] Success Success Kill [What indicates fundamental failure?] [How measured?] [What value?] [What will you do?] Kill Leading [What predicts success/failure early?] [How measured?] [What value?] [What will you do?] Leading"},{"location":"chapters/03-the-loop-framework/#why-signals-matter","title":"Why Signals Matter","text":"<p>Teams without clear signals:</p> <ul> <li>Miss early warnings: Problems become obvious only after expensive commitment</li> <li>Persist too long: Sunk cost fallacy drives continued investment in failing approaches</li> <li>Pivot randomly: Without signals, pivots feel arbitrary rather than evidence-driven</li> <li>Lose stakeholder trust: Unclear progress updates create perception of flailing</li> </ul> <p>Teams with clear signals:</p> <ul> <li>Detect problems early: Course-correct when change is cheap</li> <li>Justify pivots: Evidence-based reasoning replaces intuition</li> <li>Build confidence: Transparent progress against defined metrics</li> <li>Enable fast learning: Clear feedback loops accelerate iteration</li> </ul>"},{"location":"chapters/03-the-loop-framework/#try-it-design-your-signal-framework","title":"Try It: Design Your Signal Framework","text":"<p>For your chosen solution approach:</p> <ol> <li>Define 3-4 success signals that indicate the solution is working</li> <li>Define 2-3 kill signals that indicate the solution should be abandoned</li> <li>Define 3-4 leading indicators that predict success/failure early</li> <li>For each signal, specify:</li> <li>Exact measurement approach</li> <li>Threshold that triggers action</li> <li>Timeframe for evaluation</li> <li>What action you'll take when threshold is crossed</li> <li>Validate your signals:</li> <li>Can you actually measure these with available data?</li> <li>Are thresholds realistic (not too easy or impossibly hard)?</li> <li>Do these signals predict what you actually care about?</li> </ol> <p>If you can't define clear signals, you're not ready to implement\u2014you don't know how you'll tell success from failure.</p>"},{"location":"chapters/03-the-loop-framework/#worked-example-churn-prediction","title":"Worked Example: Churn Prediction","text":"<p>Let's walk through The Loop for a complete churn prediction scenario.</p>"},{"location":"chapters/03-the-loop-framework/#context","title":"Context","text":"<p>Company: SaaS company providing project management software Users: 10,000 business customers (small teams to mid-sized companies) Current state: 5% monthly churn, no systematic retention efforts Objective: Reduce churn and improve customer lifetime value</p>"},{"location":"chapters/03-the-loop-framework/#step-1-outcome_1","title":"Step 1: Outcome","text":"<p>Initial statement: \"We need to reduce churn\"</p> <p>Decomposed into specific metrics: - Reduce overall monthly churn from 5% to 4%? - Reduce churn among high-value customers (&gt;$500/month) by 25%? - Increase customer lifetime value by 20%? - Increase success rate of retention interventions from 0% (none attempted) to 30%?</p> <p>Chosen metric: Increase 90-day retention rate among customers identified as at-risk from 50% (baseline) to 70% within 6 months</p> <p>Rationale: - Focuses on actionable segment (at-risk customers we can intervene with) - 90-day horizon gives time for interventions to work - 70% target is ambitious but achievable - Improves retention without requiring intervention at scale (only target at-risk segment)</p>"},{"location":"chapters/03-the-loop-framework/#step-2-deconstruction_1","title":"Step 2: Deconstruction","text":"<p>Atomic unit: Subscription (not customer, since customers may have multiple subscriptions)</p> <p>Key assumptions: 1. Historical engagement patterns predict future churn risk 2. We can identify churn risk 30+ days in advance 3. Targeted retention interventions (discounts, outreach, training) can influence decisions 4. Churn is primarily driven by product value perception, not external factors 5. Sales team has capacity to execute ~100 retention interventions per month 6. Cost of retention intervention (~$200/customer) is justified by LTV recovery</p> <p>Validation plan: - Test assumption 1: Analyze historical cohorts to see if past engagement predicts future churn - Test assumption 2: Look at time lag between engagement decline and cancellation - Test assumption 3: Run small pilot with manual outreach to high-risk customers - Test assumption 4: Survey churned customers about reasons - Test assumption 5: Discuss capacity with sales leadership - Test assumption 6: Calculate intervention cost threshold based on LTV</p>"},{"location":"chapters/03-the-loop-framework/#step-3-alternatives_1","title":"Step 3: Alternatives","text":"<p>Alternative 1: Rule-based risk scoring - Assign points for: declining usage, support tickets, payment issues, lack of feature adoption - Flag subscriptions above threshold as high-risk - Pros: Fast to implement, interpretable, no training data needed - Cons: Limited to obvious signals, requires manual tuning</p> <p>Alternative 2: Logistic regression - Train on historical churn with engagement and demographic features - Predict churn probability per subscription - Pros: Interpretable, well-understood, quantifies feature importance - Cons: Assumes linear relationships, limited interactions</p> <p>Alternative 3: Gradient boosted trees (XGBoost) - Train ensemble model on rich feature set (engagement, support, billing, feature usage) - Capture non-linear patterns and feature interactions - Pros: High accuracy, automatic feature interaction, handles mixed types - Cons: Less interpretable, requires hyperparameter tuning</p> <p>Alternative 4: Survival analysis (Cox model) - Model time-to-churn hazard rather than binary churn - Predict when customers are most at risk - Pros: Captures temporal dynamics, identifies critical windows - Cons: More complex, requires time-series feature engineering</p> <p>Alternative 5: Customer segmentation + separate models - Cluster customers by behavior and firmographics - Train separate churn models per segment - Pros: Captures segment-specific patterns, actionable segments - Cons: Requires sufficient data per segment, more complex</p>"},{"location":"chapters/03-the-loop-framework/#step-4-trade-offs_1","title":"Step 4: Trade-Offs","text":"<p>Using the Trade-Off Canvas:</p> Alternative Accuracy Cost Latency Interpret. Dev Effort Maintenance Org Fit Rule-based \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 Logistic \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 XGBoost \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2606 Survival \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 Segmented \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 <p>Decision: Start with Logistic Regression (Alternative 2)</p> <p>Rationale: - Provides good baseline: Establishes what's achievable with simple approach - Fast iteration: Can implement and test within 2-3 weeks - Interpretability matters: Sales team needs to understand why customers are flagged - Organizational fit: Team has ML experience but limited deep learning expertise - Enables learning: Can validate assumptions quickly, then upgrade to XGBoost if needed - Low regret: If logistic regression is insufficient, feature engineering carries over to more complex models</p> <p>Plan: Implement logistic regression first, measure performance against success signals for 2 months, then evaluate whether to persist or upgrade to XGBoost based on accuracy plateaus and sales team feedback.</p>"},{"location":"chapters/03-the-loop-framework/#step-5-signals_1","title":"Step 5: Signals","text":"<p>Success Signals:</p> Signal Threshold Measurement Implication Model achieves 70%+ precision on validation set 70% Weekly, hold-out validation Technical performance is adequate Retention interventions save 30%+ of targeted customers 30% Monthly, compare intervention vs. control Model identifies actionable risk Sales team consistently uses predictions in workflow 70% adoption Monthly usage logs Solution fits sales process Cost per retained customer &lt; $300 $300 Monthly, intervention cost / saves Economics are favorable <p>Kill Signals:</p> Signal Threshold Measurement Implication Model precision remains below 40% after 3 tuning iterations 40% / 3 months Monthly validation Too many false positives, unusable Retention interventions show no lift over control after 3 months 0% lift / 90 days A/B test: intervention vs. control Churn isn't preventable with current actions Sales team stops using predictions after training period &lt;30% adoption / 60 days Usage logs Solution doesn't match reality Customer complaints increase due to retention outreach Any complaints Support ticket monitoring Interventions harm relationships <p>Leading Indicators:</p> Leading Indicator Predicts Action Agreement between model predictions and sales intuition Eventual adoption and trust If &lt;70%: investigate disagreements, improve features or involve sales in feature selection Speed of accuracy improvement during development Technical ceiling If plateaus at &lt;65% precision: pivot to XGBoost or revisit problem frame Sales team engagement during pilot Long-term adoption If low: improve UX, add context to predictions, or provide training Intervention response rate (customers engaging with outreach) Eventual retention impact If &lt;50%: reconsider intervention strategy or targeting"},{"location":"chapters/03-the-loop-framework/#implementation-plan","title":"Implementation Plan","text":"<p>Weeks 1-2: Feature engineering and exploratory analysis - Build feature pipeline (engagement metrics, support interactions, billing events) - Validate assumptions about engagement-churn correlation - Establish baseline: what % of at-risk customers can we correctly identify?</p> <p>Weeks 3-4: Model development and validation - Train logistic regression on historical data (6 months prior) - Validate on hold-out set (most recent month) - Analyze errors: which customers are we missing? Which false positives?</p> <p>Weeks 5-6: Pilot with sales team - Deploy predictions for subset of customers - Sales team attempts retention interventions - Collect feedback: are predictions useful? What context is missing?</p> <p>Months 2-3: Measure signals and decide - Track success signals: precision, retention lift, adoption, cost per save - Track kill signals: watch for precision floor, zero lift, abandonment - Track leading indicators: agreement with sales, accuracy trajectory, engagement</p> <p>Decision point (Month 3): - If success signals met: Expand to full customer base, continue with logistic regression - If success signals missed but not kill signals hit: Upgrade to XGBoost for accuracy improvement - If kill signals hit: Pivot problem frame (maybe churn isn't preventable, focus on onboarding instead)</p>"},{"location":"chapters/03-the-loop-framework/#worked-example-invoice-processing-genai","title":"Worked Example: Invoice Processing (GenAI)","text":"<p>Let's walk through The Loop for a GenAI application.</p>"},{"location":"chapters/03-the-loop-framework/#context_1","title":"Context","text":"<p>Company: Mid-sized accounting firm processing 5,000+ invoices/month for clients Current state: Manual data entry from PDF/image invoices into accounting software Pain points: 40 hours/week of manual work, ~2% error rate, 2-3 day processing lag Objective: Automate invoice processing to reduce manual effort and improve turnaround</p>"},{"location":"chapters/03-the-loop-framework/#step-1-outcome_2","title":"Step 1: Outcome","text":"<p>Initial statement: \"Automate invoice processing\"</p> <p>Decomposed into specific metrics: - Reduce manual data entry time by 80%? - Achieve 99%+ accuracy on invoice extraction? - Process invoices within 24 hours of receipt? - Reduce error rate from 2% to 0.5%?</p> <p>Chosen metric: Reduce manual processing time by 60% while maintaining error rate below 2% within 3 months</p> <p>Rationale: - 60% reduction is ambitious but achievable with automation + human review - Maintaining current error rate prevents quality regression - 3-month timeline aligns with quarterly planning - Time savings translate directly to cost savings and capacity for higher-value work</p>"},{"location":"chapters/03-the-loop-framework/#step-2-deconstruction_2","title":"Step 2: Deconstruction","text":"<p>Atomic unit: Invoice line item (not entire invoice)</p> <p>Why line item? - Invoices have varying numbers of line items (1-50+) - Extraction errors often affect specific line items, not entire invoice - Quality metrics need line-level granularity to diagnose problems - Allows partial automation (high-confidence lines auto-approved, low-confidence reviewed)</p> <p>Key assumptions: 1. Invoices follow predictable structural patterns despite format variation 2. Line items contain sufficient context for categorization (description, amount, vendor) 3. Vendors provide accurate information (extraction accuracy doesn't validate correctness) 4. 95%+ extraction accuracy is sufficient to provide value (some manual correction acceptable) 5. Cost of LLM API calls is justified by labor savings (~$0.01/invoice vs. $5 manual processing) 6. Finance team will trust automated extraction with spot-checking</p> <p>Validation plan: - Test assumption 1: Manually review 100 diverse invoices to assess format variability - Test assumption 2: Check if line item descriptions alone enable categorization - Test assumption 3: Analyze historical errors (extraction vs. vendor errors) - Test assumption 4: Calculate acceptable error rate based on review time - Test assumption 5: Estimate API costs for expected volume - Test assumption 6: Discuss with finance team, understand trust requirements</p>"},{"location":"chapters/03-the-loop-framework/#step-3-alternatives_2","title":"Step 3: Alternatives","text":"<p>Alternative 1: Template matching + OCR - Define templates for common invoice formats (top 20 vendors) - Use OCR + regex to extract fields based on position - Pros: Fast, deterministic, low per-invoice cost - Cons: Brittle to format changes, doesn't handle novel vendors, high maintenance</p> <p>Alternative 2: Fine-tuned document extraction model (LayoutLM) - Fine-tune LayoutLM on labeled invoices - Extract structured data from document layout - Pros: Handles format variation, learns from examples, state-of-the-art for documents - Cons: Requires 500+ labeled invoices, longer development time, GPU infrastructure</p> <p>Alternative 3: GPT-4 with structured prompting - OCR invoice to text, pass to GPT-4 with JSON schema prompt - Request structured output (vendor, date, line items with descriptions/amounts/categories) - Pros: Minimal training data, flexible, handles edge cases, rapid iteration - Cons: API cost (~$0.05-0.10 per invoice), latency (5-10 seconds), less predictable</p> <p>Alternative 4: GPT-4 Vision (multimodal) - Pass invoice image directly to GPT-4 Vision - Extract structured data from visual layout without separate OCR - Pros: No OCR preprocessing, handles complex layouts, captures visual structure - Cons: Higher API cost (~$0.10-0.20 per invoice), newer technology</p> <p>Alternative 5: Hybrid: OCR + GPT-4 + validation rules - Use OCR to extract text - GPT-4 to structure and categorize - Business rules to validate (amounts sum correctly, dates are valid, categories match chart of accounts) - Pros: Balances cost/accuracy, catches errors, handles edge cases gracefully - Cons: More complex pipeline, requires orchestration</p> <p>Alternative 6: Hybrid: Template matching + GPT-4 fallback - Try template matching first for known formats - Fall back to GPT-4 for unknown formats or low-confidence extractions - Pros: Optimizes cost (cheap for common cases, robust for edge cases) - Cons: Most complex, requires two systems</p>"},{"location":"chapters/03-the-loop-framework/#step-4-trade-offs_2","title":"Step 4: Trade-Offs","text":"<p>Using the Trade-Off Canvas:</p> Alternative Accuracy Cost/Invoice Latency Handles Variation Dev Effort Maintenance Template \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 Fine-tuned \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 GPT-4 text \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 GPT-4 vision \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 OCR+GPT-4+rules \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 Template+GPT-4 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 <p>Decision: Start with GPT-4 with structured prompting (Alternative 3)</p> <p>Rationale: - Fastest time to value: Can implement and test within 1-2 weeks - Handles variability: Vendor format diversity makes template matching fragile - Cost is justified: $0.05-0.10/invoice &lt; $5 manual processing cost - Enables learning: Can rapidly iterate on prompts based on error patterns - Low regret: Prompt engineering insights transfer to GPT-4 Vision or fine-tuning if needed - Team capability: No ML expertise required, can leverage existing dev team</p> <p>Plan: Implement GPT-4 text extraction with structured prompts, evaluate for 1 month, then decide whether to: - Persist if accuracy &gt;95% and cost acceptable - Upgrade to GPT-4 Vision if OCR errors are main failure mode - Add validation rules if systematic errors can be caught with business logic - Consider fine-tuning if cost becomes prohibitive at scale</p>"},{"location":"chapters/03-the-loop-framework/#step-5-signals_2","title":"Step 5: Signals","text":"<p>Success Signals:</p> Signal Threshold Measurement Implication Extraction accuracy &gt;95% on validation set 95% Weekly, manually review 50 invoices Technical performance is adequate Manual review time decreases by 50%+ 50% Monthly, time tracking Automation delivers efficiency Error rate remains below 2% 2% Monthly, downstream error tracking Quality is maintained Finance team reduces spot-check frequency 20% \u2192 10% spot-check Monthly, team feedback Trust is building Cost per invoice &lt; $0.15 $0.15 Weekly, API cost monitoring Economics are sustainable <p>Kill Signals:</p> Signal Threshold Implication Extraction accuracy plateaus below 90% after prompt tuning 90% / 4 weeks LLM approach insufficient, need fine-tuning Manual correction time exceeds manual entry time Any time correction &gt; entry Automation creates more work Error rate increases to &gt;3% 3% / 2 months Quality regression unacceptable API costs exceed $0.25/invoice $0.25 / sustained Economics don't work, need cheaper alternative Finance team loses trust and reverts to manual Any reversion Solution is broken <p>Leading Indicators:</p> Leading Indicator Predicts Action Extraction accuracy on diverse sample during development Production performance If &lt;93%: improve prompts or add examples Finance team engagement with review UI during pilot Long-term adoption If low: simplify UI or improve flagging Pattern in extraction errors (OCR vs. categorization vs. validation) Systematic weaknesses If OCR errors dominant: upgrade to GPT-4 Vision API cost trajectory as volume scales Sustainability If trending &gt;$0.20: optimize prompts or consider alternatives Time to manually correct flagged line items Review efficiency If &gt;30 seconds/item: improve error highlighting"},{"location":"chapters/03-the-loop-framework/#implementation-plan_1","title":"Implementation Plan","text":"<p>Week 1: Prompt engineering and validation - Develop structured JSON schema for invoice output - Test GPT-4 prompts on 50 diverse invoices (various vendors, formats, complexities) - Analyze errors: OCR quality, field extraction accuracy, categorization correctness - Establish baseline: current manual processing time per invoice</p> <p>Week 2: Build pipeline and review UI - Integrate OCR \u2192 GPT-4 \u2192 structured output pipeline - Build simple review UI: side-by-side comparison of invoice image and extracted data - Implement confidence scoring: flag low-confidence extractions for human review</p> <p>Weeks 3-4: Pilot with finance team - Process 100 invoices through automated system - Finance team reviews all outputs, corrects errors, provides feedback - Measure: extraction accuracy, review time per invoice, error patterns - Iterate on prompts based on systematic errors</p> <p>Month 2: Expand and optimize - Roll out to 500 invoices/month with selective human review (high-confidence auto-approved) - Monitor success signals: accuracy, time savings, error rate, cost - Monitor kill signals: watch for quality regressions or cost overruns - Track leading indicators: error patterns, finance team satisfaction</p> <p>Decision point (Month 2-3): - If success signals met: Scale to full volume, reduce human review to spot-checking - If OCR errors dominate: Upgrade to GPT-4 Vision (eliminate OCR preprocessing) - If categorization errors dominate: Add validation rules or provide category examples in prompt - If cost is issue: Optimize prompts for token efficiency or explore template matching for common vendors - If kill signals hit: Revert to manual or pivot to fine-tuned model approach</p>"},{"location":"chapters/03-the-loop-framework/#reflection-questions","title":"Reflection Questions","text":"<p>Use these questions to deepen your understanding of The Loop:</p> <ol> <li> <p>On outcome definition: Think about a recent AI project. What was the implicit outcome metric? If you made it explicit, would different stakeholders have agreed? How might the project have differed with a different outcome definition?</p> </li> <li> <p>On atomic units: For a problem you're working on, what happens if you choose the wrong granularity? How would diagnosing problems differ if you chose one level too coarse or one level too fine?</p> </li> <li> <p>On assumptions: What assumptions did you make in a past project that turned out to be false? How did you discover the assumption was invalid? What would have happened if you'd surfaced and tested the assumption earlier?</p> </li> <li> <p>On alternatives: Think about a solution you implemented recently. What alternatives did you consider? What alternatives did you NOT consider that, in hindsight, might have been better? What prevented you from exploring more alternatives?</p> </li> <li> <p>On trade-offs: Describe a recent technical decision involving trade-offs. Were the trade-offs explicit or implicit during the decision? Did different stakeholders prioritize dimensions differently? How would making trade-offs more explicit have changed the discussion?</p> </li> <li> <p>On signals: For a current or recent project, what signals did you use to evaluate progress? Were they leading or lagging indicators? Did you define success and kill signals upfront, or figure them out as you went? What would have changed with clearer signals earlier?</p> </li> <li> <p>On The Loop: Think about a failed or struggling AI project. Work backward through The Loop\u2014where did the framing break down? Was it an unclear outcome? Invalid assumption? Missed alternative? Unstated trade-off? Lack of signals?</p> </li> <li> <p>On iteration: How often do you revisit your problem framing? When new information arrives, do you update your outcome definition, validate assumptions, reconsider alternatives, or recalibrate signals? What prevents more frequent Loop iteration?</p> </li> </ol>"},{"location":"chapters/03-the-loop-framework/#portfolio-project-apply-the-loop","title":"Portfolio Project: Apply The Loop","text":"<p>Objective: Demonstrate your ability to systematically frame an AI problem using The Loop framework.</p>"},{"location":"chapters/03-the-loop-framework/#instructions","title":"Instructions","text":"<p>Choose one of the following scenarios (or propose your own with instructor approval):</p> <p>Scenario A: Educational Content Recommendation A university wants to build a system that recommends learning resources (articles, videos, exercises) to students based on their learning progress, struggling topics, and goals.</p> <p>Scenario B: Manufacturing Quality Prediction A factory wants to predict which products coming off the assembly line are likely to have defects, enabling early intervention before shipping to customers.</p> <p>Scenario C: Customer Support Ticket Routing A SaaS company receives 1,000+ support tickets per day and wants to automatically route them to the right team and prioritize them by urgency and complexity.</p> <p>Scenario D: Medical Diagnosis Assistance A healthcare provider wants an AI system to help clinicians identify potential diagnoses based on patient symptoms, medical history, and test results.</p>"},{"location":"chapters/03-the-loop-framework/#deliverable","title":"Deliverable","text":"<p>Complete the following analysis for your chosen scenario:</p> <p>1. Outcome Definition (Step 1) - State the vague business objective - Generate 4-5 specific outcome metrics - Choose one metric and justify why it's the right frame - Articulate what constraints, trade-offs, and priorities this metric implies</p> <p>2. Problem Deconstruction (Step 2) - Identify the atomic unit and justify the granularity choice - Complete the Atomic Unit Canvas - List 6-8 critical assumptions your frame depends on - For each assumption, explain what breaks if it's false - Identify the 2-3 most fragile assumptions and propose how you'd validate them</p> <p>3. Alternatives Menu (Step 3) - Generate 5-6 distinct solution alternatives spanning simple to complex - For each alternative, document:   - Brief approach description   - Key strengths and weaknesses   - Data requirements   - Development effort estimate - Ensure your menu includes rule-based, classical ML, and GenAI options</p> <p>4. Trade-Off Analysis (Step 4) - Complete the Trade-Off Canvas for your alternatives - Identify the 2-3 most critical trade-off dimensions for your context - Choose one alternative and justify based on:   - Your context (resources, constraints, capabilities)   - The trade-offs you're willing to accept   - Why this is better than other alternatives for your situation</p> <p>5. Signal Framework (Step 5) - Complete the Signals Canvas with:   - 3-4 success signals (with thresholds and measurement approaches)   - 2-3 kill signals (with thresholds and conditions)   - 3-4 leading indicators (with what they predict and enabling actions) - For each signal, explain:   - How you'll measure it with available data   - Why this threshold is meaningful   - What action you'll take when the signal triggers</p> <p>6. Implementation Plan - Outline a 3-month implementation and evaluation plan - Include specific milestones, decision points, and pivot triggers - Explain how you'll use signals to decide whether to persist, pivot, or stop</p>"},{"location":"chapters/03-the-loop-framework/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>Your analysis will be evaluated on:</p> <ul> <li>Systematic application of The Loop: Did you work through all five steps methodically?</li> <li>Depth of reasoning: Did you explore second-order effects, hidden assumptions, and edge cases?</li> <li>Explicit articulation: Did you make trade-offs, assumptions, and priorities transparent?</li> <li>Contextual awareness: Did you ground decisions in realistic constraints and capabilities?</li> <li>Signal quality: Are your signals measurable, timely, and actionable?</li> <li>Clarity: Is your reasoning easy to follow and well-organized?</li> </ul> <p>Length: 8-12 pages (including canvases and tables)</p> <p>Submission: Upload PDF to the course portal by [deadline]</p>"},{"location":"chapters/03-the-loop-framework/#summary","title":"Summary","text":"<p>The Loop framework provides a systematic process for transforming ambiguous business problems into actionable AI initiatives:</p> <ol> <li>OUTCOME: Define the metric\u2014it IS the problem frame</li> <li>DECONSTRUCTION: Identify atomic units and surface assumptions</li> <li>ALTERNATIVES: Build a comprehensive menu before committing</li> <li>TRADE-OFFS: Choose based on explicit priorities and constraints</li> <li>SIGNALS: Define success, failure, and leading indicators upfront</li> </ol> <p>Key insights from this chapter:</p> <ul> <li>Metric choice determines the problem you solve: Different outcomes imply fundamentally different solutions, even when casual language sounds the same</li> <li>Assumptions are fragile: Every frame rests on beliefs that, if false, invalidate the entire approach\u2014surface them early so you can test them</li> <li>Premature convergence wastes resources: Generate alternatives systematically before committing\u2014your first idea is rarely your best</li> <li>No solution dominates: Every choice involves trade-offs\u2014make them explicit so stakeholders can make informed decisions</li> <li>Signals enable fast pivots: Clear success criteria, kill conditions, and leading indicators catch problems while change is still cheap</li> </ul> <p>The Loop is iterative, not one-time: Initial framing is based on incomplete information. Implementation generates data. Good teams cycle through The Loop quickly, using each iteration to refine understanding and adjust course.</p> <p>Use The Loop as a diagnostic tool: When projects feel stuck, work backward through the steps to find where framing broke down. When stakeholders disagree, use The Loop's structured language to articulate differences. When new information arrives, cycle through The Loop to update your frame.</p> <p>In the next chapters, we'll build on The Loop:</p> <ul> <li>Chapter 4 (Diagnosis) teaches how to read signals from live systems to detect when frames need adjustment</li> <li>Chapter 5 (Pivot) covers decision-making: when to persist, pivot, or stop based on signal evidence</li> <li>Chapter 6 (Application) applies The Loop to complex, multi-stakeholder case studies across domains</li> </ul> <p>For now, practice The Loop on your portfolio project. The framework will feel mechanical at first, but with repetition it becomes second nature\u2014a systematic way of thinking about AI problems that prevents common failure modes and enables better strategic decisions.</p>"},{"location":"chapters/03-the-loop-framework/quiz/","title":"Quiz: The Loop Framework","text":""},{"location":"chapters/03-the-loop-framework/quiz/#chapter-3-the-loop-framework","title":"Chapter 3: The Loop Framework","text":"<p>Level: Intermediate (CORE FRAMEWORK) Bloom's Distribution: Remember (25%), Understand (30%), Apply (30%), Analyze (15%)</p>"},{"location":"chapters/03-the-loop-framework/quiz/#instructions","title":"Instructions","text":"<p>Select the best answer for each question. Questions test your understanding of The Loop framework, outcome metrics, signal types, and problem-solving methodology.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#questions","title":"Questions","text":""},{"location":"chapters/03-the-loop-framework/quiz/#question-1-remember","title":"Question 1 (Remember)","text":"<p>What are the five sequential steps of The Loop?</p> <p>A) Identify \u2192 Measure \u2192 Analyze \u2192 Decide \u2192 Execute B) Frame \u2192 Ideate \u2192 Build \u2192 Test \u2192 Learn C) Define Problem \u2192 Set Metrics \u2192 Gather Data \u2192 Adjust \u2192 Repeat D) Understand \u2192 Strategize \u2192 Implement \u2192 Monitor \u2192 Refine</p> <p>Answer: A</p> <p>Explanation: The Loop's five steps, executed in strict sequence, are: (1) Identify the problem, (2) Measure the outcome, (3) Analyze the data, (4) Decide on adjustments, and (5) Execute changes. This cyclical process ensures continuous improvement based on empirical feedback.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#question-2-remember","title":"Question 2 (Remember)","text":"<p>Which signal type indicates that the project should be halted immediately?</p> <p>A) Leading indicator B) Success signal C) Kill signal D) Outcome metric</p> <p>Answer: C</p> <p>Explanation: A kill signal is a predetermined threshold or condition that, when triggered, signals immediate project termination. Unlike success signals (which indicate progress) or leading indicators (which predict future outcomes), kill signals protect resources by stopping underperforming initiatives.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#question-3-understand","title":"Question 3 (Understand)","text":"<p>What makes a good outcome metric for use in The Loop?</p> <p>A) It measures inputs and efforts invested in the solution B) It directly measures the end result users care about, is quantifiable, and can be tracked regularly C) It focuses on process compliance and adherence to procedures D) It combines multiple unrelated measurements into a single score</p> <p>Answer: B</p> <p>Explanation: A good outcome metric measures what actually matters to users (not effort or process), is quantifiable so you can compare before/after, and can be tracked regularly to inform Loop iterations. Vanity metrics or activity metrics fail this standard.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#question-4-understand","title":"Question 4 (Understand)","text":"<p>How do \"atomic units\" relate to problem deconstruction?</p> <p>A) They are large, comprehensive problems that cannot be broken down further B) They are the smallest meaningful pieces a problem can be divided into while remaining solvable C) They represent the total cost of solving an entire problem D) They are theoretical units used only in academic frameworks</p> <p>Answer: B</p> <p>Explanation: Atomic units are the smallest, irreducible components that problems can be decomposed into. They are meaningful enough to solve independently and test, yet small enough to iterate on quickly. Identifying atomic units enables parallel work and faster learning.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#question-5-understand","title":"Question 5 (Understand)","text":"<p>What is the primary purpose of documenting assumptions in The Loop?</p> <p>A) To assign blame if the solution fails B) To identify which beliefs underpin your strategy so they can be tested empirically C) To exclude stakeholders who disagree with the plan D) To create legal documentation for compliance</p> <p>Answer: B</p> <p>Explanation: Assumptions are the unspoken beliefs driving your approach. By explicitly documenting them, you can test each one during Loop iterations. This turns hidden risks (assumptions you didn't know you were making) into testable hypotheses, improving decision-making.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#question-6-apply","title":"Question 6 (Apply)","text":"<p>You're building an AI tutoring assistant and want to measure success. Your outcome metric is \"daily active users.\" Why might this be problematic, and what would be a better metric?</p> <p>A) Daily active users is problematic because it measures input, not outcome. A better metric would be \"improvement in student test scores\" or \"student confidence levels\" B) Daily active users is problematic because it's too easy to achieve. A better metric would be \"revenue per user\" C) Daily active users is problematic because it doesn't measure AI quality. A better metric would be \"number of features shipped\" D) Daily active users is problematic because it requires daily tracking. A better metric would be \"monthly active users\"</p> <p>Answer: A</p> <p>Explanation: Daily active users measures engagement (an input) rather than the actual outcome\u2014whether students are learning better. Outcome metrics for educational AI should focus on learning gains, skill improvement, or student confidence, which directly measure whether the tool achieves its intended purpose.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#question-7-apply","title":"Question 7 (Apply)","text":"<p>A startup is using The Loop to improve their product's conversion rate. They identify three atomic units: (1) Simplify checkout, (2) Add trust signals, (3) Optimize email reminders. They can only pursue two simultaneously. Using the Loop framework, how should they prioritize?</p> <p>A) Choose the two that sound most innovative B) Choose the two with the highest assumed impact and lowest implementation cost; test both in parallel with clear outcome metrics for each C) Start with all three sequentially; this is the safest approach D) Choose based on which the CEO prefers</p> <p>Answer: B</p> <p>Explanation: The Loop emphasizes parallel testing of high-impact, low-effort experiments. With limited capacity, prioritize atomic units by impact \u00d7 feasibility, then run them with independent metrics to isolate what actually drives conversion. This accelerates learning compared to sequential testing.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#question-8-apply","title":"Question 8 (Apply)","text":"<p>You're designing an AI content moderation system. Your success signal is \"zero harmful content gets published.\" Your kill signal is \"we block more than 10% of legitimate content.\" What does this tell us about your alternatives menu?</p> <p>A) The alternatives menu is irrelevant to signal design B) The success and kill signals reveal tension between competing outcomes, suggesting your alternatives menu should explore trade-off points between safety and usability C) The signals are contradictory and the project should be abandoned D) You need only a success signal; kill signals are redundant</p> <p>Answer: B</p> <p>Explanation: Success and kill signals that create opposing tensions (zero harmful vs. &lt;10% false positives) reveal inherent trade-offs. This signals that your alternatives menu should explore different moderation strategies with different trade-off profiles: stricter moderation (fewer false negatives, more false positives) vs. looser approaches, different AI thresholds, human review queues, etc.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#question-9-analyze","title":"Question 9 (Analyze)","text":"<p>A team conducts a Loop iteration and discovers their leading indicator (user session duration) improved 20%, but the success signal (account upgrade rate) declined 15%. What should they conclude?</p> <p>A) The leading indicator is broken; ignore it and focus only on success signals B) The iteration failed completely and should be abandoned C) Their leading indicator may be misaligned with true user value; they should investigate what's driving session duration vs. upgrade decisions D) Success signals always trump leading indicators; optimize purely for upgrades</p> <p>Answer: C</p> <p>Explanation: Misalignment between leading and success signals is a valuable diagnostic. It suggests the leading indicator doesn't predict the outcome you actually care about. This is not failure\u2014it's a discovery that lets you recalibrate. Investigate: Are users spending more time but getting frustrated? Is the upgraded feature not addressing their core need? This shapes the next Loop iteration.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#question-10-analyze","title":"Question 10 (Analyze)","text":"<p>You're at the end of a Loop cycle. Your data shows the problem is more nuanced than originally framed: what you thought was a single atomic unit actually has three interdependent sub-components. How should you respond in the next Loop cycle?</p> <p>A) Give up; the problem is too complex for The Loop B) Revise your problem deconstruction; return to The Loop's first step with a refined atomic unit breakdown that accounts for these dependencies C) Ignore the finding and continue with the original plan D) Hire more engineers to force the original solution through</p> <p>Answer: B</p> <p>Explanation: This is The Loop working as intended. Empirical data revealing that your problem decomposition was incomplete is not failure\u2014it's learning that feeds directly into the next iteration. Return to framing with better understanding. This refinement cycle (where atomic units may themselves decompose further) is central to the framework's power.</p>"},{"location":"chapters/03-the-loop-framework/quiz/#quiz-metadata","title":"Quiz Metadata","text":"<ul> <li>Total Questions: 10</li> <li>Remember (25%): Questions 1\u20132 (2 questions)</li> <li>Understand (30%): Questions 3\u20135 (3 questions)</li> <li>Apply (30%): Questions 6\u20138 (3 questions)</li> <li>Analyze (15%): Questions 9\u201310 (2 questions)</li> <li>Answer Distribution: A=25% (Q1,7,9), B=25% (Q3,6,8), C=25% (Q2,4,10), D=25% (Q5)</li> </ul>"},{"location":"chapters/03-the-loop-framework/quiz/#key-concepts-tested","title":"Key Concepts Tested","text":"<ul> <li>\u2713 The 5 steps of The Loop in sequence</li> <li>\u2713 Outcome metric characteristics and common pitfalls</li> <li>\u2713 Atomic unit identification and decomposition</li> <li>\u2713 Success, kill, and leading signal distinctions</li> <li>\u2713 Real-world application to scenario problems</li> <li>\u2713 Problem deconstruction and iteration refinement</li> <li>\u2713 Trade-off analysis through signal design</li> <li>\u2713 Data interpretation and framework adaptation</li> </ul>"},{"location":"chapters/04-diagnosis/","title":"Chapter 4: Diagnosis - Reading Signals","text":""},{"location":"chapters/04-diagnosis/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to:</p> <ol> <li>Diagnose AI system issues through systematic input and output analysis</li> <li>Perform error analysis on model predictions to identify failure patterns</li> <li>Design and execute sanity checks for AI systems</li> <li>Detect model drift, data drift, and concept drift in production systems</li> </ol>"},{"location":"chapters/04-diagnosis/#introduction","title":"Introduction","text":"<p>In March 2016, a major financial institution's credit risk model suddenly started rejecting 40% more loan applications than usual. The data science team spent three days analyzing model weights, checking for bugs in the inference code, and retraining on recent data. Nothing worked. Finally, a junior analyst discovered the issue: a vendor had changed the format of income data from annual to monthly values without notification. The model was seeing everyone's income as 1/12th of reality.</p> <p>This story illustrates a fundamental truth about AI systems: they fail silently and subtly. Unlike traditional software that crashes with error messages, AI systems continue producing outputs even when something is deeply wrong. They don't know they're confused.</p> <p>Diagnosis is the art of reading signals\u2014recognizing when something is wrong, isolating where the problem lives, and understanding why it happened. This chapter teaches you systematic approaches to diagnosing AI system failures, from quick sanity checks to deep error analysis.</p> <p>Think of yourself as a medical diagnostician. You don't jump straight to treatment. You observe symptoms, run tests, form hypotheses, and narrow possibilities until you understand the root cause. AI diagnosis follows the same pattern.</p>"},{"location":"chapters/04-diagnosis/#section-1-input-diagnosis","title":"Section 1: Input Diagnosis","text":"<p>Most AI failures start with the data. Before you question the model, question what the model sees.</p>"},{"location":"chapters/04-diagnosis/#key-idea-the-input-hypothesis","title":"Key Idea: The Input Hypothesis","text":"<p>When an AI system misbehaves, start with this hypothesis: \"The inputs have changed in ways the model wasn't trained to handle.\"</p> <p>Input diagnosis examines three dimensions:</p> <p>1. Data Distribution Shifts</p> <p>Models learn patterns from training data. When production data looks different, performance degrades. Common shifts include:</p> <ul> <li>Covariate shift: Input features change but relationships remain (e.g., camera quality improves, making images sharper)</li> <li>Prior shift: Class frequencies change (e.g., fraud rates spike during holidays)</li> <li>Sample selection bias: You're seeing a non-representative subset (e.g., model trained on US data, deployed globally)</li> </ul> <p>Diagnostic approach:</p> <pre><code># Compare training vs production distributions\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\ndef diagnose_distribution_shift(train_data, prod_data, feature):\n    \"\"\"\n    Compare distributions using visualization and statistical tests.\n    \"\"\"\n    # Visual comparison\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.hist(train_data[feature], alpha=0.5, label='Training', bins=50)\n    plt.hist(prod_data[feature], alpha=0.5, label='Production', bins=50)\n    plt.legend()\n    plt.title(f'{feature} Distribution Comparison')\n\n    plt.subplot(1, 2, 2)\n    plt.boxplot([train_data[feature], prod_data[feature]],\n                labels=['Training', 'Production'])\n    plt.title(f'{feature} Box Plot')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Statistical test for distribution equality\n    statistic, pvalue = stats.ks_2samp(train_data[feature],\n                                        prod_data[feature])\n\n    print(f\"Kolmogorov-Smirnov test for {feature}:\")\n    print(f\"  Statistic: {statistic:.4f}\")\n    print(f\"  P-value: {pvalue:.4f}\")\n\n    if pvalue &lt; 0.05:\n        print(f\"  \u26a0\ufe0f  Significant distribution shift detected!\")\n    else:\n        print(f\"  \u2713 Distributions are similar\")\n\n    return statistic, pvalue\n\n# Run for all features\nfor feature in numerical_features:\n    diagnose_distribution_shift(train_df, production_df, feature)\n</code></pre> <p>2. Data Quality Issues</p> <p>Even with stable distributions, quality problems corrupt inputs:</p> <ul> <li>Missing values: NULL, empty strings, sentinel values (-999, 0)</li> <li>Invalid values: Out-of-range numbers, malformed text, corrupted files</li> <li>Encoding errors: Wrong character sets, timezone issues, unit mismatches</li> <li>Upstream failures: API timeouts, database corruptions, ETL bugs</li> </ul> <p>Diagnostic checklist:</p> <pre><code>def data_quality_report(df):\n    \"\"\"\n    Generate comprehensive data quality diagnostics.\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"DATA QUALITY DIAGNOSTIC REPORT\")\n    print(\"=\" * 60)\n\n    # 1. Missing values\n    print(\"\\n1. MISSING VALUES:\")\n    missing = df.isnull().sum()\n    missing_pct = 100 * missing / len(df)\n    missing_df = pd.DataFrame({\n        'Count': missing,\n        'Percentage': missing_pct\n    })\n    print(missing_df[missing_df['Count'] &gt; 0].sort_values('Count',\n                                                           ascending=False))\n\n    # 2. Data types\n    print(\"\\n2. DATA TYPES:\")\n    print(df.dtypes.value_counts())\n\n    # 3. Cardinality\n    print(\"\\n3. CARDINALITY (unique values):\")\n    for col in df.columns:\n        unique_count = df[col].nunique()\n        unique_pct = 100 * unique_count / len(df)\n        print(f\"  {col}: {unique_count} ({unique_pct:.1f}%)\")\n\n    # 4. Value ranges\n    print(\"\\n4. NUMERIC RANGES:\")\n    print(df.describe())\n\n    # 5. Suspicious values\n    print(\"\\n5. SUSPICIOUS PATTERNS:\")\n    for col in df.select_dtypes(include=['number']).columns:\n        # Check for sentinel values\n        if (df[col] == -999).any():\n            print(f\"  \u26a0\ufe0f  {col}: Contains -999 (possible sentinel)\")\n        if (df[col] == 0).mean() &gt; 0.5:\n            print(f\"  \u26a0\ufe0f  {col}: &gt;50% zeros\")\n\n        # Check for outliers (IQR method)\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        outliers = ((df[col] &lt; (Q1 - 3 * IQR)) |\n                    (df[col] &gt; (Q3 + 3 * IQR))).sum()\n        if outliers &gt; 0:\n            print(f\"  \u26a0\ufe0f  {col}: {outliers} extreme outliers\")\n\n    return missing_df\n\n# Run diagnostics\nquality_report = data_quality_report(production_df)\n</code></pre> <p>3. Feature Engineering Bugs</p> <p>Derived features are fragile. A timezone change, a library update, or a dependency version bump can break feature computation silently.</p> <p>Diagnostic strategy:</p> <ul> <li>Recompute features: Run feature engineering on sample data and compare to stored features</li> <li>Check feature correlations: Features that were correlated in training should remain correlated</li> <li>Monitor feature statistics: Track mean, std, min, max for each feature over time</li> </ul>"},{"location":"chapters/04-diagnosis/#example-the-case-of-the-disappearing-weekend","title":"Example: The Case of the Disappearing Weekend","text":"<p>A ride-sharing company's demand forecasting model started underpredicting Sunday rides by 30%. The data science team investigated:</p> <pre><code># Check temporal features\nproduction_df['day_of_week'] = pd.to_datetime(production_df['timestamp']).dt.dayofweek\n\n# Compare to training data\ntrain_dow_dist = train_df['day_of_week'].value_counts(normalize=True)\nprod_dow_dist = production_df['day_of_week'].value_counts(normalize=True)\n\nprint(\"Day of week distribution:\")\nprint(pd.DataFrame({\n    'Training': train_dow_dist,\n    'Production': prod_dow_dist\n}))\n\n# Output:\n#     Training  Production\n# 0   0.142     0.143      # Monday\n# 1   0.143     0.143      # Tuesday\n# 2   0.143     0.143      # Wednesday\n# 3   0.143     0.143      # Thursday\n# 4   0.144     0.144      # Friday\n# 5   0.143     0.141      # Saturday\n# 6   0.142     0.000      # Sunday \u26a0\ufe0f\n</code></pre> <p>The culprit? A database migration changed the timestamp column from local time to UTC. Sunday rides in California (UTC-8) were being labeled as Monday in the new system. The model never saw \"Sunday\" data in production.</p>"},{"location":"chapters/04-diagnosis/#try-it","title":"Try It","text":"<p>Exercise: Input Diagnosis Lab</p> <p>You're given two datasets: <code>train.csv</code> (historical data) and <code>current.csv</code> (last week's production data). A previously 85%-accurate sentiment classifier is now at 72% accuracy.</p> <ol> <li>Load both datasets and compare feature distributions</li> <li>Generate a data quality report for production data</li> <li>Identify at least three input-related issues that could explain the performance drop</li> <li>For each issue, propose a specific diagnostic test</li> </ol> <pre><code># Starter code\nimport pandas as pd\nimport numpy as np\n\ntrain_df = pd.read_csv('train.csv')\ncurrent_df = pd.read_csv('current.csv')\n\n# Your diagnosis here\n# Hint: Check text length, character distributions,\n# language detection, encoding issues\n</code></pre> <p>Reflection: Before reading the next section, write down: What percentage of AI failures do you think originate from input issues vs. model issues? Why?</p>"},{"location":"chapters/04-diagnosis/#section-2-output-diagnosis","title":"Section 2: Output Diagnosis","text":"<p>Once you've ruled out (or fixed) input issues, examine what the model produces.</p>"},{"location":"chapters/04-diagnosis/#key-idea-error-patterns-reveal-root-causes","title":"Key Idea: Error Patterns Reveal Root Causes","text":"<p>Errors aren't random. They cluster around failure modes\u2014systematic weaknesses in how the model understands the problem. Output diagnosis reveals these patterns.</p> <p>1. Confusion Analysis</p> <p>For classification tasks, the confusion matrix is your first diagnostic tool:</p> <pre><code>from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\ndef diagnose_classification_errors(y_true, y_pred, class_names):\n    \"\"\"\n    Comprehensive classification diagnostics.\n    \"\"\"\n    # Confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names,\n                yticklabels=class_names)\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.title('Confusion Matrix')\n    plt.show()\n\n    # Classification report\n    print(\"\\nDetailed Metrics by Class:\")\n    print(classification_report(y_true, y_pred,\n                                target_names=class_names))\n\n    # Identify most confused pairs\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    np.fill_diagonal(cm_normalized, 0)  # Ignore correct predictions\n\n    confused_pairs = []\n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            if cm_normalized[i, j] &gt; 0.1:  # &gt;10% confusion rate\n                confused_pairs.append({\n                    'True': class_names[i],\n                    'Predicted': class_names[j],\n                    'Error_Rate': cm_normalized[i, j],\n                    'Count': cm[i, j]\n                })\n\n    if confused_pairs:\n        print(\"\\n\u26a0\ufe0f  SYSTEMATIC CONFUSIONS (&gt;10% error rate):\")\n        confused_df = pd.DataFrame(confused_pairs).sort_values(\n            'Error_Rate', ascending=False)\n        print(confused_df)\n\n    return cm, confused_pairs\n\n# Run diagnostics\ncm, confused_pairs = diagnose_classification_errors(\n    y_true=test_labels,\n    y_pred=predictions,\n    class_names=['cat', 'dog', 'bird', 'fish']\n)\n</code></pre> <p>What to look for:</p> <ul> <li>Asymmetric confusion: Model confuses A\u2192B more than B\u2192A (suggests class imbalance or feature bias)</li> <li>Cluster confusion: Model confuses several similar classes (suggests insufficient discriminative features)</li> <li>Systematic misclassification: One class predicts as another &gt;20% of time (suggests labeling error or concept overlap)</li> </ul> <p>2. Regression Error Patterns</p> <p>For continuous predictions, examine residuals (true - predicted):</p> <pre><code>def diagnose_regression_errors(y_true, y_pred, features_df=None):\n    \"\"\"\n    Regression error diagnostics.\n    \"\"\"\n    residuals = y_true - y_pred\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # 1. Residual plot\n    axes[0, 0].scatter(y_pred, residuals, alpha=0.5)\n    axes[0, 0].axhline(y=0, color='r', linestyle='--')\n    axes[0, 0].set_xlabel('Predicted Value')\n    axes[0, 0].set_ylabel('Residual')\n    axes[0, 0].set_title('Residual Plot')\n\n    # 2. Residual distribution\n    axes[0, 1].hist(residuals, bins=50)\n    axes[0, 1].set_xlabel('Residual')\n    axes[0, 1].set_ylabel('Frequency')\n    axes[0, 1].set_title('Residual Distribution')\n\n    # 3. Q-Q plot (normality check)\n    from scipy import stats\n    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n    axes[1, 0].set_title('Q-Q Plot')\n\n    # 4. Actual vs Predicted\n    axes[1, 1].scatter(y_true, y_pred, alpha=0.5)\n    axes[1, 1].plot([y_true.min(), y_true.max()],\n                    [y_true.min(), y_true.max()],\n                    'r--', linewidth=2)\n    axes[1, 1].set_xlabel('True Value')\n    axes[1, 1].set_ylabel('Predicted Value')\n    axes[1, 1].set_title('Actual vs Predicted')\n\n    plt.tight_layout()\n    plt.show()\n\n    # Statistical tests\n    print(\"ERROR DIAGNOSTICS:\")\n    print(f\"  Mean Absolute Error: {np.abs(residuals).mean():.3f}\")\n    print(f\"  Root Mean Squared Error: {np.sqrt((residuals**2).mean()):.3f}\")\n    print(f\"  Mean Residual (bias): {residuals.mean():.3f}\")\n    print(f\"  Std Residual: {residuals.std():.3f}\")\n\n    # Check for heteroscedasticity (non-constant variance)\n    # Split predictions into quartiles\n    quartiles = pd.qcut(y_pred, q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n    variance_by_quartile = [residuals[quartiles == q].var()\n                            for q in ['Q1', 'Q2', 'Q3', 'Q4']]\n\n    if max(variance_by_quartile) &gt; 2 * min(variance_by_quartile):\n        print(\"\\n  \u26a0\ufe0f  HETEROSCEDASTICITY DETECTED:\")\n        print(\"     Error variance changes with prediction magnitude\")\n        for i, var in enumerate(variance_by_quartile):\n            print(f\"     Q{i+1} variance: {var:.3f}\")\n\n    # Identify worst predictions\n    abs_errors = np.abs(residuals)\n    worst_idx = np.argsort(abs_errors)[-10:]\n\n    print(\"\\n  WORST 10 PREDICTIONS:\")\n    worst_df = pd.DataFrame({\n        'True': y_true.iloc[worst_idx] if hasattr(y_true, 'iloc') else y_true[worst_idx],\n        'Predicted': y_pred[worst_idx],\n        'Error': residuals.iloc[worst_idx] if hasattr(residuals, 'iloc') else residuals[worst_idx]\n    })\n    print(worst_df)\n\n    return residuals\n\n# Run diagnostics\nresiduals = diagnose_regression_errors(y_test, predictions)\n</code></pre> <p>What to look for:</p> <ul> <li>Systematic bias: Mean residual \u2260 0 (model consistently over/under-predicts)</li> <li>Heteroscedasticity: Variance changes with prediction magnitude (model uncertainty not calibrated)</li> <li>Nonlinear patterns: Residuals show curves (model missing nonlinear relationships)</li> <li>Outliers: Extreme errors indicate edge cases</li> </ul> <p>3. Failure Mode Analysis</p> <p>Beyond aggregate metrics, identify specific failure modes:</p> <pre><code>def identify_failure_modes(df, predictions, errors, threshold=0.9):\n    \"\"\"\n    Find slices of data where model systematically fails.\n    \"\"\"\n    df = df.copy()\n    df['prediction'] = predictions\n    df['error'] = errors\n    df['is_error'] = (errors &gt; threshold)\n\n    print(\"FAILURE MODE ANALYSIS:\")\n    print(f\"Overall error rate: {df['is_error'].mean():.1%}\\n\")\n\n    failure_modes = []\n\n    # Analyze categorical features\n    for col in df.select_dtypes(include=['object', 'category']).columns:\n        error_by_category = df.groupby(col)['is_error'].agg(['mean', 'count'])\n        error_by_category = error_by_category[error_by_category['count'] &gt; 10]\n\n        # Find categories with &gt;2x average error rate\n        avg_error = df['is_error'].mean()\n        problematic = error_by_category[error_by_category['mean'] &gt; 2 * avg_error]\n\n        if len(problematic) &gt; 0:\n            print(f\"\u26a0\ufe0f  Feature: {col}\")\n            print(problematic.sort_values('mean', ascending=False))\n            print()\n\n            for category in problematic.index:\n                failure_modes.append({\n                    'Feature': col,\n                    'Value': category,\n                    'Error_Rate': problematic.loc[category, 'mean'],\n                    'Count': problematic.loc[category, 'count']\n                })\n\n    # Analyze numerical features (binned)\n    for col in df.select_dtypes(include=['number']).columns:\n        if col in ['prediction', 'error', 'is_error']:\n            continue\n\n        df[f'{col}_bin'] = pd.qcut(df[col], q=5, duplicates='drop',\n                                     labels=False)\n        error_by_bin = df.groupby(f'{col}_bin')['is_error'].agg(['mean', 'count'])\n\n        # Find bins with &gt;2x average error rate\n        problematic_bins = error_by_bin[error_by_bin['mean'] &gt; 2 * avg_error]\n\n        if len(problematic_bins) &gt; 0:\n            print(f\"\u26a0\ufe0f  Feature: {col} (binned)\")\n            print(problematic_bins.sort_values('mean', ascending=False))\n            print()\n\n    return pd.DataFrame(failure_modes)\n\n# Example usage\nfailure_modes = identify_failure_modes(\n    df=test_df,\n    predictions=predictions,\n    errors=abs_errors,\n    threshold=10.0  # Define what constitutes an \"error\"\n)\n</code></pre>"},{"location":"chapters/04-diagnosis/#example-the-medical-imaging-blind-spot","title":"Example: The Medical Imaging Blind Spot","text":"<p>A chest X-ray classifier achieved 94% accuracy in testing but 78% in production. Output diagnosis revealed:</p> <ul> <li>Confusion analysis: Model confused pneumonia with normal 15% of time (asymmetric\u2014normal\u2192pneumonia was only 2%)</li> <li>Failure mode analysis: Error rate was 35% for patients &gt;70 years old, vs 8% for patients &lt;70</li> <li>Error inspection: Manual review of failures showed older patients often had subtle, atypical presentations</li> </ul> <p>Root cause: Training data oversampled clear, textbook cases of pneumonia (mostly younger patients). Model learned the \"obvious\" pattern but missed subtle variations.</p> <p>Fix: Rebalance training data by age group, augment with harder cases, add age as an explicit feature.</p>"},{"location":"chapters/04-diagnosis/#try-it_1","title":"Try It","text":"<p>Exercise: Output Pattern Analysis</p> <p>You have predictions from a loan approval model:</p> <ul> <li><code>loan_predictions.csv</code>: customer_id, true_label (approved/denied), predicted_label, confidence_score, age, income, credit_score, loan_amount</li> </ul> <p>Tasks:</p> <ol> <li>Create a confusion matrix and identify asymmetric confusions</li> <li>Plot residuals if you convert to a regression problem (confidence score)</li> <li>Run failure mode analysis to find demographic slices with high error rates</li> <li>Write a 3-sentence summary: \"The model struggles most with . This happens because . To fix it, we should ___.\"</li> </ol>"},{"location":"chapters/04-diagnosis/#section-3-error-analysis-framework","title":"Section 3: Error Analysis Framework","text":"<p>Diagnosis isn't just finding errors\u2014it's understanding them systematically.</p>"},{"location":"chapters/04-diagnosis/#key-idea-categorize-quantify-prioritize","title":"Key Idea: Categorize, Quantify, Prioritize","text":"<p>The error analysis framework:</p> <p>Step 1: Sample Errors</p> <p>Don't analyze all errors\u2014sample strategically:</p> <ul> <li>Random sample: Representative baseline</li> <li>High-confidence errors: Model was confident but wrong (worst failures)</li> <li>Low-confidence errors: Model was uncertain (might be ambiguous)</li> <li>Boundary cases: Predictions near decision threshold</li> <li>Stratified sample: Ensure all failure modes represented</li> </ul> <pre><code>def sample_errors_strategically(df, predictions, true_labels,\n                                 confidence_scores, n_samples=100):\n    \"\"\"\n    Sample errors for manual analysis.\n    \"\"\"\n    df = df.copy()\n    df['prediction'] = predictions\n    df['true_label'] = true_labels\n    df['confidence'] = confidence_scores\n    df['is_error'] = (predictions != true_labels)\n\n    errors = df[df['is_error']]\n\n    # Sample breakdown\n    samples = []\n\n    # 30 random errors\n    samples.append(errors.sample(min(30, len(errors))))\n\n    # 30 high-confidence errors (model was sure but wrong)\n    high_conf_errors = errors.nlargest(min(30, len(errors)), 'confidence')\n    samples.append(high_conf_errors)\n\n    # 20 low-confidence errors (model was uncertain)\n    low_conf_errors = errors.nsmallest(min(20, len(errors)), 'confidence')\n    samples.append(low_conf_errors)\n\n    # 20 boundary cases (confidence near 0.5 for binary)\n    boundary = errors.iloc[(errors['confidence'] - 0.5).abs().argsort()[:20]]\n    samples.append(boundary)\n\n    # Combine and deduplicate\n    error_sample = pd.concat(samples).drop_duplicates()\n\n    return error_sample.head(n_samples)\n</code></pre> <p>Step 2: Categorize Errors</p> <p>For each sampled error, manually label the failure type:</p> <p>Common error categories:</p> <ul> <li>Annotation error: Label was wrong, model was right</li> <li>Ambiguous case: No clear correct answer</li> <li>Missing feature: Model lacked necessary information</li> <li>Edge case: Rare scenario not well-represented in training</li> <li>Model limitation: Model architecture can't capture the pattern</li> <li>Distributional shift: Test example differs from training distribution</li> </ul> <pre><code># Create annotation template\nerror_sample['error_category'] = ''\nerror_sample['notes'] = ''\n\n# Export for manual labeling\nerror_sample.to_csv('errors_to_annotate.csv', index=False)\n\n# After manual labeling, analyze\nannotated = pd.read_csv('errors_annotated.csv')\n\nprint(\"ERROR BREAKDOWN:\")\nprint(annotated['error_category'].value_counts(normalize=True))\n</code></pre> <p>Step 3: Quantify Impact</p> <p>Not all errors matter equally. Estimate fix impact:</p> <pre><code>def quantify_error_impact(annotated_errors, total_errors):\n    \"\"\"\n    Estimate impact of fixing each error category.\n    \"\"\"\n    category_counts = annotated_errors['error_category'].value_counts()\n\n    # Extrapolate to all errors (assuming sample is representative)\n    sample_size = len(annotated_errors)\n\n    impact = []\n    for category, count in category_counts.items():\n        # Estimated proportion of all errors\n        proportion = count / sample_size\n        estimated_total = proportion * total_errors\n\n        impact.append({\n            'Category': category,\n            'Sample_Count': count,\n            'Estimated_Total': int(estimated_total),\n            'Potential_Improvement': f\"{proportion:.1%}\"\n        })\n\n    impact_df = pd.DataFrame(impact).sort_values('Estimated_Total',\n                                                  ascending=False)\n    print(\"ESTIMATED ERROR IMPACT:\")\n    print(impact_df)\n\n    return impact_df\n</code></pre> <p>Step 4: Prioritize Fixes</p> <p>Prioritize by:</p> <ol> <li>Impact: How many errors would this fix?</li> <li>Feasibility: How hard is it to fix?</li> <li>Risk: What could go wrong if we change this?</li> </ol> Error Category Impact Feasibility Priority Missing feature High Medium High Annotation error Medium Easy High Edge case Low Hard Low Ambiguous Medium Hard Medium"},{"location":"chapters/04-diagnosis/#example-customer-churn-error-analysis","title":"Example: Customer Churn Error Analysis","text":"<p>A telecom company analyzed 200 churn prediction errors:</p> <p>Error breakdown:</p> <ul> <li>35% annotation errors (customers marked as \"churned\" but actually on vacation)</li> <li>25% missing feature (model didn't know about customer service calls in last month)</li> <li>20% edge cases (customers who moved to areas without service)</li> <li>15% distributional shift (new promotion changed behavior)</li> <li>5% ambiguous (customers who paused service temporarily)</li> </ul> <p>Impact analysis:</p> <ul> <li>Fixing annotation errors: Easy, +7% accuracy</li> <li>Adding service call feature: Medium effort, +5% accuracy</li> <li>Handling edge cases: Hard, +4% accuracy (but only 2% of customer base)</li> </ul> <p>Decision: Fix annotations first (quick win), add feature second, defer edge cases.</p>"},{"location":"chapters/04-diagnosis/#try-it_2","title":"Try It","text":"<p>Exercise: Run Your Own Error Analysis</p> <p>Pick any classifier you've built (or use a provided dataset). Perform error analysis:</p> <ol> <li>Sample 50-100 errors strategically</li> <li>Manually categorize each error (use the categories above or define your own)</li> <li>Quantify the impact of each category</li> <li>Write a prioritized action plan with estimated accuracy improvements</li> </ol> <p>Template:</p> <pre><code>ERROR ANALYSIS REPORT\n=====================\n\nTotal errors: ___\nSample size: ___\n\nError Breakdown:\n- Category 1: ___% (estimated ___ total errors)\n- Category 2: ___% (estimated ___ total errors)\n- ...\n\nPrioritized Actions:\n1. [Action] - Expected impact: ___%, Effort: [Low/Med/High]\n2. [Action] - Expected impact: ___%, Effort: [Low/Med/High]\n3. ...\n</code></pre>"},{"location":"chapters/04-diagnosis/#section-4-sanity-checks","title":"Section 4: Sanity Checks","text":"<p>Before deep diagnosis, run quick tests to catch obvious issues.</p>"},{"location":"chapters/04-diagnosis/#key-idea-fail-fast-debug-faster","title":"Key Idea: Fail Fast, Debug Faster","text":"<p>Sanity checks are simple tests that should always pass. When they fail, something is fundamentally broken.</p> <p>The Sanity Check Hierarchy:</p> <p>Level 1: Data Sanity</p> <pre><code>def data_sanity_checks(df):\n    \"\"\"\n    Basic data sanity tests.\n    \"\"\"\n    checks = []\n\n    # Check 1: Non-empty\n    assert len(df) &gt; 0, \"Empty dataset\"\n    checks.append(f\"\u2713 Dataset non-empty: {len(df)} rows\")\n\n    # Check 2: Expected columns\n    expected_columns = {'feature1', 'feature2', 'label'}\n    assert expected_columns.issubset(df.columns), \\\n        f\"Missing columns: {expected_columns - set(df.columns)}\"\n    checks.append(f\"\u2713 All expected columns present\")\n\n    # Check 3: No all-null columns\n    null_cols = df.columns[df.isnull().all()].tolist()\n    assert len(null_cols) == 0, f\"All-null columns: {null_cols}\"\n    checks.append(f\"\u2713 No all-null columns\")\n\n    # Check 4: Reasonable label distribution\n    if 'label' in df.columns:\n        label_dist = df['label'].value_counts(normalize=True)\n        min_class = label_dist.min()\n        assert min_class &gt; 0.01, \\\n            f\"Severe class imbalance: smallest class is {min_class:.1%}\"\n        checks.append(f\"\u2713 Label distribution reasonable\")\n\n    # Check 5: Value ranges make sense\n    for col in df.select_dtypes(include=['number']).columns:\n        if col.endswith('_probability'):\n            assert df[col].between(0, 1).all(), \\\n                f\"{col} should be probability [0,1]\"\n        if col.endswith('_count'):\n            assert (df[col] &gt;= 0).all(), \\\n                f\"{col} should be non-negative count\"\n\n    checks.append(f\"\u2713 Value ranges valid\")\n\n    for check in checks:\n        print(check)\n\n    return True\n\ntry:\n    data_sanity_checks(production_df)\n    print(\"\\n\u2705 All data sanity checks passed\")\nexcept AssertionError as e:\n    print(f\"\\n\u274c Sanity check failed: {e}\")\n</code></pre> <p>Level 2: Model Sanity</p> <pre><code>def model_sanity_checks(model, X_test, y_test):\n    \"\"\"\n    Basic model sanity tests.\n    \"\"\"\n    checks = []\n\n    # Check 1: Model makes predictions\n    predictions = model.predict(X_test[:10])\n    assert len(predictions) == 10, \"Model not producing predictions\"\n    checks.append(f\"\u2713 Model produces predictions\")\n\n    # Check 2: Predictions in valid range\n    if hasattr(model, 'classes_'):  # Classifier\n        assert set(predictions).issubset(set(model.classes_)), \\\n            f\"Predictions outside class set\"\n        checks.append(f\"\u2713 Predictions are valid classes\")\n\n    # Check 3: Better than random\n    from sklearn.metrics import accuracy_score\n    accuracy = accuracy_score(y_test, model.predict(X_test))\n    random_baseline = 1.0 / len(model.classes_)\n\n    assert accuracy &gt; random_baseline * 1.1, \\\n        f\"Model accuracy {accuracy:.1%} barely beats random {random_baseline:.1%}\"\n    checks.append(f\"\u2713 Model beats random baseline\")\n\n    # Check 4: Predictions vary (not all same class)\n    unique_preds = len(set(predictions))\n    assert unique_preds &gt; 1, \"Model predicts only one class\"\n    checks.append(f\"\u2713 Model produces diverse predictions\")\n\n    # Check 5: Deterministic predictions (same input = same output)\n    pred1 = model.predict(X_test[:5])\n    pred2 = model.predict(X_test[:5])\n    assert (pred1 == pred2).all(), \"Model is non-deterministic\"\n    checks.append(f\"\u2713 Predictions are deterministic\")\n\n    for check in checks:\n        print(check)\n\n    return True\n</code></pre> <p>Level 3: Pipeline Sanity</p> <pre><code>def pipeline_sanity_checks(pipeline_func, sample_input):\n    \"\"\"\n    Test full prediction pipeline end-to-end.\n    \"\"\"\n    checks = []\n\n    # Check 1: Pipeline runs without errors\n    try:\n        output = pipeline_func(sample_input)\n        checks.append(f\"\u2713 Pipeline executes successfully\")\n    except Exception as e:\n        raise AssertionError(f\"Pipeline failed: {e}\")\n\n    # Check 2: Output format correct\n    assert isinstance(output, dict), \"Output should be dictionary\"\n    assert 'prediction' in output, \"Output missing 'prediction' key\"\n    checks.append(f\"\u2713 Output format correct\")\n\n    # Check 3: Latency acceptable\n    import time\n    start = time.time()\n    for _ in range(10):\n        pipeline_func(sample_input)\n    avg_latency = (time.time() - start) / 10\n\n    assert avg_latency &lt; 1.0, \\\n        f\"Pipeline too slow: {avg_latency:.2f}s per prediction\"\n    checks.append(f\"\u2713 Latency acceptable ({avg_latency*1000:.0f}ms)\")\n\n    # Check 4: Same input = same output (full pipeline)\n    output1 = pipeline_func(sample_input)\n    output2 = pipeline_func(sample_input)\n    assert output1 == output2, \"Pipeline is non-deterministic\"\n    checks.append(f\"\u2713 Pipeline is deterministic\")\n\n    for check in checks:\n        print(check)\n\n    return True\n</code></pre> <p>The Invariant Test</p> <p>Powerful sanity check: define invariants (things that should never change) and test them:</p> <pre><code>def test_invariants(model, test_cases):\n    \"\"\"\n    Test model invariants - predictions that should never change.\n\n    Example invariants:\n    - Changing irrelevant feature shouldn't change prediction\n    - Monotonic relationship: increasing X should increase Y\n    - Symmetry: f(a,b) = f(b,a) for symmetric features\n    \"\"\"\n\n    # Example: Sentiment should be robust to typos\n    clean_text = \"This product is amazing\"\n    typo_text = \"This produkt is amazng\"\n\n    sentiment_clean = model.predict([clean_text])[0]\n    sentiment_typo = model.predict([typo_text])[0]\n\n    assert sentiment_clean == sentiment_typo, \\\n        \"Model not robust to typos\"\n\n    # Example: Price prediction should increase with size\n    small_house = {'sqft': 1000, 'bedrooms': 2}\n    large_house = {'sqft': 2000, 'bedrooms': 2}\n\n    price_small = model.predict([small_house])[0]\n    price_large = model.predict([large_house])[0]\n\n    assert price_large &gt; price_small, \\\n        \"Price should increase with square footage\"\n\n    print(\"\u2705 All invariants hold\")\n</code></pre>"},{"location":"chapters/04-diagnosis/#example-the-sanity-check-that-saved-launch","title":"Example: The Sanity Check That Saved Launch","text":"<p>A fraud detection model was ready for production. One engineer insisted on a final sanity check:</p> <pre><code># Sanity: Model should flag transactions &gt;$10k as high risk\nlarge_transactions = test_df[test_df['amount'] &gt; 10000]\nhigh_risk_pct = (predictions[large_transactions.index] == 'high_risk').mean()\n\nassert high_risk_pct &gt; 0.5, \\\n    f\"Only {high_risk_pct:.1%} of large transactions flagged as high risk\"\n</code></pre> <p>It failed. Only 12% of large transactions were flagged. Investigation revealed a preprocessing bug: amounts were being normalized by account balance, so $10k was \"small\" for wealthy customers. The model learned this pattern.</p> <p>Fix: Remove normalization for fraud detection (raw amounts matter), add account context as separate feature.</p>"},{"location":"chapters/04-diagnosis/#try-it_3","title":"Try It","text":"<p>Exercise: Design Your Sanity Checks</p> <p>For a model you're familiar with (or a hypothetical one), design 5 sanity checks:</p> <ol> <li>One data sanity check</li> <li>One model sanity check</li> <li>One pipeline sanity check</li> <li>Two invariant tests specific to your domain</li> </ol> <p>Write them as executable Python assertions with clear error messages.</p>"},{"location":"chapters/04-diagnosis/#section-5-ai-specific-diagnostics","title":"Section 5: AI-Specific Diagnostics","text":"<p>AI systems have unique failure modes not seen in traditional software.</p>"},{"location":"chapters/04-diagnosis/#key-idea-models-decay-over-time","title":"Key Idea: Models Decay Over Time","text":"<p>Unlike traditional code, ML models degrade even when nothing changes in the code. The world shifts beneath them.</p> <p>1. Data Drift Detection</p> <p>Data drift: Input distributions change over time.</p> <pre><code>from scipy.stats import ks_2samp\nimport matplotlib.pyplot as plt\n\ndef detect_data_drift(reference_data, current_data, features, threshold=0.05):\n    \"\"\"\n    Detect drift in input features using statistical tests.\n    \"\"\"\n    drift_report = []\n\n    for feature in features:\n        # Kolmogorov-Smirnov test\n        statistic, pvalue = ks_2samp(\n            reference_data[feature].dropna(),\n            current_data[feature].dropna()\n        )\n\n        is_drift = pvalue &lt; threshold\n\n        drift_report.append({\n            'Feature': feature,\n            'KS_Statistic': statistic,\n            'P_Value': pvalue,\n            'Drift_Detected': is_drift\n        })\n\n        # Visualize if drift detected\n        if is_drift:\n            plt.figure(figsize=(10, 4))\n            plt.hist(reference_data[feature].dropna(),\n                     alpha=0.5, label='Reference', bins=50, density=True)\n            plt.hist(current_data[feature].dropna(),\n                     alpha=0.5, label='Current', bins=50, density=True)\n            plt.title(f'{feature} - DRIFT DETECTED (p={pvalue:.4f})')\n            plt.legend()\n            plt.show()\n\n    drift_df = pd.DataFrame(drift_report)\n\n    print(\"DATA DRIFT REPORT:\")\n    print(drift_df)\n    print(f\"\\n{drift_df['Drift_Detected'].sum()} / {len(features)} features show drift\")\n\n    return drift_df\n</code></pre> <p>2. Concept Drift Detection</p> <p>Concept drift: Relationship between inputs and outputs changes.</p> <pre><code>def detect_concept_drift(model, data_stream, window_size=1000,\n                         threshold=0.05):\n    \"\"\"\n    Detect concept drift by monitoring accuracy over time.\n    Uses sliding window and Page-Hinkley test.\n    \"\"\"\n    accuracies = []\n    windows = []\n\n    for i in range(0, len(data_stream) - window_size, window_size // 2):\n        window = data_stream.iloc[i:i+window_size]\n\n        predictions = model.predict(window.drop('label', axis=1))\n        accuracy = (predictions == window['label']).mean()\n\n        accuracies.append(accuracy)\n        windows.append(i + window_size // 2)\n\n    # Plot accuracy over time\n    plt.figure(figsize=(12, 5))\n    plt.plot(windows, accuracies, marker='o')\n    plt.axhline(y=np.mean(accuracies), color='r', linestyle='--',\n                label=f'Mean: {np.mean(accuracies):.3f}')\n    plt.xlabel('Sample Index')\n    plt.ylabel('Accuracy')\n    plt.title('Model Performance Over Time (Concept Drift Detection)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n    # Statistical test for trend\n    from scipy.stats import linregress\n    slope, intercept, r_value, p_value, std_err = linregress(windows, accuracies)\n\n    if p_value &lt; threshold and slope &lt; 0:\n        print(f\"\u26a0\ufe0f  CONCEPT DRIFT DETECTED:\")\n        print(f\"   Accuracy declining over time (slope={slope:.6f}, p={p_value:.4f})\")\n        print(f\"   Performance dropped {-slope * len(data_stream):.1%} over dataset\")\n    else:\n        print(f\"\u2713 No significant concept drift detected\")\n\n    return accuracies, windows\n</code></pre> <p>3. Model Drift Detection</p> <p>Model drift: Model's predictions change even with same inputs (due to retraining, library updates, hardware differences).</p> <pre><code>def detect_model_drift(model_v1, model_v2, test_data):\n    \"\"\"\n    Compare predictions between two model versions.\n    \"\"\"\n    preds_v1 = model_v1.predict(test_data)\n    preds_v2 = model_v2.predict(test_data)\n\n    # Agreement rate\n    agreement = (preds_v1 == preds_v2).mean()\n\n    print(f\"MODEL DRIFT ANALYSIS:\")\n    print(f\"  Prediction agreement: {agreement:.1%}\")\n\n    if agreement &lt; 0.95:\n        print(f\"  \u26a0\ufe0f  Significant model drift detected!\")\n\n        # Find disagreements\n        disagreements = test_data[preds_v1 != preds_v2]\n        print(f\"  {len(disagreements)} disagreements out of {len(test_data)}\")\n\n        # Analyze disagreement patterns\n        print(f\"\\n  Sample disagreements:\")\n        print(disagreements.head())\n    else:\n        print(f\"  \u2713 Models are consistent\")\n\n    return agreement\n</code></pre> <p>4. Bias and Fairness Diagnostics</p> <pre><code>def diagnose_bias(predictions, true_labels, protected_attribute,\n                  group_names):\n    \"\"\"\n    Detect bias across demographic groups.\n    \"\"\"\n    from sklearn.metrics import confusion_matrix\n\n    print(\"BIAS DIAGNOSTIC REPORT\")\n    print(\"=\" * 60)\n\n    for group_value, group_name in zip(protected_attribute.unique(),\n                                        group_names):\n        mask = (protected_attribute == group_value)\n\n        group_preds = predictions[mask]\n        group_labels = true_labels[mask]\n\n        accuracy = (group_preds == group_labels).mean()\n        positive_rate = (group_preds == 1).mean()\n\n        # True positive rate (sensitivity)\n        cm = confusion_matrix(group_labels, group_preds)\n        if cm.shape[0] &gt; 1:\n            tpr = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if cm[1, 1] + cm[1, 0] &gt; 0 else 0\n            fpr = cm[0, 1] / (cm[0, 1] + cm[0, 0]) if cm[0, 1] + cm[0, 0] &gt; 0 else 0\n        else:\n            tpr = fpr = 0\n\n        print(f\"\\nGroup: {group_name} (n={mask.sum()})\")\n        print(f\"  Accuracy: {accuracy:.3f}\")\n        print(f\"  Positive prediction rate: {positive_rate:.3f}\")\n        print(f\"  True positive rate: {tpr:.3f}\")\n        print(f\"  False positive rate: {fpr:.3f}\")\n\n    # Compute fairness metrics\n    groups = protected_attribute.unique()\n\n    # Demographic parity: P(pred=1 | group=A) \u2248 P(pred=1 | group=B)\n    pos_rates = []\n    for group in groups:\n        mask = (protected_attribute == group)\n        pos_rate = (predictions[mask] == 1).mean()\n        pos_rates.append(pos_rate)\n\n    demographic_parity_diff = max(pos_rates) - min(pos_rates)\n\n    print(f\"\\n{'=' * 60}\")\n    print(f\"FAIRNESS METRICS:\")\n    print(f\"  Demographic parity difference: {demographic_parity_diff:.3f}\")\n\n    if demographic_parity_diff &gt; 0.1:\n        print(f\"  \u26a0\ufe0f  Significant disparity detected (&gt;{10}%)\")\n    else:\n        print(f\"  \u2713 Demographic parity acceptable (&lt;{10}%)\")\n</code></pre>"},{"location":"chapters/04-diagnosis/#example-the-seasonal-drift","title":"Example: The Seasonal Drift","text":"<p>An e-commerce recommendation model performed well in testing (January) but degraded in production (November-December):</p> <pre><code># Monthly performance tracking\nmonthly_accuracy = {\n    'Jan': 0.85, 'Feb': 0.84, 'Mar': 0.83,\n    'Apr': 0.82, 'May': 0.81, 'Jun': 0.80,\n    'Jul': 0.79, 'Aug': 0.78, 'Sep': 0.77,\n    'Oct': 0.75, 'Nov': 0.68, 'Dec': 0.65\n}\n</code></pre> <p>Drift diagnosis:</p> <ul> <li>Data drift: November-December showed 3x higher proportion of \"gift\" searches</li> <li>Concept drift: Purchase patterns changed (people buying for others, not themselves)</li> <li>Solution: Retrain monthly with seasonal data, add \"gift intent\" feature</li> </ul>"},{"location":"chapters/04-diagnosis/#try-it_4","title":"Try It","text":"<p>Exercise: Drift Detection Simulation</p> <p>Simulate a data stream with drift:</p> <pre><code>import numpy as np\nimport pandas as pd\n\n# Generate data with drift\nnp.random.seed(42)\n\n# Months 1-6: Normal distribution mean=50\ndata_normal = np.random.normal(50, 10, size=(6000, 5))\n\n# Months 7-12: Distribution shifts, mean=55\ndata_shifted = np.random.normal(55, 10, size=(6000, 5))\n\n# Combine\ndata_stream = np.vstack([data_normal, data_shifted])\ndata_df = pd.DataFrame(data_stream,\n                       columns=['feature1', 'feature2', 'feature3',\n                               'feature4', 'feature5'])\ndata_df['month'] = np.repeat(range(1, 13), 1000)\n\n# Your task:\n# 1. Split into reference (months 1-3) and monitoring (months 4-12)\n# 2. Run drift detection for each month against reference\n# 3. Plot drift statistics over time\n# 4. Identify when drift began\n</code></pre>"},{"location":"chapters/04-diagnosis/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Input vs. Output: In your experience, what percentage of model failures originate from data issues vs. model issues? Why do you think data issues are so common?</p> </li> <li> <p>Error Analysis ROI: You have 200 errors to analyze but only time for 50. How do you choose which errors to investigate? What sampling strategy maximizes learning?</p> </li> <li> <p>Sanity Check Philosophy: Some engineers write dozens of sanity checks, others write few. What's your philosophy? When does sanity checking become excessive?</p> </li> <li> <p>Drift Response: You detect data drift but performance hasn't dropped yet. Do you retrain the model preemptively or wait for performance degradation? What factors influence your decision?</p> </li> <li> <p>Diagnostic Depth: When diagnosing a production issue, how do you balance speed (fixing it quickly) vs. thoroughness (understanding root cause)?</p> </li> <li> <p>Human in the Loop: Error analysis requires manual inspection. How do you scale this as your model sees millions of predictions? What role does human judgment play?</p> </li> </ol>"},{"location":"chapters/04-diagnosis/#portfolio-project-diagnostic-report","title":"Portfolio Project: Diagnostic Report","text":""},{"location":"chapters/04-diagnosis/#project-brief","title":"Project Brief","text":"<p>You are a machine learning engineer at a fintech company. The credit risk model has been in production for 6 months. Performance has degraded from 88% accuracy to 79% accuracy. Your task: diagnose the problem and propose fixes.</p>"},{"location":"chapters/04-diagnosis/#dataset","title":"Dataset","text":"<p>You'll receive (or simulate):</p> <ul> <li><code>credit_train.csv</code>: Original training data (10,000 loans, features: age, income, employment_length, credit_score, loan_amount, purpose, label: approved/denied)</li> <li><code>credit_production.csv</code>: Recent production data (2,000 loans, same schema)</li> <li><code>credit_predictions.csv</code>: Model predictions on production data with confidence scores</li> </ul>"},{"location":"chapters/04-diagnosis/#deliverable","title":"Deliverable","text":"<p>Write a comprehensive Diagnostic Report (1500-2000 words) that includes:</p> <p>1. Executive Summary (200 words)</p> <ul> <li>What's broken?</li> <li>Root cause in 1-2 sentences</li> <li>Recommended fix with estimated impact</li> </ul> <p>2. Input Diagnosis (400 words)</p> <ul> <li>Distribution comparison for key features</li> <li>Data quality issues identified</li> <li>Statistical tests for drift</li> <li>Visualizations (histograms, box plots)</li> </ul> <p>3. Output Diagnosis (400 words)</p> <ul> <li>Confusion matrix analysis</li> <li>Failure mode identification</li> <li>Error patterns by demographic slice</li> <li>Worst predictions with examples</li> </ul> <p>4. Error Analysis (400 words)</p> <ul> <li>Sample 100 errors, categorize into types</li> <li>Quantify impact of each error category</li> <li>Show example errors from each category</li> <li>Prioritize fixes by impact and feasibility</li> </ul> <p>5. Sanity Checks (200 words)</p> <ul> <li>List 5 sanity checks you ran</li> <li>Did any fail? What did you learn?</li> </ul> <p>6. Drift Analysis (200 words)</p> <ul> <li>Data drift: Which features drifted?</li> <li>Concept drift: Has the input-output relationship changed?</li> <li>Model drift: If you retrained, did predictions change significantly?</li> </ul> <p>7. Action Plan (200 words)</p> <p>Prioritized list:</p> <ol> <li>[Fix] - Estimated impact, effort, risk</li> <li>[Fix] - Estimated impact, effort, risk</li> <li>...</li> </ol>"},{"location":"chapters/04-diagnosis/#evaluation-rubric","title":"Evaluation Rubric","text":"Criterion Excellent (4) Good (3) Adequate (2) Needs Work (1) Diagnosis Depth Systematically analyzes inputs, outputs, and errors with statistical rigor Covers main diagnostic areas with some statistical testing Basic analysis, mostly descriptive Superficial, no structured approach Code Quality Clean, well-documented diagnostic scripts; reproducible analysis Functional code with some documentation Code works but hard to follow Code missing or broken Visualizations Insightful plots that clearly reveal patterns; publication-quality Useful plots with clear labels Basic plots, readable Poor or missing visualizations Root Cause Clearly identifies root cause with evidence; separates symptoms from causes Identifies likely cause with supporting data Makes reasonable guess No clear root cause identified Action Plan Prioritized, specific, with estimated impact and effort Actionable recommendations Generic suggestions Vague or missing"},{"location":"chapters/04-diagnosis/#stretch-goals","title":"Stretch Goals","text":"<ul> <li>Implement automated drift detection that runs weekly</li> <li>Build a dashboard for ongoing model monitoring</li> <li>Propose an A/B testing plan to validate your fixes</li> <li>Analyze fairness: Are errors distributed equally across demographic groups?</li> </ul>"},{"location":"chapters/04-diagnosis/#submission","title":"Submission","text":"<p>Submit as Jupyter notebook with:</p> <ul> <li>All code used for diagnosis</li> <li>Inline visualizations</li> <li>Markdown narrative explaining findings</li> <li>Executive summary at the top</li> </ul> <p>Remember: The goal isn't just to find the problem\u2014it's to demonstrate systematic diagnostic thinking that generalizes to any AI system failure.</p>"},{"location":"chapters/04-diagnosis/#summary","title":"Summary","text":"<p>Diagnosis is detective work. You gather evidence, form hypotheses, run tests, and iteratively narrow possibilities until you find the root cause.</p> <p>Key frameworks:</p> <ol> <li>Input Diagnosis: Start by checking data distributions, quality, and features</li> <li>Output Diagnosis: Analyze error patterns, confusion matrices, and failure modes</li> <li>Error Analysis: Systematically categorize, quantify, and prioritize fixes</li> <li>Sanity Checks: Quick tests to catch obvious issues before deep dives</li> <li>Drift Detection: Monitor for data drift, concept drift, and model drift over time</li> </ol> <p>Diagnostic mindset:</p> <ul> <li>Be systematic: Follow a framework, don't jump to conclusions</li> <li>Question assumptions: The model is probably fine; the data probably isn't</li> <li>Fail fast: Sanity checks save hours of debugging</li> <li>Quantify everything: \"It seems worse\" \u2192 \"Accuracy dropped 8% on over-65 users\"</li> <li>Think in distributions: One error is an anecdote; 100 errors reveal patterns</li> </ul> <p>The best diagnosticians combine statistical rigor with domain intuition. They know when to trust the numbers and when to look at individual examples. They move fluidly between aggregate metrics and edge cases, always asking: \"What is this failure trying to teach me?\"</p> <p>In the next chapter, we'll take what we've learned from diagnosis and use it to repair AI systems\u2014fixing data, retraining models, and validating improvements.</p> <p>Next Chapter: Chapter 5: Pivot - Acting on Signals</p> <p>Previous Chapter: Chapter 3: The Loop Framework</p>"},{"location":"chapters/04-diagnosis/quiz/","title":"Chapter 4: Diagnosis Quiz","text":"<p>Difficulty Level: Intermediate Total Questions: 10 Estimated Time: 20-25 minutes</p>"},{"location":"chapters/04-diagnosis/quiz/#instructions","title":"Instructions","text":"<p>Answer each question based on the concepts from Chapter 4: Diagnosis. Questions progress through different cognitive levels (Remember \u2192 Understand \u2192 Apply \u2192 Analyze) to test your comprehension and ability to apply diagnostic techniques in real-world scenarios.</p>"},{"location":"chapters/04-diagnosis/quiz/#questions","title":"Questions","text":""},{"location":"chapters/04-diagnosis/quiz/#1-input-vs-output-diagnosis-remember","title":"1. Input vs Output Diagnosis [Remember]","text":"<p>When diagnosing a machine learning system's failures, what is the primary difference between Input Diagnosis and Output Diagnosis?</p> <p>A) Input Diagnosis checks if features are correctly preprocessed; Output Diagnosis evaluates prediction quality B) Input Diagnosis looks at raw data distribution; Output Diagnosis only examines model weights C) Input Diagnosis is performed before training; Output Diagnosis is performed after deployment D) Input Diagnosis detects feature engineering errors; Output Diagnosis detects hardware failures</p> <p>Correct Answer: A</p> <p>Explanation: Input Diagnosis examines whether the data being fed into the model is correct (features, distributions, data quality), while Output Diagnosis evaluates whether the model's predictions are reasonable and of sufficient quality. This distinction is crucial for narrowing down where problems originate.</p>"},{"location":"chapters/04-diagnosis/quiz/#2-sanity-check-applications-remember","title":"2. Sanity Check Applications [Remember]","text":"<p>Which of the following is an appropriate use of a Sanity Check?</p> <p>A) Verifying that predicted prices never exceed the maximum historical price in training data B) Ensuring model accuracy improves with every new batch of data C) Confirming that feature distributions are perfectly normal D) Testing that the model architecture matches the research paper exactly</p> <p>Correct Answer: A</p> <p>Explanation: Sanity checks establish basic boundary conditions and expected behavior patterns. Checking that predictions stay within reasonable ranges is a classic sanity check that catches obvious failures before deeper analysis.</p>"},{"location":"chapters/04-diagnosis/quiz/#3-types-of-drift-definition-understand","title":"3. Types of Drift - Definition [Understand]","text":"<p>Explain the relationship between Data Drift, Model Drift, and Concept Drift. Which statement correctly describes them?</p> <p>A) Data Drift occurs in features; Concept Drift occurs in the target variable's relationship to features; Model Drift is when the model becomes outdated B) They are three names for the same phenomenon C) Data Drift and Model Drift are the same thing; Concept Drift is different D) Data Drift only affects training; Concept Drift only affects deployment</p> <p>Correct Answer: A</p> <p>Explanation: These three types of drift represent different failure modes: Data Drift (P(X) changes), Concept Drift (P(Y|X) changes), and Model Drift (the model was good but is now outdated relative to new data). Understanding these distinctions helps target solutions appropriately.</p>"},{"location":"chapters/04-diagnosis/quiz/#4-error-pattern-recognition-understand","title":"4. Error Pattern Recognition [Understand]","text":"<p>You notice your classification model performs well on normal cases but fails consistently on edge cases (extreme values). This error pattern suggests:</p> <p>A) The model has high bias and needs more complex features B) There is likely data distribution mismatch between training and edge cases C) The model is overfitting to the training set D) Input Diagnosis is unnecessary; this is purely an Output Diagnosis problem</p> <p>Correct Answer: B</p> <p>Explanation: Systematic failures on edge cases indicate that the model wasn't exposed to these cases during training (Data Drift or distribution mismatch). This is an Input Diagnosis problem showing that the training distribution doesn't match the production distribution.</p>"},{"location":"chapters/04-diagnosis/quiz/#5-bias-detection-methods-understand","title":"5. Bias Detection Methods [Understand]","text":"<p>Which diagnostic approach is most effective for detecting systemic bias in a model's predictions across different demographic groups?</p> <p>A) Comparing overall model accuracy to individual group accuracies B) Running a single sanity check on the aggregate dataset C) Analyzing error distributions separately for each protected attribute D) Checking that the training data is balanced in size</p> <p>Correct Answer: C</p> <p>Explanation: Detecting bias requires analyzing error patterns within each group separately. Aggregate metrics can hide group-specific problems. This allows you to see if certain groups experience systematically higher errors.</p>"},{"location":"chapters/04-diagnosis/quiz/#6-diagnosis-scenario-model-performance-drop-apply","title":"6. Diagnosis Scenario - Model Performance Drop [Apply]","text":"<p>Your fraud detection model was deployed 6 months ago with 95% accuracy. Today it shows 78% accuracy on the same test set. Which type of drift is MOST likely, and what's your first diagnostic step?</p> <p>A) Concept Drift is most likely; retraining the model immediately is the first step B) Data Drift is most likely; compare the feature distributions of recent data to training data C) Model Drift; the model weights have corrupted and need to be reset D) This is a hardware failure; check server logs first</p> <p>Correct Answer: B</p> <p>Explanation: A dramatic accuracy drop over time with changing real-world data suggests Data Drift. The first diagnostic step is Input Diagnosis: check if feature distributions have shifted. This guides whether retraining, feature engineering changes, or data cleaning is needed.</p>"},{"location":"chapters/04-diagnosis/quiz/#7-error-analysis-in-production-apply","title":"7. Error Analysis in Production [Apply]","text":"<p>You're analyzing error logs from your recommendation system and notice that 85% of errors occur in a specific user segment (new users with sparse interaction history). What's the most appropriate diagnostic approach?</p> <p>A) Blame the feature engineering team for not creating interaction-based features B) Perform an Error Analysis on this segment and check if the training data included this distribution C) Immediately apply output post-processing rules to hide errors from this segment D) Assume this is inevitable and accept the high error rate</p> <p>Correct Answer: B</p> <p>Explanation: Segment-specific error patterns are a key diagnostic signal. You should investigate whether the training data included new users with sparse data and whether your features are appropriate for this distribution. This is Input Diagnosis focused on identifying data distribution issues.</p>"},{"location":"chapters/04-diagnosis/quiz/#8-concept-drift-detection-apply","title":"8. Concept Drift Detection [Apply]","text":"<p>Your loan default prediction model has stable feature distributions and model weights haven't changed, yet prediction accuracy is declining. Which scenario best explains this?</p> <p>A) This is impossible\u2014if features and model are stable, performance must be stable B) This indicates Concept Drift: the relationship between features and target (likelihood of default) has changed C) The test set is corrupted D) This indicates Model Drift but not Concept Drift</p> <p>Correct Answer: B</p> <p>Explanation: If features haven't changed but the relationship between features and the target has shifted (e.g., economic conditions affecting default likelihood), this is Concept Drift: P(Y|X) changed while P(X) stayed stable. This requires different solutions than Data Drift.</p>"},{"location":"chapters/04-diagnosis/quiz/#9-integrated-diagnosis-multiple-drift-types-analyze","title":"9. Integrated Diagnosis - Multiple Drift Types [Analyze]","text":"<p>You're diagnosing a computer vision model for product quality inspection. You observe: - Feature distributions have shifted from historical data (edge colors changed due to lighting upgrades) - The model's decision boundary is no longer optimal for the new distributions - The underlying product quality requirements have become stricter (fewer defects acceptable)</p> <p>Which types of drift are present, and what is the correct diagnostic priority?</p> <p>A) Only Data Drift; retrain immediately B) Data Drift, Concept Drift, and potentially Model Drift; diagnose Input (features) first, then Output (prediction patterns) C) Only Concept Drift; no retraining needed D) Model Drift exclusively; check hardware and model serving infrastructure</p> <p>Correct Answer: B</p> <p>Explanation: This scenario involves multiple drift types occurring simultaneously. Input Diagnosis (Data Drift from lighting changes) should be priority one. Then analyze Output Diagnosis for Concept Drift (stricter quality standards change P(Y|X)). Model Drift may also be at play. A layered diagnostic approach handles this complexity.</p>"},{"location":"chapters/04-diagnosis/quiz/#10-sanity-check-design-analyze","title":"10. Sanity Check Design [Analyze]","text":"<p>You're building sanity checks for a stock price prediction model. Which of the following represents an effective, practical sanity check for detecting issues early?</p> <p>A) Verify that predicted prices stay within the historical price range of each stock, and alert if \u00b120% threshold is exceeded B) Ensure that every prediction is more accurate than a naive baseline C) Check that predicted volatility never decreases from one day to the next D) Confirm that R\u00b2 score on test data is above 0.95</p> <p>Correct Answer: A</p> <p>Explanation: Effective sanity checks should (1) be computationally cheap, (2) catch obvious failures, and (3) be actionable. Option A creates a practical boundary check that's fast to compute and provides early warning of distribution shift or model failure. Options B, C, and D are either too expensive or too permissive as sanity checks.</p>"},{"location":"chapters/04-diagnosis/quiz/#answer-key-scoring","title":"Answer Key &amp; Scoring","text":"Question Answer Bloom's Level Topic 1 A Remember Input vs Output Diagnosis 2 A Remember Sanity Checks 3 A Understand Types of Drift 4 B Understand Data Distribution &amp; Error Patterns 5 C Understand Bias Detection 6 B Apply Diagnosis Scenario (Model Performance) 7 B Apply Error Analysis 8 B Apply Concept Drift 9 B Analyze Integrated Multi-Drift Diagnosis 10 A Analyze Sanity Check Design <p>Scoring: - 9-10 correct: Mastery (90-100%) - 8 correct: Proficient (80%) - 6-7 correct: Developing (60-70%) - Below 6: Review Chapter 4 material</p>"},{"location":"chapters/04-diagnosis/quiz/#discussion-prompts","title":"Discussion Prompts","text":"<p>Use these questions for classroom discussion or deeper reflection:</p> <ol> <li> <p>Input vs Output Tradeoff: In practice, why might it be easier to fix Input Diagnosis problems than Output Diagnosis problems?</p> </li> <li> <p>Drift in Your Domain: Think of an ML system in your field of work. How might Data Drift, Concept Drift, or Model Drift manifest?</p> </li> <li> <p>Sanity Check Effectiveness: What are three sanity checks you would design for a healthcare prediction model? What properties make them effective?</p> </li> <li> <p>Bias and Fairness: How does understanding drift help in detecting and addressing bias? Can a model be biased without drift?</p> </li> </ol>"},{"location":"chapters/04-diagnosis/quiz/#key-concepts-review","title":"Key Concepts Review","text":"<p>Before taking this quiz, ensure you understand:</p> <ul> <li>Input Diagnosis: Examines feature quality, distributions, and data integrity</li> <li>Output Diagnosis: Evaluates prediction quality, error patterns, and model behavior</li> <li>Data Drift: P(X) changes\u2014feature distributions shift over time</li> <li>Concept Drift: P(Y|X) changes\u2014relationship between features and target shifts</li> <li>Model Drift: Model becomes outdated relative to current performance requirements</li> <li>Sanity Checks: Fast, cheap checks to catch obvious failures before deeper analysis</li> <li>Error Analysis: Breaking down errors by segment, pattern, or feature to understand root causes</li> <li>Bias Detection: Analyzing performance metrics separately for protected attributes and subgroups</li> </ul>"},{"location":"chapters/05-pivot/","title":"Chapter 5: Pivot - Acting on Signals","text":""},{"location":"chapters/05-pivot/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to:</p> <ol> <li>Recognize when signals indicate need for change versus normal project turbulence</li> <li>Apply the Persist/Pivot/Stop decision framework to AI project challenges</li> <li>Execute system-level reframes when solutions fail to address underlying problems</li> <li>Avoid sunk cost fallacy and make rational pivot decisions under uncertainty</li> </ol>"},{"location":"chapters/05-pivot/#introduction","title":"Introduction","text":"<p>In 2018, Google launched Duplex, an AI system that could make phone calls on your behalf to book restaurant reservations or hair appointments. The demo was stunning\u2014the AI used natural speech patterns, including \"umm\" and \"uh,\" making it nearly indistinguishable from a human caller. The internet exploded with both amazement and ethical concerns about AI deception.</p> <p>Google faced a critical decision. They had invested heavily in the technology. The demo worked. But the signals were clear: users were uncomfortable with an AI that could pass as human without disclosure. Google could have persisted, defended their approach, and pushed forward. Instead, they pivoted. They redesigned the system to clearly identify itself as an automated assistant and reframed the problem from \"make AI sound exactly like humans\" to \"make AI interactions helpful and transparent.\"</p> <p>Contrast this with IBM Watson Health. Between 2015 and 2022, IBM invested billions into applying Watson's AI capabilities to healthcare\u2014oncology, drug discovery, medical imaging. The signals came early: doctors didn't trust the recommendations, the training data didn't generalize, partnerships dissolved, revenue targets weren't met. Yet IBM persisted for seven years, pouring resources into a fundamentally flawed problem framing. In 2022, they sold Watson Health for parts, at a massive loss.</p> <p>The difference? Google recognized signals and acted. IBM saw the same signals but stayed the course, trapped by sunk costs and organizational momentum. This chapter is about developing the judgment to know when to persist through normal difficulties and when to fundamentally change direction.</p>"},{"location":"chapters/05-pivot/#section-1-signal-recognition","title":"Section 1: Signal Recognition","text":"<p>Signals are everywhere in AI projects. Models underperform. Users complain. Stakeholders lose interest. Engineers burn out. The challenge isn't detecting signals\u2014it's knowing which ones demand action and which are just noise.</p>"},{"location":"chapters/05-pivot/#key-idea-strong-signals-vs-weak-signals","title":"Key Idea: Strong Signals vs. Weak Signals","text":"<p>Weak signals are isolated, inconsistent, or easily explained by temporary factors: - A single model training run fails - One user gives negative feedback - A team member has a bad week - A competitor announces a vague plan</p> <p>Strong signals are persistent, patterned, and structural: - Models consistently underperform across multiple iterations - User feedback reveals fundamental misalignment with needs - Team velocity declines despite removing obstacles - Multiple competitors solve the problem differently</p> <p>The distinction matters because weak signals invite tuning\u2014adjust hyperparameters, refine features, provide better support. Strong signals invite reframing\u2014question assumptions, examine the problem definition, consider fundamental changes.</p> <p>Consider three signal categories:</p> <p>1. Technical Signals - Weak: Model accuracy plateaus after one experiment - Strong: Accuracy plateaus across diverse architectures, data augmentation strategies, and feature engineering approaches - What it means: Weak signals suggest optimization opportunities. Strong signals suggest you're approaching the theoretical limits of your problem formulation\u2014you may need different data, different objectives, or a different problem.</p> <p>2. User Signals - Weak: Users request specific features or complain about UI details - Strong: Users consistently work around your system or abandon it at the same stage - What it means: Weak signals point to usability improvements. Strong signals indicate your system doesn't align with actual workflows or needs\u2014you may be solving the wrong problem.</p> <p>3. Economic Signals - Weak: One sales cycle takes longer than expected - Strong: Customer acquisition costs consistently exceed lifetime value, or pilots fail to convert despite positive feedback - What it means: Weak signals suggest sales execution issues. Strong signals indicate fundamental product-market misalignment\u2014you may be targeting the wrong customers or solving too small a problem.</p>"},{"location":"chapters/05-pivot/#example-healthcare-chatbot-signals","title":"Example: Healthcare Chatbot Signals","text":"<p>A team builds an AI chatbot to help patients schedule appointments and answer basic medical questions. Here's how they interpret signals:</p> <p>Month 2 - Weak Signal (Persist) - Signal: 30% of users abandon the chatbot mid-conversation - Analysis: Usage logs show confusion around one specific question type - Action: Revise question phrasing, add examples, persist with core approach</p> <p>Month 5 - Moderate Signal (Investigate) - Signal: Users consistently exit the chatbot and call the office anyway - Analysis: Interviews reveal users want appointment changes, not just booking\u2014something the chatbot can't handle - Action: Add appointment modification features, but start questioning scope</p> <p>Month 9 - Strong Signal (Pivot) - Signal: Despite new features, 70% of interactions end with \"I'd like to speak to a person\" - Analysis: Users fundamentally want human reassurance for medical decisions, not efficiency - Action: Pivot from \"replace human interaction\" to \"triage and prepare for human interaction\"\u2014the chatbot becomes a pre-call tool that gathers information so humans can provide better service</p> <p>The strong signal wasn't just one metric\u2014it was the pattern across user behavior, qualitative feedback, and business outcomes. The team needed all three perspectives to recognize the signal strength.</p>"},{"location":"chapters/05-pivot/#try-it-signal-strength-assessment","title":"Try It: Signal Strength Assessment","text":"<p>For your portfolio project (or a hypothetical AI initiative), identify three signals you've encountered or might encounter. For each:</p> <ol> <li>Describe the signal (what you observed)</li> <li>Classify it (weak, moderate, or strong)</li> <li>Explain your reasoning (what makes it that strength?)</li> <li>Propose an appropriate response (persist, investigate, or pivot)</li> </ol> <p>Template: <pre><code>Signal: [What you observed]\nStrength: [Weak/Moderate/Strong]\nReasoning: [Why this classification?]\n- Is it isolated or patterned?\n- Is it temporary or structural?\n- Does it appear in one dimension (technical/user/economic) or multiple?\nResponse: [What action does this signal warrant?]\n</code></pre></p>"},{"location":"chapters/05-pivot/#section-2-the-three-options","title":"Section 2: The Three Options","text":"<p>When strong signals appear, you face three options: Persist, Pivot, or Stop. Each has appropriate contexts. The skill is matching the option to the situation.</p>"},{"location":"chapters/05-pivot/#persist-stay-the-course","title":"Persist: Stay the Course","text":"<p>When to Persist: - Signals are weak or inconsistent - The fundamental problem framing remains sound - You have clear hypotheses about what's causing difficulties - Resources and timeline accommodate continued iteration - Team morale and stakeholder support remain intact</p> <p>Example: OpenAI's GPT Persistence</p> <p>Between GPT-1 (2018) and GPT-3 (2020), OpenAI's language models showed promise but faced skepticism. GPT-2 could generate coherent text but often went off-topic. Critics questioned whether scaling alone would work. OpenAI persisted because: - The technical signals were improving (each scale-up showed gains) - The theoretical foundation (scaling hypothesis) was sound - They had the resources to continue experiments</p> <p>By GPT-3, persistence paid off. But notice what they didn't do: they didn't persist with failed approaches within each model. They persisted with the strategy (scaling transformers) while pivoting on tactics (architecture details, training procedures).</p> <p>Warning Signs You're Persisting Too Long: - Repeating the same experiments hoping for different results - Rationalizing away multiple independent negative signals - \"We've come too far to turn back now\" thinking - Team increasingly demoralized despite leadership optimism</p>"},{"location":"chapters/05-pivot/#pivot-change-direction","title":"Pivot: Change Direction","text":"<p>When to Pivot: - Strong signals indicate your approach is fundamentally limited - The underlying problem remains valuable and solvable - You have viable alternative approaches - Resources allow for redirection without catastrophic loss</p> <p>Types of Pivots:</p> <ol> <li>Problem Pivot: Change what problem you're solving</li> <li> <p>Example: Slack started as a gaming company, pivoted to team communication when their internal chat tool proved more valuable than the game</p> </li> <li> <p>Solution Pivot: Keep the problem, change the approach</p> </li> <li> <p>Example: Switch from supervised learning to reinforcement learning, or from prediction to recommendation</p> </li> <li> <p>Customer Pivot: Same solution, different audience</p> </li> <li> <p>Example: Enterprise AI tool repackaged for small businesses, or vice versa</p> </li> <li> <p>Value Pivot: Change what aspect of your solution you emphasize</p> </li> <li>Example: From \"most accurate\" to \"most explainable,\" or from \"fastest\" to \"most cost-effective\"</li> </ol> <p>Example: Medical Imaging Pivot</p> <p>A team builds an AI system to detect lung cancer from CT scans, aiming to outperform radiologists. After 18 months:</p> <p>Signals: - Model achieves 94% accuracy\u2014good but not superhuman - Radiologists resist adoption, citing liability concerns and lack of explainability - Hospitals hesitate due to regulatory uncertainty - False negatives in edge cases create unacceptable risk</p> <p>Pivot Decision: The team could persist (keep tuning the model) or stop (abandon the project). Instead, they pivot:</p> <p>From: Replace radiologists with AI diagnosis To: Augment radiologists with AI pre-screening</p> <p>New Approach: - AI flags scans that need human review (high sensitivity, accepts false positives) - Radiologists focus attention on flagged cases - Humans make final diagnoses, AI provides decision support - Regulatory path clearer, adoption friction lower</p> <p>This system-level reframe changes the problem from \"achieve superhuman accuracy\" to \"efficiently direct human expertise.\" The technical work continues, but success criteria shift from accuracy alone to efficiency gains and radiologist satisfaction.</p>"},{"location":"chapters/05-pivot/#stop-kill-the-project","title":"Stop: Kill the Project","text":"<p>When to Stop: - Strong signals indicate the problem isn't worth solving - No viable pivots address the fundamental issues - Opportunity costs exceed potential benefits - Continuing would damage team, reputation, or resources</p> <p>Example: Google Glass Consumer Edition</p> <p>Google Glass launched in 2013 as a consumer wearable computer. Signals emerged quickly: - Privacy concerns (cameras on faces in public spaces) - Social stigma (\"Glassholes\") - Weak use cases (nothing you couldn't do with a phone) - High price ($1,500) for limited functionality - Battery life and usability issues</p> <p>Google could have pivoted to different customer segments, adjusted pricing, or refined features. Instead, they stopped the consumer edition in 2015. Why? Because the strong signals indicated fundamental problems: - The technology wasn't ready (hardware limitations) - The social context wasn't ready (privacy norms) - The value proposition wasn't there (phones already filled the need)</p> <p>But notice: They didn't stop Glass entirely. They pivoted to enterprise uses (manufacturing, healthcare) where the use cases were clearer and social concerns less pronounced. The consumer product stopped; the underlying technology found a new problem.</p> <p>Stopping Well:</p> <p>Stopping isn't failure\u2014it's resource allocation. The key is stopping early enough to: - Redeploy team members to higher-value work - Preserve stakeholder trust through transparent communication - Extract learnings that inform future projects - Avoid throwing good money after bad</p> <p>The Checklist: - [ ] Have we tested multiple pivots without success? - [ ] Are the fundamental assumptions proven wrong? - [ ] Do opportunity costs now exceed potential returns? - [ ] Would continuing damage team morale or organizational credibility? - [ ] Have we documented what we learned?</p> <p>If you answer \"yes\" to most of these, stopping is strategic, not defeatist.</p>"},{"location":"chapters/05-pivot/#section-3-system-level-reframes","title":"Section 3: System-Level Reframes","text":"<p>Sometimes the solution is wrong because the problem is wrong. System-level reframes question the problem definition itself, not just the approach to solving it.</p>"},{"location":"chapters/05-pivot/#key-idea-levels-of-reframing","title":"Key Idea: Levels of Reframing","text":"<p>Level 1 - Tactical Reframe: Change implementation details - \"Our model needs more data\" \u2192 collect more data - \"Our features are weak\" \u2192 engineer better features - This is optimization, not reframing</p> <p>Level 2 - Strategic Reframe: Change approach to the same problem - \"Our supervised model needs labels\" \u2192 switch to self-supervised learning - \"Our monolithic model is too complex\" \u2192 build an ensemble - This is pivoting within problem boundaries</p> <p>Level 3 - System Reframe: Change the problem definition - \"We're building an AI to replace human judgment\" \u2192 \"We're building tools to augment human judgment\" - \"We're predicting customer churn\" \u2192 \"We're identifying factors that drive customer success\" - This changes what success means</p>"},{"location":"chapters/05-pivot/#example-recidivism-prediction-reframe","title":"Example: Recidivism Prediction Reframe","text":"<p>Original Problem: Build an AI system to predict which defendants will commit future crimes (recidivism), helping judges make bail and sentencing decisions.</p> <p>Signals After Deployment: - Model achieves 70% accuracy\u2014better than random, worse than hoped - Disproportionate false positives among minority defendants - Judges over-rely on scores, reducing individual case consideration - Predicted high-risk defendants who stay in jail can't prove the prediction wrong (unfalsifiable) - Civil rights groups file lawsuits citing bias</p> <p>Level 1 Response (Tactical): Tune the model, add more features, balance the training data - Problem: Doesn't address fundamental issues of fairness, unfalsifiability, or over-reliance</p> <p>Level 2 Response (Strategic): Switch to different ML approaches\u2014causal inference, fairness-aware learning, interpretable models - Problem: Still framing the problem as \"predict future crime,\" which may be fundamentally flawed</p> <p>Level 3 Response (System Reframe): Change the problem - From: \"Predict who will commit crime\" - To: \"Identify defendants who need support services to succeed\" - Or: \"Predict who will appear for trial\" (actionable, verifiable outcome) - Or: \"Provide judges decision support showing similar historical cases and outcomes\"</p> <p>The system reframe recognizes that \"predicting future crime\" encodes problematic assumptions: - Assumes crime is individual choice, not systemic/environmental - Creates self-fulfilling prophecies (jail \u2192 job loss \u2192 desperation \u2192 crime) - Optimizes for incarceration, not rehabilitation or justice</p> <p>A reframe might ask: What if the problem isn't \"who will commit crime\" but \"what conditions lead to successful reentry\" or \"how do we allocate support resources\"? This changes the entire AI system\u2014different data, different models, different success metrics, different stakeholders.</p>"},{"location":"chapters/05-pivot/#triggering-system-reframes","title":"Triggering System Reframes","text":"<p>System reframes happen when you question root assumptions. Ask:</p> <ol> <li>Who defined this problem, and what were their incentives?</li> <li> <p>Example: If your problem was defined by a technology vendor, is it framed to favor their technology?</p> </li> <li> <p>What does \"success\" assume about the world?</p> </li> <li> <p>Example: Optimizing ad clicks assumes more engagement is good\u2014but what if attention is finite and precious?</p> </li> <li> <p>Who benefits from this problem definition, and who is harmed?</p> </li> <li> <p>Example: \"Predict employee turnover\" benefits employers but may harm employees flagged as flight risks</p> </li> <li> <p>What would we do if this approach were impossible?</p> </li> <li> <p>Example: If you couldn't predict recidivism, you'd focus on root causes\u2014poverty, mental health, education. Should you do that anyway?</p> </li> <li> <p>Are we solving a symptom or a cause?</p> </li> <li>Example: Predicting hospital readmissions treats readmission as a patient problem, not a systemic discharge/follow-up problem</li> </ol>"},{"location":"chapters/05-pivot/#try-it-reframing-practice","title":"Try It: Reframing Practice","text":"<p>Take your portfolio project (or a hypothetical AI system). Write three levels of response to a strong negative signal:</p> <p>Scenario: Your AI system is underperforming or facing resistance.</p> <p>Level 1 (Tactical Reframe): What implementation changes would you try?</p> <p>Level 2 (Strategic Reframe): What alternative approaches could solve the same problem?</p> <p>Level 3 (System Reframe): How could you redefine the problem itself? What assumptions would you question?</p> <p>Guiding Questions: - Who defined the original problem, and why? - What does your current success metric assume? - Who is excluded or harmed by the current problem framing? - If your current approach were impossible, what would you do instead?</p>"},{"location":"chapters/05-pivot/#section-4-cautionary-tales","title":"Section 4: Cautionary Tales","text":"<p>Learning from failure is cheaper than experiencing it firsthand. Here are three cautionary tales of projects that ignored strong signals and paid the price.</p>"},{"location":"chapters/05-pivot/#case-1-quibi-ignoring-user-behavior-signals","title":"Case 1: Quibi - Ignoring User Behavior Signals","text":"<p>The Vision: Quibi (2020) raised $1.75 billion to create premium short-form video content for mobile devices, betting that users wanted high-quality 10-minute episodes designed for phones.</p> <p>The Signals: - Beta testers preferred landscape mode; Quibi's tech forced portrait-first viewing - Users expected free content (like YouTube/TikTok); Quibi charged $5-8/month - COVID-19 hit during launch\u2014commuters (the target audience) stayed home - User retention dropped 90% after free trials ended - Competitors (TikTok, Instagram Reels) offered short video for free with massive adoption</p> <p>What They Should Have Done: - Pivot 1: Make content playable on TVs/desktops when COVID eliminated commutes - Pivot 2: Adopt ad-supported free tier when retention collapsed - System Reframe: Question whether \"professional short-form mobile video\" was a real need, or whether user-generated content had already filled it</p> <p>What They Did: Persisted with the original model, spent $1.75 billion in six months, shut down in October 2020.</p> <p>The Lesson: When user behavior contradicts your assumptions (people want free, people watch on TVs, people prefer user content), that's a system-level signal. Persisting with better marketing or content won't fix a flawed problem definition.</p>"},{"location":"chapters/05-pivot/#case-2-ibm-watson-for-oncology-ignoring-ground-truth","title":"Case 2: IBM Watson for Oncology - Ignoring Ground Truth","text":"<p>The Vision: IBM Watson would analyze medical literature and patient records to recommend cancer treatments, augmenting or replacing oncologist decision-making.</p> <p>The Signals: - Watson's recommendations often contradicted oncologist expertise - Training data came from a single hospital (Memorial Sloan Kettering), limiting generalization - Doctors found recommendations \"not safe and may be harmful\" in internal reviews - System required extensive manual coding to handle new cancer types - Partners (MD Anderson, others) canceled contracts after millions spent - No peer-reviewed studies validated clinical effectiveness</p> <p>What They Should Have Done: - Pivot 1: Shift from \"replace oncologist judgment\" to \"literature search assistant\" when recommendations proved unreliable - Pivot 2: Focus on narrow, validated use cases (e.g., flagging drug interactions) instead of general treatment recommendations - System Reframe: Recognize that oncology is too complex and contextual for pattern-matching AI; reframe to support tasks (research, documentation) rather than core decisions</p> <p>What They Did: Persisted for seven years, rebranding and repositioning without addressing fundamental issues. Sold Watson Health assets in 2022 at a massive loss.</p> <p>The Lesson: When domain experts consistently reject your system's outputs, that's a signal about problem complexity, not expert stubbornness. You may be solving a problem that requires different methods\u2014or shouldn't be solved this way at all.</p>"},{"location":"chapters/05-pivot/#case-3-zillow-offers-ignoring-economic-signals","title":"Case 3: Zillow Offers - Ignoring Economic Signals","text":"<p>The Vision: Zillow would use AI to predict home values, buy houses at algorithm-determined prices, then resell at a profit (iBuying).</p> <p>The Signals: - Algorithm struggled with volatile markets (COVID housing boom) - Home appreciation outpaced algorithm predictions\u2014Zillow paid too much - Renovation costs and holding costs exceeded models - Competitors (Opendoor, Offerpad) faced same challenges but pivoted faster - Losses mounted: $380 million in Q3 2021 alone - Inventory swelled: 7,000+ homes purchased but unsellable at profitable prices</p> <p>What They Should Have Done: - Pivot 1: Pause buying when market volatility exceeded model confidence thresholds - Pivot 2: Shift from \"profit on resale\" to \"profit on transaction fees\" (facilitation, not speculation) - System Reframe: Recognize that home pricing isn't just an ML problem\u2014it's a market timing and risk management problem; reframe to tools that help others make pricing decisions</p> <p>What They Did: Scaled up buying even as losses grew, hoping scale would fix model accuracy. Shut down Zillow Offers in November 2021, laid off 25% of staff.</p> <p>The Lesson: When your economic model depends on prediction accuracy you can't achieve, persisting with more data or better models won't help. The problem may be unpredictable, or the margins may be too thin for error. Either reframe the business model or stop.</p>"},{"location":"chapters/05-pivot/#common-patterns-in-failures","title":"Common Patterns in Failures","text":"<p>All three cases share characteristics:</p> <ol> <li>Sunk Cost Trap: Massive investment made pivoting feel like admitting failure</li> <li>Founder's Dilemma: Leadership had personal/professional identity tied to original vision</li> <li>Confirmation Bias: Focused on positive signals (funding, press, partnerships) over negative signals (user behavior, expert rejection, economic losses)</li> <li>Technological Optimism: Believed \"more data\" or \"better models\" would solve structural problems</li> <li>Missing Reframe: Never questioned whether they were solving the right problem</li> </ol>"},{"location":"chapters/05-pivot/#section-5-overcoming-sunk-cost","title":"Section 5: Overcoming Sunk Cost","text":"<p>The sunk cost fallacy is the most common reason teams persist when they should pivot or stop. Understanding its psychology is essential to making rational decisions.</p>"},{"location":"chapters/05-pivot/#the-fallacy-definition-and-dynamics","title":"The Fallacy: Definition and Dynamics","text":"<p>Sunk Cost Fallacy: The tendency to continue investing in a project because of past investment, even when future costs exceed future benefits.</p> <p>Rational Decision-Making: Evaluate options based on future costs and benefits only. Past investments are gone regardless of future decisions.</p> <p>Example: - You've spent $500,000 and 12 months building an AI system - Completing it requires another $300,000 and 6 months - Expected value if completed: $200,000 - Sunk cost thinking: \"We've invested too much to quit now\" - Rational thinking: \"Completing costs $300k to gain $200k\u2014that's a $100k loss. Stop.\"</p> <p>The $500,000 is gone whether you continue or stop. The only question is: does the additional $300,000 investment yield more than $200,000 in value?</p>"},{"location":"chapters/05-pivot/#why-we-fall-for-it","title":"Why We Fall for It","text":"<p>Psychological Drivers:</p> <ol> <li> <p>Loss Aversion: Stopping feels like admitting failure and crystallizing losses. Continuing offers (false) hope of recovery.</p> </li> <li> <p>Escalation of Commitment: Each new investment makes stopping harder, creating a spiral.</p> </li> <li> <p>Identity Protection: \"I'm not the kind of person who quits\" or \"I'm a visionary who perseveres\"\u2014stopping threatens self-image.</p> </li> <li> <p>Social Pressure: Investors, stakeholders, and teams expect persistence. Pivoting feels like letting people down.</p> </li> <li> <p>Narrative Fallacy: We construct stories where current struggles are \"the dark before the dawn\"\u2014every hero's journey has setbacks, so setbacks must mean success is near.</p> </li> </ol>"},{"location":"chapters/05-pivot/#organizational-amplification","title":"Organizational Amplification","text":"<p>Organizations magnify sunk cost fallacies through:</p> <ul> <li>Budget cycles: \"We've allocated this funding; we have to spend it\"</li> <li>Promotion incentives: Leaders who pivot risk appearing indecisive</li> <li>Siloed information: Teams closest to problems may see signals that executives don't</li> <li>Public commitments: Announced initiatives are hard to cancel without embarrassment</li> </ul>"},{"location":"chapters/05-pivot/#countering-sunk-cost-thinking","title":"Countering Sunk Cost Thinking","text":"<p>1. Pre-Commitment Strategies</p> <p>Before starting, establish decision rules:</p> <p>Example Decision Rule: \"If after 6 months, our model accuracy is below 80% AND user satisfaction is below 60%, we will pivot to a decision-support tool rather than an autonomous system.\"</p> <p>By setting thresholds in advance, you create permission to pivot without it feeling like failure.</p> <p>2. Prospective Framing</p> <p>Reframe the decision from \"should we continue?\" to \"if we were starting today, would we start this project?\"</p> <p>Exercise: The New Manager Test - Imagine a new manager arrives who knows nothing about your project's history - They evaluate: \"Should we invest [remaining resources] in this project given current evidence?\" - Would they say yes?</p> <p>This mental model removes sunk costs from consideration.</p> <p>3. Red Team Reviews</p> <p>Assign a team member (rotating role) to argue for stopping or pivoting. Make it their job to find contrary evidence.</p> <p>Benefits: - Surfaces suppressed doubts - Normalizes questioning the project - Creates safe space for dissent</p> <p>4. Time-Boxing Pivots</p> <p>Set explicit intervals for pivot consideration:</p> <p>\"Every quarter, we will evaluate whether to persist, pivot, or stop based on [specific metrics].\"</p> <p>This removes the stigma\u2014pivoting isn't weakness, it's scheduled strategic review.</p> <p>5. Celebrate Pivots</p> <p>Reframe pivots as learning victories, not failures:</p> <ul> <li>\"We discovered this problem is harder than we thought\u2014that's valuable information\"</li> <li>\"We found a better problem to solve\u2014that's strategic insight\"</li> <li>\"We stopped before wasting more resources\u2014that's discipline\"</li> </ul> <p>Example: Google celebrates \"failures\" through internal case studies. Google+ shutdown wasn't a failure; it validated that social networking wasn't core to their mission. This culture makes future pivots easier.</p>"},{"location":"chapters/05-pivot/#try-it-sunk-cost-audit","title":"Try It: Sunk Cost Audit","text":"<p>For your portfolio project, conduct a sunk cost audit:</p> <p>1. Document Sunk Costs: - Time invested: [hours/months] - Money invested: [dollars] - Reputation/commitment invested: [public statements, stakeholder promises]</p> <p>2. Apply the New Manager Test: \"If I were assigned this project today, knowing what I know now, would I start it?\" - If yes: What would I do differently? - If no: Why not?</p> <p>3. Identify Commitment Mechanisms: What makes stopping/pivoting hard beyond rational calculation? - Personal identity (\"I'm not a quitter\") - Social pressure (\"I told my advisor this would work\") - Organizational inertia (\"We've budgeted for this\")</p> <p>4. Design a Pre-Commitment Rule: What objective criteria would trigger a pivot/stop decision? - Metric thresholds - Timeline checkpoints - Resource limits</p> <p>Template: \"If [measurable condition] is not met by [date], we will [specific pivot/stop action].\"</p>"},{"location":"chapters/05-pivot/#reflection-questions","title":"Reflection Questions","text":"<ol> <li> <p>Signal Interpretation: Think of a time you ignored a strong signal (in any domain\u2014projects, relationships, career). What made you ignore it? What would it take to recognize that pattern earlier next time?</p> </li> <li> <p>Persist vs. Pivot: Consider an AI project facing difficulties. What evidence would convince you to persist? What evidence would convince you to pivot? How do you distinguish \"hard but solvable\" from \"fundamentally flawed\"?</p> </li> <li> <p>System Reframes: Choose a well-known AI system (facial recognition, content recommendation, predictive policing, etc.). How could you reframe the problem it's solving at a system level? What assumptions would you question?</p> </li> <li> <p>Sunk Cost Scenarios: You've spent 18 months building an AI product. User feedback is mixed, technical performance is mediocre, and a competitor just launched a better solution. Your team wants to persist. What questions would you ask? What would you need to know to decide?</p> </li> <li> <p>Ethical Pivots: When should ethical concerns trigger a pivot or stop, even if technical and economic signals suggest persisting? What's your threshold?</p> </li> </ol>"},{"location":"chapters/05-pivot/#portfolio-project-pivot-analysis","title":"Portfolio Project: Pivot Analysis","text":"<p>Document a pivot decision\u2014either from your own experience or a hypothetical AI project. Your analysis should demonstrate mastery of the persist/pivot/stop framework and system-level reframing.</p>"},{"location":"chapters/05-pivot/#requirements","title":"Requirements","text":"<p>1. Project Context (300-400 words) - What problem was the AI system trying to solve? - Who were the stakeholders and users? - What was the technical approach? - What were the initial success criteria?</p> <p>2. Signal Documentation (400-500 words)</p> <p>Identify and analyze at least three strong signals:</p> <p>For each signal: - Description: What did you observe? - Category: Technical, user, or economic? - Strength: Why is this a strong signal (persistent, patterned, structural)? - Evidence: Specific metrics, quotes, or observations</p> <p>3. Decision Analysis (500-600 words)</p> <p>Evaluate the three options:</p> <p>Persist: - Under what conditions would persisting make sense? - What hypotheses would you test if persisting? - What would constitute evidence that persistence was working?</p> <p>Pivot: - What type of pivot? (Problem, solution, customer, or value) - What would the pivoted approach look like specifically? - How does this address the strong signals? - What new risks does pivoting introduce?</p> <p>Stop: - What conditions would justify stopping entirely? - What would you do with the team/resources if stopping? - How would you extract value from learnings?</p> <p>Your Recommendation: Which option and why?</p> <p>4. System Reframe (400-500 words)</p> <p>Go beyond tactical changes: - What assumptions underlie the original problem definition? - Who defined the problem, and what were their incentives/constraints? - How could you reframe the problem at a system level? - What would success look like under the reframed problem? - How would this reframe change the AI system (data, models, metrics, stakeholders)?</p> <p>5. Sunk Cost Analysis (300-400 words)</p> <ul> <li>What costs have been sunk?</li> <li>What psychological or organizational factors might bias toward persistence?</li> <li>How would you apply the New Manager Test?</li> <li>What pre-commitment rules would prevent sunk cost fallacy going forward?</li> </ul>"},{"location":"chapters/05-pivot/#evaluation-criteria","title":"Evaluation Criteria","text":"<p>Your pivot analysis will be evaluated on:</p> <ul> <li>Signal Recognition (25%): Ability to identify and classify strong signals with evidence</li> <li>Decision Framework (25%): Rigorous application of persist/pivot/stop criteria</li> <li>System Thinking (25%): Depth of system-level reframing, questioning root assumptions</li> <li>Sunk Cost Awareness (15%): Recognition of cognitive biases and mitigation strategies</li> <li>Clarity &amp; Evidence (10%): Clear writing, specific examples, logical argumentation</li> </ul>"},{"location":"chapters/05-pivot/#submission-format","title":"Submission Format","text":"<ul> <li>Length: 1,800-2,500 words</li> <li>Format: PDF or Markdown</li> <li>Include: Diagrams, metrics, or quotes as supporting evidence</li> <li>Due: [Date per course schedule]</li> </ul>"},{"location":"chapters/05-pivot/#example-topics","title":"Example Topics","text":"<p>If you don't have a personal project to analyze, consider:</p> <ul> <li>Amazon's Alexa Skill Store: Why didn't third-party skills succeed? Should Amazon pivot or persist?</li> <li>Healthcare Chatbots: When do symptom-checkers need to pivot to human handoff?</li> <li>Autonomous Delivery Robots: Urban sidewalk robots vs. warehouse robots\u2014different problems?</li> <li>AI Content Moderation: When to pivot from \"detect harmful content\" to \"promote healthy engagement\"?</li> <li>Resume Screening AI: Bias concerns\u2014tactical fixes or system reframe needed?</li> </ul> <p>Choose a case with enough public information to support analysis, or use a hypothetical scenario grounded in real technical and social constraints.</p> <p>Next Chapter Preview: Chapter 6 explores \"Alignment\" - ensuring your AI system's objectives match human values and stakeholder needs, even as both evolve over time. You'll learn techniques for value elicitation, objective specification, and long-term alignment under uncertainty.</p>"},{"location":"chapters/05-pivot/quiz/","title":"Chapter 5: Pivot - Quiz","text":"<p>Chapter Type: Advanced | Difficulty: Graduate-level | Total Questions: 10</p> <p>This quiz assesses your understanding of when and how to make persist, pivot, and stop decisions in AI problem framing. Focus areas include signal recognition, decision thresholds, sunk cost fallacy awareness, and system-level reframes.</p>"},{"location":"chapters/05-pivot/quiz/#question-1-remember-decision-options","title":"Question 1 (Remember) | Decision Options","text":"<p>Your team has spent 6 weeks building a classification model to predict which customers will churn. Early performance metrics show 78% accuracy, but stakeholder feedback indicates the business needs 85% accuracy to act on predictions reliably.</p> <p>What are the three core decision options your team faces at this point?</p> <p>A) Build a more complex model, hire more data scientists, or abandon the project entirely B) Persist with optimization, pivot to a different approach, or stop the project C) Collect more data, improve feature engineering, or switch to a regression model D) Deploy immediately, wait for more data, or consult with executives</p> <p>Correct Answer: B</p> <p>Explanation: The three fundamental decision options in The Loop are: - Persist: Continue with the current problem frame and solution approach - Pivot: Change the problem frame, approach, or resource allocation - Stop: Abandon the current direction as unviable</p> <p>Options A and C confuse specific tactics with strategic decision choices. Option D conflates decision-making with stakeholder management.</p>"},{"location":"chapters/05-pivot/quiz/#question-2-remember-signal-recognition-definition","title":"Question 2 (Remember) | Signal Recognition Definition","text":"<p>Which of the following best describes Signal Recognition in the context of pivot decisions?</p> <p>A) The ability to statistically detect anomalies in data streams B) The ability to identify early indicators that current strategy is working or needs to change C) The process of collecting metrics from production models D) The technique of filtering out noise from sensor data</p> <p>Correct Answer: B</p> <p>Explanation: Signal recognition is the cognitive skill of identifying early indicators or patterns that suggest whether the current approach should continue (success signal) or change (kill signal). It's distinct from data anomaly detection or production monitoring, though those activities may feed signal recognition.</p>"},{"location":"chapters/05-pivot/quiz/#question-3-understand-sunk-cost-fallacy-in-decision-making","title":"Question 3 (Understand) | Sunk Cost Fallacy in Decision-Making","text":"<p>A team has invested 8 weeks and $120,000 in building an end-to-end ML pipeline for demand forecasting. During a sprint retrospective, they discover that a simple statistical baseline (which takes 2 hours to build) would achieve 89% of the pipeline's accuracy at a fraction of the cost.</p> <p>What cognitive bias is most at play if the team decides to continue with the ML pipeline despite this discovery?</p> <p>A) Confirmation bias\u2014they only see evidence supporting their original approach B) Escalation of commitment\u2014continuing to invest despite evidence the current path is suboptimal C) Hammer bias\u2014they're overreliant on machine learning as a tool D) Anchoring bias\u2014they're fixated on the original 8-week timeline</p> <p>Correct Answer: B</p> <p>Explanation: Escalation of commitment describes the tendency to continue investing in a course of action despite evidence it is failing or suboptimal, driven by sunk costs and desire to avoid admitting past mistakes. The team has already \"spent\" the $120,000 and 8 weeks; the rational decision (pivot to the simpler approach) is emotionally difficult because it feels like acknowledging waste. This is the definition of sunk cost fallacy in action.</p>"},{"location":"chapters/05-pivot/quiz/#question-4-understand-decision-threshold-concept","title":"Question 4 (Understand) | Decision Threshold Concept","text":"<p>You're building a model to identify patients at high risk for hospital readmission. Your model outputs a probability between 0 and 1. The hospital can only accommodate intensive follow-up interventions for 15% of patients due to resource constraints.</p> <p>How does understanding Decision Threshold inform your pivot/persist decision?</p> <p>A) It determines which algorithm (neural networks vs. random forest) you should use B) It establishes the boundary probability value used to convert continuous model outputs into actionable decisions C) It tells you how much data you need to collect D) It measures whether your training data is representative of production</p> <p>Correct Answer: B</p> <p>Explanation: A decision threshold is the boundary value that converts the model's continuous output (probability) into a discrete action (intervene or don't intervene). In this case, you'd set the threshold so that the top 15% of patients by risk score are targeted. If the model cannot distinguish patients sharply enough to enable meaningful action at that threshold, it becomes a signal to pivot away from prediction and toward other approaches.</p>"},{"location":"chapters/05-pivot/quiz/#question-5-apply-recognizing-a-pivot-signal","title":"Question 5 (Apply) | Recognizing a Pivot Signal","text":"<p>A recommendation system has been in production for 6 months. Initial metrics showed a 12% increase in user engagement. However, you notice: - Engagement gains have plateaued for 8 weeks despite weekly model retraining - User satisfaction surveys show engagement is increasingly driven by novelty shock rather than genuine preference matches - A/B tests of competing recommendation strategies show no statistical difference</p> <p>Which decision best reflects the signal your data is sending?</p> <p>A) Persist\u2014you should continue optimizing the model parameters to break through the plateau B) Pivot\u2014the current frame (accuracy-driven recommendations) may not address why engagement is stalling C) Stop\u2014recommendation systems cannot sustain engagement improvements over time D) Persist with more data\u2014collect user feedback to improve the training signal</p> <p>Correct Answer: B</p> <p>Explanation: The plateau, lack of improvement despite effort, and user satisfaction data all suggest the current problem frame (predict what users will engage with) may not be addressing the underlying issue (sustained preference matching). A pivot might reframe the problem toward: diversity, serendipity, preference discovery, or longer-term satisfaction rather than immediate engagement. Continuing to optimize a fundamentally limited approach (Answer A) represents escalation of commitment.</p>"},{"location":"chapters/05-pivot/quiz/#question-6-apply-sunk-cost-vs-sound-decision","title":"Question 6 (Apply) | Sunk Cost vs. Sound Decision","text":"<p>Your team built a sophisticated NLP pipeline for analyzing customer support tickets (3 months of work, high complexity). A new intern asks: \"Why don't we just use the off-the-shelf sentiment analysis API? It's $500/month, handles our volume, and would save us massive maintenance overhead.\"</p> <p>Initial reaction from senior engineers: \"We've already invested so much in our custom pipeline\u2014we can't throw away all that work.\"</p> <p>How should leadership evaluate this intern's suggestion?</p> <p>A) Reject it outright\u2014the sunk cost of 3 months of work justifies continuing with the pipeline B) Evaluate based on forward-looking factors (cost, accuracy, maintenance, time to deploy improvements) independent of past investment C) Implement a hybrid approach combining both systems to recoup the sunk investment D) Conduct a full rewrite of the custom pipeline to prove its superiority before considering alternatives</p> <p>Correct Answer: B</p> <p>Explanation: Rational decision-making requires evaluating options based on forward-looking factors: future cost, performance, maintainability, and time to value. The 3 months already spent is a sunk cost\u2014it doesn't change whether the intern's alternative is better going forward. Good leaders make this distinction explicit and protect teams from escalation of commitment bias by asking: \"If we were starting today with this choice, which would we pick?\" If the answer is the API, that's a strong pivot signal.</p>"},{"location":"chapters/05-pivot/quiz/#question-7-analyze-system-level-reframe-in-pivoting","title":"Question 7 (Analyze) | System-Level Reframe in Pivoting","text":"<p>A bank built a model to detect fraudulent credit card transactions. The model achieves 95% accuracy but creates operational problems: legitimate transactions are blocked at high rates, customer frustration is mounting, and support costs are rising.</p> <p>Tactical pivots attempted: - Adjusted the decision threshold (still too many false positives) - Added new features to improve precision (marginal gains) - Switched to different algorithms (similar results)</p> <p>Why might a System-Level Reframe be necessary here? Which reframe makes most sense?</p> <p>A) The problem frame is still \"identify fraud with high accuracy\"\u2014a System-Level Reframe might shift to \"minimize fraud AND customer friction\" or \"balance fraud loss with operational cost\" B) The system architecture is outdated and needs modernization C) The bank should build separate models for different customer segments D) The team needs to hire fraud domain experts to improve feature engineering</p> <p>Correct Answer: A</p> <p>Explanation: A system-level reframe changes the boundaries, stakeholders, or goals of the problem, not just the tactical approach. The team has been optimizing within a narrow frame (accuracy), but the system-level problem includes multiple competing objectives: fraud prevention, customer trust, operational efficiency, and business cost. Reframing to explicitly balance these creates room for new solutions\u2014like risk-tiered interventions (high-confidence fraud blocks, medium-confidence challenges, low-confidence approvals), real-time customer contact, or fraud scoring without hard blocks.</p>"},{"location":"chapters/05-pivot/quiz/#question-8-analyze-multiple-pivot-types-decision","title":"Question 8 (Analyze) | Multiple Pivot Types Decision","text":"<p>You're working on a churn prediction project where diagnosis revealed: \"Customers leave because of unresolved support tickets, not product dissatisfaction.\"</p> <p>Your team is debating how to respond: - Person A: Pivot the Problem\u2014move from predicting churn to solving support quality - Person B: Pivot the Approach\u2014use the churn model but add support ticket features - Person C: Pivot the Tool\u2014switch from ML classification to a rules-based support escalation system</p> <p>Which pivot type(s) directly address the root cause you've identified?</p> <p>A) Only Person A's Problem Pivot B) Person B's Approach Pivot alone addresses it best C) Persons A and C; pivoting the problem and/or tool aligns with the diagnosis D) All three pivots are equally valid responses</p> <p>Correct Answer: C</p> <p>Explanation: - Problem Pivot (Person A): Directly reframes from \"predict churn\" to \"prevent unresolved tickets,\" addressing root cause - Approach Pivot (Person B): Optimizes the original frame rather than addressing the root cause\u2014this is escalation of commitment - Tool Pivot (Person C): A rules-based escalation system might be more appropriate than ML classification for a deterministic support problem</p> <p>Either the problem or tool pivot (or both) align with your diagnosis. The approach pivot keeps you in the original frame.</p>"},{"location":"chapters/05-pivot/quiz/#question-9-analyze-evaluating-conflicting-signals","title":"Question 9 (Analyze) | Evaluating Conflicting Signals","text":"<p>Your demand forecasting model is showing mixed signals: - Production accuracy has remained stable at 87% for 12 weeks - Stakeholders are increasingly frustrated that the model misses seasonal patterns (e.g., Black Friday spike) - A/B testing shows the model-driven inventory decisions outperform human judgment by 4% in waste reduction - The model is computationally expensive (requires 6 hours to retrain daily) - New competitors are using real-time pricing signals your model doesn't incorporate</p> <p>How should you prioritize these conflicting signals when deciding persist/pivot/stop?</p> <p>A) Persist because overall accuracy is stable and business impact is positive B) Stop because competitors have newer approaches C) Systematically weight signals by business impact and feasibility\u2014the seasonal pattern misses and real-time data gaps may signal a pivot toward ensemble or hybrid approaches D) Pivot immediately without further analysis because 6 hours of computation is unacceptable</p> <p>Correct Answer: C</p> <p>Explanation: Real-world decisions involve conflicting signals. The framework asks you to: 1. Weight signals by business impact (customer frustration from seasonal misses, competitive threat from missing real-time data) 2. Assess feasibility of addressing each signal 3. Determine if the current frame is sufficient or needs reframing</p> <p>The positive business impact argues for persistence in the core idea, but the unaddressed seasonal and real-time patterns suggest a pivot toward more sophisticated approaches (ensemble with seasonal models, real-time data integration) rather than stop.</p>"},{"location":"chapters/05-pivot/quiz/#question-10-evaluate-justifying-a-pivot-decision","title":"Question 10 (Evaluate) | Justifying a Pivot Decision","text":"<p>You're presenting a decision to pivot away from a supervised learning approach to churn prediction and instead adopt a system redesign focused on reducing the friction points causing churn (based on recent user research).</p> <p>A skeptical VP asks: \"We spent $300K building the ML system. Why should we trust your diagnosis over the work we've already done?\"</p> <p>Which justification most directly addresses the VP's legitimate concern and demonstrates sound reasoning?</p> <p>A) \"The new approach will cost less and deploy faster, so the ROI will be better\" B) \"The user research identified root causes that the data alone cannot reveal; the ML model optimizes for a symptom (churn prediction) rather than the underlying problem (friction). We're pivoting because the problem frame itself was incomplete\" C) \"The old approach was based on flawed assumptions that your team didn't catch during initial design\" D) \"New machine learning techniques have become available that will definitely work better\"</p> <p>Correct Answer: B</p> <p>Explanation: This answer demonstrates: - Clear problem reframing: Distinguishes between the symptom (churn events) and root cause (system friction) - Reasoning transparency: Explains why the original frame was incomplete, not attacking the team's competence - Forward-looking perspective: Focuses on what will actually solve the business problem, not on justifying past investment - Diagnostic confidence: Grounds the pivot in additional evidence (user research) that the original problem frame missed</p> <p>Answers A and D appeal to cost/novelty without addressing root cause. Answer C attacks the original team, creating defensiveness rather than alignment. Answer B invites the VP to understand why the pivot is necessary, converting skepticism into support.</p>"},{"location":"chapters/05-pivot/quiz/#answer-key-summary","title":"Answer Key Summary","text":"Question Type Answer Concept 1 Remember B Decision Options (Persist/Pivot/Stop) 2 Remember B Signal Recognition 3 Understand B Escalation of Commitment / Sunk Cost 4 Understand B Decision Threshold 5 Apply B Recognizing Pivot Signals 6 Apply B Sunk Cost Fallacy in Decision-Making 7 Analyze A System-Level Reframe 8 Analyze C Pivot Types (Problem/Approach/Tool) 9 Analyze C Conflicting Signal Prioritization 10 Evaluate B Justifying Pivot with Sound Reasoning"},{"location":"chapters/05-pivot/quiz/#distribution-achieved","title":"Distribution Achieved","text":"<ul> <li>Remember: 2 questions (20%)</li> <li>Understand: 2 questions (20%)</li> <li>Apply: 2 questions (20%)</li> <li>Analyze: 3 questions (30%)</li> <li>Evaluate: 1 question (10%)</li> <li>Create: 0 questions (0%)</li> </ul> <p>Total: 10 questions aligned to Advanced chapter requirements and core teaching focus areas.</p>"},{"location":"chapters/06-application/","title":"Application - Full Case Studies","text":""},{"location":"chapters/06-application/#summary","title":"Summary","text":"<p>This capstone chapter synthesizes all course concepts through detailed case studies of AI projects that succeeded or failed based on problem framing decisions. You'll analyze complete applications of The Loop framework across classic ML and GenAI domains, identify common pivot patterns that transcend technology choices, and develop a practical Monday Morning Checklist for applying problem framing skills to any AI initiative.</p> <p>The chapter moves from theoretical understanding to professional practice, demonstrating how experienced practitioners diagnose misaligned projects early, pivot systematically rather than reactively, and build organizational muscle for better upstream decision-making. By studying both successful pivots and costly failures, you'll develop pattern recognition abilities that make problem framing feel less like following a checklist and more like exercising practiced judgment.</p>"},{"location":"chapters/06-application/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter integrates concepts from across the entire course:</p> <ol> <li>Problem Framing (Chapters 1-3)</li> <li>The Loop Framework (Chapter 3)</li> <li>Outcome Metrics (Chapter 3)</li> <li>Problem Deconstruction (Chapter 3)</li> <li>Alternatives Menu (Chapter 2)</li> <li>Trade-off Analysis (Chapter 3)</li> <li>Signals and Diagnosis (Chapter 4)</li> <li>Pivot Decisions (Chapter 5)</li> <li>Persist Decisions (Chapter 5)</li> <li>Kill Signals (Chapter 4)</li> <li>Pattern Bridges (synthesis concept)</li> <li>Monday Morning Checklist (applied framework)</li> <li>Case Study Analysis (professional skill)</li> <li>Cautionary Tales (learning from failures)</li> <li>ML Pivot (Chapter 5)</li> </ol>"},{"location":"chapters/06-application/#prerequisites","title":"Prerequisites","text":"<p>This chapter assumes mastery of all previous chapters:</p> <ul> <li>Chapter 1: The AI Problem Framing Mindset</li> <li>Chapter 2: AI Solution Alternatives</li> <li>Chapter 3: The Loop Framework</li> <li>Chapter 4: Diagnosis - Reading Signals</li> <li>Chapter 5: Pivot - Acting on Signals</li> </ul>"},{"location":"chapters/06-application/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this chapter, you will be able to:</p> <ol> <li>Analyze complete AI case studies using The Loop framework to identify where framing succeeded or failed</li> <li>Diagnose common failure patterns across diverse AI projects (classic ML and GenAI)</li> <li>Identify pivot patterns that recur across different problem domains and solution types</li> <li>Apply the Monday Morning Checklist to assess new AI projects systematically</li> <li>Synthesize course concepts into a coherent problem framing practice ready for professional application</li> <li>Evaluate peer problem framings using the same critical lens applied to case studies</li> <li>Create a complete problem framing portfolio demonstrating systematic thinking from ambiguity to implementation</li> </ol>"},{"location":"chapters/06-application/#introduction-what-separates-success-from-failure","title":"Introduction: What Separates Success from Failure","text":"<p>You've seen the statistics: 85% of AI projects fail to deliver business value. The technical literature focuses on model performance, data quality, and infrastructure challenges. But when experienced practitioners diagnose failures post-mortem, a different pattern emerges.</p> <p>Projects fail because teams built the wrong thing\u2014technically competent solutions to incorrectly framed problems. A churn prediction model with 92% accuracy that doesn't reduce churn. A fraud detection system so sensitive it blocks legitimate transactions. A chatbot that answers questions perfectly but doesn't solve the underlying knowledge access problem.</p> <p>The difference between successful and unsuccessful AI practitioners is not model architecture knowledge or coding skill. It's the systematic habit of asking upstream questions before committing to implementation:</p> <ul> <li>What outcome are we actually pursuing?</li> <li>What are we assuming about the problem structure?</li> <li>What alternatives exist beyond the obvious approach?</li> <li>How will we know if we're solving the right problem?</li> <li>What signals would tell us to pivot?</li> </ul> <p>This chapter shows you these habits in action through detailed case studies. You'll see how practitioners applied The Loop to navigate ambiguity, how they recognized early signals of misalignment, and how they made evidence-based decisions to persist or pivot. More importantly, you'll see the common patterns that transcend specific domains\u2014the repeatable structures that make problem framing a transferable skill rather than domain-specific intuition.</p>"},{"location":"chapters/06-application/#classic-ml-pivot-case-studies","title":"Classic ML Pivot Case Studies","text":""},{"location":"chapters/06-application/#case-study-1-churn-prediction-customer-success-intervention-system","title":"Case Study 1: Churn Prediction \u2192 Customer Success Intervention System","text":"<p>Initial Framing (Incorrect)</p> <p>A B2B SaaS company framed their retention challenge as: \"We need to predict which customers will churn so we can intervene.\" The team built a classification model using historical usage data, support tickets, and billing patterns to predict 30-day churn risk.</p> <p>Loop Analysis - Original Frame</p> <p>Outcome: Reduce customer churn by 15% within 6 months Atomic Unit: Individual customer account Decomposition: - Identify at-risk customers (model task) - Trigger alerts to customer success team - Execute retention campaigns (out of scope for ML team)</p> <p>Alternatives Considered: 1. Logistic regression (interpretable coefficients for CS team) 2. Gradient boosting (higher accuracy, less interpretable) 3. Rule-based risk scoring (simple, no training required)</p> <p>Trade-offs: Team chose gradient boosting for accuracy, accepting interpretability loss</p> <p>Signals Defined: - Model accuracy (AUC-ROC on held-out test set) - Churn rate reduction (6-month trend) - Customer success team adoption (% of alerts acted on)</p> <p>What Actually Happened</p> <p>The model achieved 89% AUC-ROC and was deployed to production. The customer success team received alerts for high-risk accounts. After 3 months:</p> <ul> <li>Churn rate: unchanged</li> <li>CS team alert response rate: 23%</li> <li>Most common CS team feedback: \"We already knew these customers were struggling\"</li> </ul> <p>Diagnosis Through Signals</p> <p>The weak signal came not from model metrics (which looked excellent) but from user behavior: the CS team ignored most alerts. Interviews revealed:</p> <ul> <li>CS team already tracked usage patterns manually</li> <li>The model identified customers currently struggling, not those who would struggle soon</li> <li>By the time predictions triggered, CS team had exhausted intervention options</li> <li>The real constraint was what intervention to apply, not who needs help</li> </ul> <p>The Pivot</p> <p>The team returned to The Loop with new understanding:</p> <p>New Outcome: Enable CS team to intervene 60-90 days before risk escalates (not 30 days)</p> <p>New Decomposition: - Identify leading indicators of future struggle (not current struggle) - Recommend specific interventions based on struggle type - Track intervention effectiveness to learn what works</p> <p>New Alternatives: 1. Early warning system (predict engagement decline before revenue impact) 2. Intervention recommendation engine (match struggle patterns to proven interventions) 3. Causal inference model (identify which interventions actually reduce churn vs. selection effects)</p> <p>New Trade-offs: Accepted lower precision in early warnings in exchange for more lead time; prioritized actionable insights over prediction accuracy</p> <p>Outcome After Pivot</p> <p>The team built an early warning system that flagged accounts showing usage decline patterns 60 days before historical churn points, paired with recommended interventions based on similar past customers who were saved. Six months post-pivot:</p> <ul> <li>Churn reduction: 12% (close to original goal)</li> <li>CS team adoption: 81% of recommendations acted on</li> <li>New discovery: Product onboarding gaps were the #1 addressable churn driver</li> </ul> <p>Key Lessons</p> <ol> <li>Prediction accuracy doesn't equal business value: The original model was technically excellent but strategically misaligned</li> <li>User behavior is a signal, not noise: CS team ignoring alerts was data, not a training problem</li> <li>Atomic unit matters: Moving from \"at-risk customer\" to \"intervention opportunity\" changed the entire problem structure</li> <li>Leading indicators &gt; lagging indicators: Predicting current state is easier but less valuable than predicting future risk while it's preventable</li> </ol> <p>Pattern: This case demonstrates the Prediction \u2192 Action pivot\u2014moving from forecasting outcomes to enabling interventions.</p>"},{"location":"chapters/06-application/#case-study-2-fraud-detection-risk-scoring-and-adaptive-limits","title":"Case Study 2: Fraud Detection \u2192 Risk Scoring and Adaptive Limits","text":"<p>Initial Framing (Incorrect)</p> <p>A fintech company framed payment fraud as: \"We need to detect fraudulent transactions before they complete.\" The team built a binary classifier to approve or reject transactions in real-time based on historical fraud patterns.</p> <p>Loop Analysis - Original Frame</p> <p>Outcome: Reduce fraud losses by 40% while maintaining customer experience</p> <p>Atomic Unit: Individual transaction</p> <p>Decomposition: - Extract transaction features (amount, merchant, location, time) - Train classifier on historical fraud labels - Deploy real-time prediction API - Block transactions predicted as fraud</p> <p>Alternatives Considered: 1. Rule-based system (high precision, low recall) 2. Random forest (balanced performance) 3. Neural network (highest accuracy potential) 4. Anomaly detection (catch novel fraud patterns)</p> <p>Trade-offs: Chose random forest for balance of accuracy and interpretability; accepted latency constraint of &lt;100ms</p> <p>Signals Defined: - Fraud detection rate (% of fraud caught) - False positive rate (% of legitimate transactions blocked) - Customer complaint rate (proxy for UX impact)</p> <p>What Actually Happened</p> <p>The model achieved 94% fraud detection with 2% false positive rate\u2014excellent by industry standards. But three months post-deployment:</p> <ul> <li>Fraud losses: reduced 35% (close to goal)</li> <li>Customer complaints: increased 300%</li> <li>Revenue impact: $2M monthly from abandoned purchases after false declines</li> <li>VIP customer churn: +8% quarter-over-quarter</li> </ul> <p>Diagnosis Through Signals</p> <p>The false positive rate looked low (2%), but the absolute volume was crushing customer experience. Analysis revealed:</p> <ul> <li>2% of 50M monthly transactions = 1M false declines</li> <li>Customers who experienced false declines had 40% cart abandonment on retry</li> <li>High-value customers (who spent more, triggering fraud signals) were disproportionately affected</li> <li>The binary approve/reject decision treated all risks equally</li> </ul> <p>The Pivot</p> <p>New Outcome: Minimize total cost (fraud losses + customer friction + lost revenue) rather than just fraud losses</p> <p>New Decomposition: - Estimate transaction risk on continuous scale (not binary) - Adjust friction level based on risk score (not blanket block) - Segment customers by value and risk tolerance - Monitor customer behavior post-friction</p> <p>New Alternatives: 1. Risk scoring + adaptive friction (step-up authentication for risky transactions) 2. Customer-specific risk thresholds (VIP customers get higher limits) 3. Velocity limits + real-time risk adjustment (dynamic fraud detection) 4. Hybrid approach: rules for obvious fraud, ML for gray area, humans for VIPs</p> <p>New Trade-offs: Accepted slightly higher fraud losses in exchange for better customer experience and revenue retention; prioritized precision over recall for high-friction interventions</p> <p>Outcome After Pivot</p> <p>The team rebuilt the system as a risk scoring platform with four response tiers:</p> <ol> <li>Very low risk: instant approval</li> <li>Low-medium risk: SMS verification</li> <li>Medium-high risk: additional authentication</li> <li>Very high risk: human review</li> </ol> <p>Customer segmentation allowed VIP customers higher risk thresholds before friction was introduced. Six months post-pivot:</p> <ul> <li>Total cost reduction: 52% (fraud + friction + revenue)</li> <li>Customer complaints: returned to baseline</li> <li>VIP churn: back to pre-deployment levels</li> <li>Fraud losses: reduced 31% (less than original goal but better total outcome)</li> </ul> <p>Key Lessons</p> <ol> <li>Optimize the right objective: Minimizing fraud losses alone created greater total cost</li> <li>Binary decisions are often wrong: Continuous risk scores enable proportional responses</li> <li>Atomic unit drives solution: Moving from \"transaction\" to \"customer transaction in context\" enabled segmentation</li> <li>Second-order effects matter: False positive rate was low but impact was devastating</li> </ol> <p>Pattern: This case demonstrates the Classification \u2192 Scoring pivot\u2014moving from binary decisions to continuous risk assessment with adaptive responses.</p>"},{"location":"chapters/06-application/#case-study-3-recommendation-engine-search-optimization","title":"Case Study 3: Recommendation Engine \u2192 Search Optimization","text":"<p>Initial Framing (Incorrect)</p> <p>An e-commerce platform framed product discovery as: \"We need to recommend products customers will buy.\" The team built a collaborative filtering recommendation system using purchase history and browsing behavior.</p> <p>Loop Analysis - Original Frame</p> <p>Outcome: Increase conversion rate by 20% through personalized recommendations</p> <p>Atomic Unit: User session</p> <p>Decomposition: - Collect user behavior data (clicks, purchases, time-on-page) - Train collaborative filtering model - Display personalized recommendations on homepage and product pages - Track click-through and conversion rates</p> <p>Alternatives Considered: 1. Collaborative filtering (user-user similarity) 2. Content-based filtering (item similarity) 3. Matrix factorization (latent factors) 4. Deep learning embeddings (learn complex patterns)</p> <p>Trade-offs: Chose matrix factorization for scalability and cold-start performance; accepted lack of interpretability</p> <p>Signals Defined: - Recommendation click-through rate (CTR) - Conversion rate from recommendations - Revenue attributed to recommendations - Coverage (% of products recommended)</p> <p>What Actually Happened</p> <p>The recommendation engine launched with strong initial metrics:</p> <ul> <li>Recommendation CTR: 8.3% (vs. 2.1% baseline for non-personalized)</li> <li>Conversion rate: +12% for users who clicked recommendations</li> </ul> <p>But after six months, concerning patterns emerged:</p> <ul> <li>Overall site conversion rate: +4% (far below 20% goal)</li> <li>Search usage: declining 15% quarter-over-quarter</li> <li>Customer surveys: \"Hard to find what I'm looking for\"</li> <li>Product catalog coverage: recommendations showed only 12% of inventory</li> </ul> <p>Diagnosis Through Signals</p> <p>The recommendation system was working too well\u2014it optimized for engagement with familiar products at the expense of discovery. Deep dive analysis revealed:</p> <ul> <li>Recommendations reinforced past purchases (users who bought hammers saw more hammers)</li> <li>Search abandonment was high (users gave up when search didn't find specific needs)</li> <li>Conversion gains came from existing intent, not discovery</li> <li>Users with specific needs (search-driven) had worse experience than browsers (recommendation-driven)</li> </ul> <p>The real problem: customers already knew what they wanted, but search couldn't find it.</p> <p>The Pivot</p> <p>New Outcome: Help customers find what they need, whether they know what it is or not</p> <p>New Decomposition: - Intent-driven customers (searching for specific items) \u2192 improve search relevance - Discovery-driven customers (browsing, open to suggestions) \u2192 improve recommendations - Measure success separately for each journey</p> <p>New Alternatives: 1. Semantic search (understand query intent, not just keywords) 2. Query expansion (suggest related terms for failed searches) 3. Hybrid search-recommendation (recommend when search returns few results) 4. Faceted navigation improvements (help customers filter effectively)</p> <p>New Trade-offs: Redirected 60% of engineering effort to search quality; accepted that recommendations serve a narrower use case than originally assumed</p> <p>Outcome After Pivot</p> <p>The team implemented semantic search using product embeddings, query expansion for ambiguous searches, and better failure handling when search returned no results. Recommendations were repositioned as \"inspiration\" rather than primary discovery mechanism. Six months post-pivot:</p> <ul> <li>Overall conversion rate: +18% (close to original goal)</li> <li>Search success rate: +34%</li> <li>Search usage: recovered to baseline levels</li> <li>Customer satisfaction scores: +22 points</li> <li>Recommendation CTR: dropped to 4.1% (but total conversions still higher)</li> </ul> <p>Key Lessons</p> <ol> <li>Success on proxies can mask failure on outcomes: Recommendation metrics looked good while site goals suffered</li> <li>User segmentation matters: Different customer intents need different solutions</li> <li>Optimization can create blindness: Focusing on recommendations prevented seeing search quality issues</li> <li>Coverage metrics reveal bias: Only 12% catalog coverage was a red flag ignored during initial deployment</li> </ol> <p>Pattern: This case demonstrates the Engagement Optimization \u2192 User Need Fulfillment pivot\u2014moving from maximizing clicks to solving actual customer problems.</p>"},{"location":"chapters/06-application/#genai-pivot-case-studies","title":"GenAI Pivot Case Studies","text":""},{"location":"chapters/06-application/#case-study-4-customer-support-chatbot-retrieval-augmented-knowledge-base","title":"Case Study 4: Customer Support Chatbot \u2192 Retrieval-Augmented Knowledge Base","text":"<p>Initial Framing (Incorrect)</p> <p>A software company framed support automation as: \"We need a chatbot that can answer customer questions to reduce support ticket volume.\" The team fine-tuned a language model on historical support tickets and documentation.</p> <p>Loop Analysis - Original Frame</p> <p>Outcome: Reduce support ticket volume by 30% through automated responses</p> <p>Atomic Unit: Customer question</p> <p>Decomposition: - Collect training data (support tickets, documentation, FAQs) - Fine-tune LLM on company-specific language and solutions - Deploy chatbot interface on support portal - Measure deflection rate (questions answered without human)</p> <p>Alternatives Considered: 1. Rule-based FAQ matching (simple, limited coverage) 2. Retrieval-only system (accurate but not conversational) 3. Fine-tuned model (conversational, domain-aware) 4. RAG (Retrieval-Augmented Generation) (combines retrieval accuracy with generation)</p> <p>Trade-offs: Chose fine-tuning for end-to-end conversational experience; accepted higher compute cost and maintenance burden</p> <p>Signals Defined: - Deflection rate (% of questions answered without escalation) - Answer accuracy (human evaluation sample) - User satisfaction (thumbs up/down) - Response latency</p> <p>What Actually Happened</p> <p>The chatbot launched with promising early metrics:</p> <ul> <li>Deflection rate: 38% (exceeded goal)</li> <li>User satisfaction: 72% thumbs-up rate</li> <li>Response latency: &lt;2 seconds</li> </ul> <p>But after three months, the signals deteriorated:</p> <ul> <li>Deflection rate: dropped to 19%</li> <li>User satisfaction: dropped to 51%</li> <li>Support team escalations: increasingly frustrated customers</li> <li>Common complaint: \"The bot gave me outdated information\"</li> </ul> <p>Diagnosis Through Signals</p> <p>The fine-tuned model had learned patterns from historical tickets, but couldn't adapt to:</p> <ul> <li>Product updates (features changed, but model training was static)</li> <li>Edge cases not in training data (hallucinated solutions)</li> <li>Multi-step troubleshooting (gave generic advice instead of systematic diagnosis)</li> </ul> <p>The real problem emerged from qualitative analysis: customers didn't want conversation\u2014they wanted correct, current, specific answers.</p> <p>The Pivot</p> <p>New Outcome: Help customers find accurate solutions faster (not necessarily through conversation)</p> <p>New Decomposition: - Knowledge retrieval (find relevant documentation) - Answer synthesis (summarize retrieved content) - Confidence assessment (know when to escalate vs. answer) - Content freshness (detect when documentation is outdated)</p> <p>New Alternatives: 1. Pure retrieval (return documentation links, no generation) 2. RAG with conservative generation (cite sources, admit uncertainty) 3. Structured troubleshooting trees (guide diagnosis step-by-step) 4. Hybrid: retrieval for known issues, human for edge cases</p> <p>New Trade-offs: Accepted less conversational UX in exchange for accuracy and transparency; prioritized source citation over fluent generation</p> <p>Outcome After Pivot</p> <p>The team rebuilt the system using RAG with strict source citation requirements. When confidence was low, the system showed relevant documentation and offered human escalation. For common issues, it displayed step-by-step troubleshooting guides. Six months post-pivot:</p> <ul> <li>Deflection rate: 41% (sustained above goal)</li> <li>User satisfaction: 84% (higher than original peak)</li> <li>Escalation quality: support team reported better-informed customers</li> <li>Documentation coverage: system surfaced gaps in docs, leading to content improvements</li> <li>Hallucination rate: &lt;2% (vs. 18% with fine-tuned approach)</li> </ul> <p>Key Lessons</p> <ol> <li>Conversational \u2260 helpful: Users cared about accuracy, not chatbot personality</li> <li>Fine-tuning creates staleness: Static training can't keep up with product evolution</li> <li>Transparency builds trust: Showing sources increased confidence even when answers were uncertain</li> <li>Failure modes matter more than success cases: 18% hallucination rate destroyed trust despite 82% accuracy</li> </ol> <p>Pattern: This case demonstrates the Generation \u2192 Retrieval pivot\u2014moving from learned responses to grounded information access.</p>"},{"location":"chapters/06-application/#case-study-5-document-summarization-structured-information-extraction","title":"Case Study 5: Document Summarization \u2192 Structured Information Extraction","text":"<p>Initial Framing (Incorrect)</p> <p>A legal services firm framed contract review as: \"We need to summarize contracts so lawyers can review them faster.\" The team built a multi-document summarization system using LLMs to generate executive summaries of legal contracts.</p> <p>Loop Analysis - Original Frame</p> <p>Outcome: Reduce contract review time by 50%</p> <p>Atomic Unit: Individual contract document</p> <p>Decomposition: - Ingest contract PDFs - Extract text and structure - Generate executive summary highlighting key terms - Display summary to reviewing lawyer</p> <p>Alternatives Considered: 1. Extractive summarization (select important sentences) 2. Abstractive summarization (generate new summary text) 3. Template-based extraction (fill standard contract fields) 4. Clause classification (identify contract types)</p> <p>Trade-offs: Chose abstractive summarization for readability and conciseness; accepted risk of missed details</p> <p>Signals Defined: - Summary quality (lawyer rating 1-5) - Review time reduction (before/after comparison) - Lawyer adoption rate (% using summaries) - Error catch rate (missed critical terms)</p> <p>What Actually Happened</p> <p>Initial deployment showed mixed results:</p> <ul> <li>Summary quality: 3.8/5.0 average rating</li> <li>Review time: 20% reduction (far below 50% goal)</li> <li>Adoption rate: 62%</li> <li>Error catch rate: lawyers still read full contracts</li> </ul> <p>Qualitative feedback revealed the core issue: lawyers didn't need summaries\u2014they needed specific information quickly.</p> <p>Typical lawyer workflow: 1. Check liability cap amounts 2. Verify termination clauses 3. Compare payment terms to standard 4. Flag unusual provisions</p> <p>Summaries provided narrative overview but didn't answer specific questions efficiently.</p> <p>Diagnosis Through Signals</p> <p>The 20% time reduction came from lawyers skimming summaries first, but they still needed to search the full document for specific clauses. The summarization framing assumed lawyers wanted to understand contracts generally, but the actual task was finding specific provisions quickly.</p> <p>The Pivot</p> <p>New Outcome: Enable lawyers to extract specific contract provisions 10x faster than manual search</p> <p>New Decomposition: - Clause identification (detect liability, termination, payment, etc.) - Structured extraction (pull specific values: dates, amounts, parties) - Comparison to standards (flag deviations from firm templates) - Search and navigation (jump to relevant sections)</p> <p>New Alternatives: 1. Structured extraction with validation (extract key fields, verify completeness) 2. Clause-level QA (answer specific questions about contract terms) 3. Comparison dashboard (show standard vs. actual terms side-by-side) 4. Hybrid: extraction for standard clauses, QA for ad-hoc questions</p> <p>New Trade-offs: Accepted higher upfront effort to define extraction schema; prioritized precision over coverage (extract fewer things correctly vs. everything approximately)</p> <p>Outcome After Pivot</p> <p>The team built a structured extraction system that identified 25 critical contract elements (liability caps, termination notice periods, payment terms, etc.) and displayed them in a standardized dashboard. When lawyers needed non-standard information, a QA interface allowed natural language questions grounded in the contract text. Six months post-pivot:</p> <ul> <li>Review time: 58% reduction (exceeded goal)</li> <li>Adoption rate: 94%</li> <li>Lawyer satisfaction: 4.6/5.0</li> <li>Error catch rate: improved (structured format made gaps obvious)</li> <li>New use case: automated contract comparison for negotiations</li> </ul> <p>Key Lessons</p> <ol> <li>Summarization solves a different problem than search: Lawyers didn't need less text\u2014they needed specific answers</li> <li>Structured output &gt; narrative output: Standardized extraction enabled comparison and review workflows</li> <li>Coverage/precision trade-off: Extracting 25 things correctly was better than 100 things approximately</li> <li>User workflow determines UX: Understanding how lawyers use information shaped the interface</li> </ol> <p>Pattern: This case demonstrates the Summarization \u2192 Extraction pivot\u2014moving from condensed narrative to structured data retrieval.</p>"},{"location":"chapters/06-application/#case-study-6-content-generation-content-assistance","title":"Case Study 6: Content Generation \u2192 Content Assistance","text":"<p>Initial Framing (Incorrect)</p> <p>A marketing agency framed blog content creation as: \"We need AI to generate blog posts automatically to scale content production.\" The team built a system that generated full blog articles from topic keywords and outlines.</p> <p>Loop Analysis - Original Frame</p> <p>Outcome: Increase content production by 5x while maintaining quality</p> <p>Atomic Unit: Individual blog post</p> <p>Decomposition: - Input: topic keywords, target audience, desired length - Generation: produce full blog post with introduction, body, conclusion - Review: human editor approves or requests regeneration - Publish: deploy to CMS</p> <p>Alternatives Considered: 1. Template-based generation (fill structured templates) 2. Full generative model (end-to-end article creation) 3. Outline expansion (generate from detailed human outline) 4. Section-by-section generation (break into chunks)</p> <p>Trade-offs: Chose outline expansion for balance of quality and automation; accepted that human involvement was still needed</p> <p>Signals Defined: - Content volume (posts per week) - First-pass approval rate (% of generations published without major edits) - Engagement metrics (traffic, time-on-page, shares) - Editorial time per post</p> <p>What Actually Happened</p> <p>The system increased output volume dramatically:</p> <ul> <li>Content volume: 6x baseline (exceeded goal)</li> <li>First-pass approval rate: 34%</li> <li>Editorial time: 45 minutes per post (down from 3 hours)</li> </ul> <p>But engagement metrics revealed problems:</p> <ul> <li>Traffic: unchanged (more content didn't attract more readers)</li> <li>Time-on-page: decreased 22%</li> <li>Shares: decreased 41%</li> <li>SEO rankings: declining for competitive keywords</li> </ul> <p>Diagnosis Through Signals</p> <p>The generated content was generic, lacked unique insights, and didn't match the agency's editorial voice. Editors spent most time rewriting for voice and adding original analysis\u2014exactly the high-value work they hoped to automate.</p> <p>The fundamental misalignment: clients valued the agency for unique perspective and expertise, not just content volume. Generic AI-generated posts competed poorly with existing content and damaged the brand.</p> <p>The Pivot</p> <p>New Outcome: Enable writers to produce higher-quality content faster (not more content automatically)</p> <p>New Decomposition: - Research assistance (gather relevant sources and data) - Outline generation (suggest structure based on topic) - Draft sections (generate first drafts that writers refine) - Editing assistance (improve clarity, fix grammar, suggest rephrasings)</p> <p>New Alternatives: 1. Research + outline tool (help planning, writers do drafting) 2. Section-level assistance (generate rough drafts for revision) 3. Editing copilot (improve human-written drafts) 4. Hybrid: AI for research and structure, humans for insights and voice</p> <p>New Trade-offs: Accepted lower content volume in exchange for higher per-post quality; prioritized editorial control over automation</p> <p>Outcome After Pivot</p> <p>The team rebuilt the system as a writing assistant that helped with research (finding relevant studies, statistics, examples), generated structural outlines, and offered alternative phrasings during editing. Writers retained full creative control. Six months post-pivot:</p> <ul> <li>Content volume: 2.8x baseline (lower than auto-generation but sustainable)</li> <li>Editorial time: 1.5 hours per post (more than auto-generation, less than baseline)</li> <li>Engagement metrics: all exceeded baseline (traffic +18%, time-on-page +12%, shares +31%)</li> <li>Client retention: improved (content quality strengthened brand)</li> <li>SEO rankings: recovered and exceeded pre-AI levels</li> </ul> <p>Key Lessons</p> <ol> <li>More \u2260 better: Volume without quality damaged the brand</li> <li>Augmentation &gt; automation: Helping experts was more valuable than replacing them</li> <li>Voice and perspective matter: Generic content competed poorly with existing web content</li> <li>User control is a feature: Writers trusted and adopted tools that enhanced rather than replaced their judgment</li> </ol> <p>Pattern: This case demonstrates the Full Automation \u2192 Human-AI Collaboration pivot\u2014moving from end-to-end generation to expert assistance.</p>"},{"location":"chapters/06-application/#pattern-bridges-common-pivot-patterns-across-domains","title":"Pattern Bridges: Common Pivot Patterns Across Domains","text":"<p>The case studies above span different domains (B2B SaaS, fintech, e-commerce, support, legal, marketing) and different AI approaches (classification, recommendation, chatbot, summarization, generation). Yet they share recurring patterns\u2014structural similarities that make pivots recognizable even in unfamiliar contexts.</p> <p>These pattern bridges are transferable mental models. When you encounter a new AI problem, recognizing which pattern it matches helps you anticipate likely failure modes and consider alternative framings proactively.</p>"},{"location":"chapters/06-application/#pattern-1-prediction-action-churn-case","title":"Pattern 1: Prediction \u2192 Action (Churn Case)","text":"<p>Structure: Initial framing focuses on forecasting outcomes. Pivot shifts to enabling interventions.</p> <p>Why It Recurs: - Prediction feels like the ML task (it's what models do) - But business value comes from acting on predictions, not just knowing them - Action requires lead time, specificity, and intervention options</p> <p>Other Examples: - Equipment failure prediction \u2192 Maintenance scheduling optimization - Hospital readmission prediction \u2192 Discharge planning improvement - Employee attrition prediction \u2192 Retention program targeting</p> <p>Recognition Signals: - Stakeholders ignore predictions they \"already knew\" - High prediction accuracy but no outcome improvement - Users ask \"what should I do about this?\" after seeing predictions</p> <p>Framing Questions: - What action will stakeholders take with this prediction? - How much lead time do they need to intervene? - What interventions are available and effective? - Is the constraint prediction quality or intervention capacity?</p>"},{"location":"chapters/06-application/#pattern-2-classification-scoring-fraud-case","title":"Pattern 2: Classification \u2192 Scoring (Fraud Case)","text":"<p>Structure: Initial framing uses binary decisions (approve/reject, fraud/legitimate). Pivot shifts to continuous risk scores with proportional responses.</p> <p>Why It Recurs: - Classification is simpler to implement (binary threshold) - But real-world costs are non-uniform (false positives on VIP customers hurt more) - Proportional responses (more friction for higher risk) balance trade-offs better</p> <p>Other Examples: - Loan approval (binary) \u2192 Credit risk scoring (tiered terms) - Content moderation (remove/allow) \u2192 Risk-based review queues (auto/review/escalate) - Applicant screening (hire/reject) \u2192 Interview prioritization (tiers)</p> <p>Recognition Signals: - Low false positive rate but high impact from false positives - Uniform treatment causing outsized harm to important segments - Calls for \"manual review\" on edge cases</p> <p>Framing Questions: - Are all errors equally costly? - Can we adjust response intensity to risk level? - What intermediate actions exist between accept and reject? - Which segments deserve different treatment?</p>"},{"location":"chapters/06-application/#pattern-3-engagement-need-fulfillment-recommendation-case","title":"Pattern 3: Engagement \u2192 Need Fulfillment (Recommendation Case)","text":"<p>Structure: Initial framing optimizes for interaction metrics (clicks, time-on-site). Pivot shifts to solving user goals (find what they need).</p> <p>Why It Recurs: - Engagement metrics are easy to measure and optimize - But engagement can be shallow (clicking without value) - User satisfaction comes from accomplishing goals, not just interacting</p> <p>Other Examples: - Video recommendations (watch time) \u2192 Content satisfaction (completion, ratings) - News feed (scroll depth) \u2192 Information value (learned something useful) - Search results (CTR) \u2192 Query success (found answer)</p> <p>Recognition Signals: - Metrics improving but user satisfaction declining - Users \"engaging\" more but accomplishing less - Surveys show frustration despite high usage</p> <p>Framing Questions: - What is the user trying to accomplish? - Are engagement metrics proxies for that goal or distractions? - What would \"success\" look like from the user's perspective? - Are we optimizing the platform's goals or the user's goals?</p>"},{"location":"chapters/06-application/#pattern-4-generation-retrieval-chatbot-case","title":"Pattern 4: Generation \u2192 Retrieval (Chatbot Case)","text":"<p>Structure: Initial framing uses learned generation (fine-tuning, training). Pivot shifts to grounded retrieval (RAG, search, citations).</p> <p>Why It Recurs: - Generation feels more \"AI-like\" (creative, conversational) - But generation risks hallucination and staleness - Many use cases value accuracy over fluency</p> <p>Other Examples: - Code generation \u2192 Code search with examples - Report writing \u2192 Template filling with data retrieval - Question answering \u2192 Source citation with excerpts</p> <p>Recognition Signals: - Hallucination complaints (users catch factual errors) - Requests for \"where did this come from?\" - Accuracy degrading over time (model staleness)</p> <p>Framing Questions: - Does the user need original content or access to existing knowledge? - How important is factual accuracy vs. conversational quality? - Can we ground responses in verifiable sources? - How will we keep information current?</p>"},{"location":"chapters/06-application/#pattern-5-summarization-extraction-legal-case","title":"Pattern 5: Summarization \u2192 Extraction (Legal Case)","text":"<p>Structure: Initial framing condenses information (summaries, abstracts). Pivot shifts to structured extraction (specific fields, searchable data).</p> <p>Why It Recurs: - Summarization seems helpful (less to read) - But users often want specific information, not shorter narrative - Structured extraction enables search, comparison, and automation</p> <p>Other Examples: - Meeting summaries \u2192 Action item extraction - Research paper summaries \u2192 Method and results extraction - Financial document summaries \u2192 Numerical data extraction</p> <p>Recognition Signals: - Users still search full documents after reading summaries - Requests for \"Can you pull out X?\" for specific fields - Need to compare across documents (easier with structured data)</p> <p>Framing Questions: - What specific questions do users have about these documents? - Do they need narrative understanding or specific data points? - Will they compare multiple documents (suggesting structured output)? - What format makes the information most actionable?</p>"},{"location":"chapters/06-application/#pattern-6-full-automation-human-ai-collaboration-content-case","title":"Pattern 6: Full Automation \u2192 Human-AI Collaboration (Content Case)","text":"<p>Structure: Initial framing replaces human work entirely. Pivot shifts to augmenting human expertise.</p> <p>Why It Recurs: - Automation promises maximum efficiency (remove human bottleneck) - But expertise, judgment, and creativity are hard to automate - Augmentation leverages AI for drudgework, human for high-value decisions</p> <p>Other Examples: - Automated diagnosis \u2192 Decision support for doctors - Automated code review \u2192 Suggestion tools for developers - Automated translation \u2192 Translation assistance for professional translators</p> <p>Recognition Signals: - Quality issues (AI alone doesn't match expert output) - Experts reject or ignore the automated system - Value comes from unique perspective AI can't replicate</p> <p>Framing Questions: - What parts of the workflow are tedious vs. requiring expertise? - Can AI handle routine cases while escalating edge cases? - Do humans trust fully automated outputs? - Where does irreplaceable human judgment matter?</p>"},{"location":"chapters/06-application/#pattern-recognition-in-practice","title":"Pattern Recognition in Practice","text":"<p>When encountering a new AI problem, ask:</p> <ol> <li>Does it match a known pattern? (Use the six above as starting templates)</li> <li>What failure mode does this pattern predict? (Each has characteristic warning signs)</li> <li>What alternative framing would avoid that failure? (Each pattern has an alternative structure)</li> <li>What signals would tell you which framing is correct? (Design experiments to test)</li> </ol> <p>This pattern-based thinking accelerates problem framing by providing starting hypotheses rather than blank-slate exploration. You're not reinventing analysis for each new problem\u2014you're applying structured frameworks informed by past failures.</p>"},{"location":"chapters/06-application/#the-monday-morning-checklist","title":"The Monday Morning Checklist","text":"<p>You walk into work Monday morning and someone says: \"We need an AI solution for [business problem].\" How do you respond?</p> <p>This checklist operationalizes The Loop framework into a repeatable routine for any AI problem framing scenario. Use it as a forcing function to slow down and ask upstream questions before committing to implementation.</p>"},{"location":"chapters/06-application/#phase-1-outcome-clarity-5-minutes","title":"Phase 1: Outcome Clarity (5 minutes)","text":"<p>Before you discuss solutions, establish the destination.</p> <ul> <li>[ ] What is the measurable outcome we're pursuing?</li> <li>Not: \"improve customer experience\"</li> <li> <p>Yes: \"reduce support ticket volume by 25% while maintaining satisfaction &gt;4.0/5.0\"</p> </li> <li> <p>[ ] Who defines success for this outcome?</p> </li> <li>Is it the person asking for AI, or a different stakeholder?</li> <li> <p>Do they agree on what success looks like?</p> </li> <li> <p>[ ] What is the timeline and acceptable cost?</p> </li> <li>When do we need to show results?</li> <li> <p>What resources (people, compute, data) are available?</p> </li> <li> <p>[ ] What happens if we don't pursue this?</p> </li> <li>Is this urgent or aspirational?</li> <li>What's the opportunity cost of not solving it?</li> </ul> <p>Red flags: - Multiple conflicting goals (\"improve quality and reduce cost and ship faster\") - Vague outcomes that can't be measured - Solution already decided (\"we need a chatbot\" instead of \"we need better support\")</p> <p>If red flags appear: Stop and clarify outcome with stakeholders before proceeding.</p>"},{"location":"chapters/06-application/#phase-2-problem-deconstruction-15-minutes","title":"Phase 2: Problem Deconstruction (15 minutes)","text":"<p>Before you explore solutions, understand the problem structure.</p> <ul> <li>[ ] What is the atomic unit of this problem?</li> <li>What is the smallest independently meaningful component?</li> <li> <p>Will we measure success at that granularity or aggregate?</p> </li> <li> <p>[ ] What are we assuming about the problem?</p> </li> <li>What would have to be true for this framing to make sense?</li> <li> <p>Which assumptions can we validate quickly?</p> </li> <li> <p>[ ] Who are the stakeholders and users?</p> </li> <li>Who will use the AI output?</li> <li>Who will be affected by it?</li> <li> <p>Whose behavior needs to change for success?</p> </li> <li> <p>[ ] What constraints exist?</p> </li> <li>Data availability (do we have labels? volume? quality?)</li> <li>Latency requirements (real-time? batch?)</li> <li>Interpretability needs (black box acceptable? need explanations?)</li> <li> <p>Fairness/safety requirements (regulated domain? ethical risks?)</p> </li> <li> <p>[ ] What is the current workflow?</p> </li> <li>How is this problem solved today?</li> <li>What's broken about the current approach?</li> <li>What works well that we shouldn't break?</li> </ul> <p>Red flags: - No clear atomic unit (problem is too abstract) - Unvalidated assumptions presented as facts - Key stakeholders not identified or consulted - No understanding of current state (assuming AI is obviously better)</p> <p>If red flags appear: Do discovery work (interviews, observation, data exploration) before proceeding.</p>"},{"location":"chapters/06-application/#phase-3-alternatives-menu-20-minutes","title":"Phase 3: Alternatives Menu (20 minutes)","text":"<p>Before you commit to a solution, explore the possibility space.</p> <ul> <li>[ ] What are 5+ different ways to approach this?</li> <li>Include non-ML alternatives (rules, heuristics, process changes)</li> <li>Include different ML archetypes (classification, ranking, generation, etc.)</li> <li> <p>Include hybrid approaches (ML + rules, human-in-loop)</p> </li> <li> <p>[ ] What is the simplest viable approach?</p> </li> <li>What's the baseline (current state or naive solution)?</li> <li>What's the minimal ML solution that could work?</li> <li> <p>What would a rule-based system look like?</p> </li> <li> <p>[ ] What is the most sophisticated approach?</p> </li> <li>What would we do with unlimited resources?</li> <li> <p>What emerging research could apply here?</p> </li> <li> <p>[ ] Do we even need AI for this?</p> </li> <li>Can we solve it with better data, processes, or UX?</li> <li>Is the constraint actually technical?</li> </ul> <p>Red flags: - Only one solution considered - Defaulting to whatever is trendy (LLMs, deep learning) without justification - Skipping simple baselines - \"AI\" is the goal rather than the outcome</p> <p>If red flags appear: Force divergent thinking\u2014research different approaches, consult experts in other domains, look for analogous solved problems.</p>"},{"location":"chapters/06-application/#phase-4-trade-off-analysis-20-minutes","title":"Phase 4: Trade-off Analysis (20 minutes)","text":"<p>Before you choose, understand what you're giving up.</p> <p>For each viable alternative, assess:</p> <ul> <li>[ ] Development cost</li> <li>Data requirements (collection, labeling, cleaning)</li> <li>Engineering effort (person-months to build)</li> <li> <p>Compute cost (training and inference)</p> </li> <li> <p>[ ] Operational characteristics</p> </li> <li>Latency (real-time? seconds? minutes?)</li> <li>Accuracy/quality (good enough for outcome?)</li> <li> <p>Reliability (failure modes? edge cases?)</p> </li> <li> <p>[ ] Maintenance burden</p> </li> <li>How often must it be retrained?</li> <li>How much monitoring does it need?</li> <li> <p>What breaks when data distribution shifts?</p> </li> <li> <p>[ ] Organizational fit</p> </li> <li>Do we have expertise to build and maintain this?</li> <li>Does it align with existing infrastructure?</li> <li> <p>Will stakeholders trust and adopt it?</p> </li> <li> <p>[ ] Risk profile</p> </li> <li>What's the worst-case failure mode?</li> <li>Are there fairness, safety, or compliance risks?</li> <li>Can we mitigate or is risk inherent?</li> </ul> <p>Create a comparison matrix: Rows = alternatives, Columns = criteria, Cells = qualitative assessment</p> <p>Red flags: - Choosing solely on accuracy without considering cost/latency/maintenance - Ignoring organizational constraints (building what the team can't maintain) - Underestimating ongoing costs (training once vs. continuous retraining)</p> <p>If red flags appear: Build trade-off matrix explicitly and review with stakeholders\u2014make decisions transparent.</p>"},{"location":"chapters/06-application/#phase-5-signal-design-15-minutes","title":"Phase 5: Signal Design (15 minutes)","text":"<p>Before you build, define how you'll know if you're on the right track.</p> <ul> <li>[ ] What are the success metrics?</li> <li>Model metrics (accuracy, precision, recall, etc.)</li> <li>Business metrics (conversion, retention, cost reduction)</li> <li> <p>User metrics (satisfaction, adoption, engagement)</p> </li> <li> <p>[ ] What are the leading indicators?</p> </li> <li>What can we measure early that predicts long-term success?</li> <li> <p>What would tell us we're headed in the wrong direction?</p> </li> <li> <p>[ ] What are the kill signals?</p> </li> <li>What threshold would make us stop or pivot?</li> <li> <p>What failure mode would invalidate the approach?</p> </li> <li> <p>[ ] How will we instrument this?</p> </li> <li>What data do we need to collect to measure these signals?</li> <li> <p>Can we measure them in a pilot before full deployment?</p> </li> <li> <p>[ ] What is the evaluation timeline?</p> </li> <li>When will we check signals? (weekly? monthly?)</li> <li>How long until we expect meaningful signal? (avoid premature judgment)</li> </ul> <p>Red flags: - Only tracking model metrics (ignoring business outcomes) - No plan for measuring impact (assuming deployment = success) - No defined failure criteria (never considering stopping)</p> <p>If red flags appear: Design instrumentation and evaluation plan before building\u2014treat measurement as part of the product.</p>"},{"location":"chapters/06-application/#phase-6-commit-or-defer-5-minutes","title":"Phase 6: Commit or Defer (5 minutes)","text":"<p>Final check before proceeding.</p> <ul> <li>[ ] Can we clearly answer all previous checklist items?</li> <li>If not, what gaps remain?</li> <li> <p>Can we fill gaps quickly or do we need more discovery?</p> </li> <li> <p>[ ] Do stakeholders agree on the framing?</p> </li> <li>Have we aligned on outcomes, constraints, and approach?</li> <li> <p>Are there unresolved conflicts?</p> </li> <li> <p>[ ] Do we have the resources to execute?</p> </li> <li>Data, talent, compute, time</li> <li> <p>Or do we need to acquire them first?</p> </li> <li> <p>[ ] Is this the right priority?</p> </li> <li>Given other projects, is this the best use of resources?</li> <li>What are we not doing if we pursue this?</li> </ul> <p>Three possible outcomes:</p> <ol> <li>Commit: Framing is clear, trade-offs understood, resources available \u2192 build it</li> <li>Defer: Good framing but not the right time/priority \u2192 revisit later</li> <li>Reframe: Gaps in understanding, misalignment, or better alternatives identified \u2192 return to earlier phases</li> </ol> <p>Avoid: Proceeding when framing is unclear or stakeholders are misaligned. The cost of building the wrong thing exceeds the cost of more discovery.</p>"},{"location":"chapters/06-application/#checklist-in-practice-example-walkthrough","title":"Checklist in Practice: Example Walkthrough","text":"<p>Scenario: \"We need a recommendation system for our learning platform to personalize content.\"</p> <p>Phase 1: Outcome Clarity - Measurable outcome: Increase course completion rate from 35% to 50% in 6 months - Success owner: VP of Product (measured via analytics dashboard) - Timeline: Pilot in 3 months, full rollout in 6 months - Cost: 2 engineers for 3 months, $10k compute budget - Opportunity cost: Delaying mobile app improvements</p> <p>Phase 2: Problem Deconstruction - Atomic unit: Individual learner session (not learner or course) - Assumptions: (1) Personalization improves completion, (2) We have signal for good recommendations, (3) Learners will follow recommendations - Stakeholders: Learners (need relevant content), instructors (want completions), ops team (maintains system) - Constraints: Must work with cold-start (new users), &lt;500ms latency, explainable (learners want to know \"why this?\") - Current workflow: Manual course browsing, search, instructor recommendations</p> <p>Phase 3: Alternatives Menu 1. Collaborative filtering (user-user similarity) 2. Content-based filtering (course similarity) 3. Sequence modeling (what comes after course X?) 4. Popularity + basic rules (trending + match user goals) 5. Instructor-curated paths (no ML, just better UX)</p> <p>Phase 4: Trade-off Analysis</p> Alternative Dev Cost Latency Accuracy Maintenance Interpretability Collaborative Medium Low Medium Medium Low Content-based Low Low Medium Low High Sequence model High Medium High High Low Popularity Low Low Low Low High Curated paths Low N/A Medium Low High <p>Decision: Start with content-based filtering (low cost, explainable, adequate accuracy) with plan to add collaborative signals if successful.</p> <p>Phase 5: Signal Design - Success metrics: Completion rate (business), CTR on recommendations (proxy), user satisfaction survey - Leading indicators: Recommendation CTR (weekly), time-to-first-completion (monthly) - Kill signals: CTR &lt;5%, completion rate unchanged after 3 months, satisfaction &lt;3.5/5.0 - Instrumentation: Log all recommendation impressions and clicks, track completion by source (organic vs. recommended) - Timeline: Weekly metric reviews, pivot decision at 3 months</p> <p>Phase 6: Commit - All questions answered clearly - Stakeholders aligned (Product, Engineering, Instructors) - Resources available (engineers allocated, data exists) - High priority (completion rate is OKR)</p> <p>Outcome: Commit to content-based recommendation pilot with defined success criteria and evaluation timeline.</p>"},{"location":"chapters/06-application/#reflection-questions","title":"Reflection Questions","text":"<p>These questions guide your synthesis of the chapter concepts:</p> <ol> <li> <p>Case Study Analysis: Choose one case study from this chapter. What was the earliest signal that the original framing was wrong? Could that signal have been detected before deployment?</p> </li> <li> <p>Pattern Recognition: Think of an AI project you've worked on or observed. Which pivot pattern (if any) does it match? If it succeeded, did it avoid the pattern's typical failure mode? If it failed, would the alternative framing have helped?</p> </li> <li> <p>Monday Morning Application: Take a current or hypothetical AI problem in your domain. Walk through the Monday Morning Checklist. Where do you encounter gaps in your understanding? What questions can't you answer yet?</p> </li> <li> <p>Trade-off Preferences: Across the case studies, different teams made different trade-off decisions (accuracy vs. latency, automation vs. control, engagement vs. satisfaction). What trade-off patterns do you notice in your organization? Are they explicit or implicit?</p> </li> <li> <p>Signal Blindness: In the fraud detection case, the team tracked false positive rate but missed false positive impact. What metrics do you track that might suffer from similar abstraction? What signals are you not measuring?</p> </li> <li> <p>Pivot Resistance: The content generation case showed reluctance to abandon full automation even when augmentation worked better. What psychological or organizational factors make pivoting difficult? How would you structure projects to make pivots easier?</p> </li> <li> <p>Pattern Bridges Transfer: Take Pattern 2 (Classification \u2192 Scoring) and apply it to a domain not covered in the case studies. What problem might benefit from this pivot? What would the continuous scoring enable that binary classification prevents?</p> </li> <li> <p>Checklist Customization: The Monday Morning Checklist provides a generic framework. What domain-specific questions would you add for your industry (healthcare, finance, education, etc.)? What constraints or stakeholders are always relevant in your context?</p> </li> </ol>"},{"location":"chapters/06-application/#final-portfolio-complete-problem-framing-portfolio","title":"Final Portfolio: Complete Problem Framing Portfolio","text":"<p>This capstone assignment synthesizes all course concepts into a comprehensive problem framing portfolio demonstrating your ability to work systematically from ambiguity to actionable implementation plans.</p>"},{"location":"chapters/06-application/#portfolio-requirements","title":"Portfolio Requirements","text":"<p>Your portfolio must include:</p>"},{"location":"chapters/06-application/#1-problem-selection-and-context-500-words","title":"1. Problem Selection and Context (500 words)","text":"<p>Select a real or realistic AI opportunity in a domain you understand well. Provide:</p> <ul> <li>Business context: What organization, what challenge, what constraints exist</li> <li>Current state: How is this problem addressed today (if at all)</li> <li>Stakeholders: Who cares about solving this and why</li> <li>Success criteria: What would \"solved\" look like</li> </ul> <p>Quality indicators: - Sufficient complexity (not trivial, not unsolvable) - Clear business value (not AI for AI's sake) - Real constraints (not unconstrained thought experiment)</p>"},{"location":"chapters/06-application/#2-complete-loop-analysis-1500-words","title":"2. Complete Loop Analysis (1500 words)","text":"<p>Apply all five Loop phases systematically:</p> <p>Outcome Definition: - Specific, measurable outcome with timeline - Clarification of what success means quantitatively - Identification of whose definition of success matters</p> <p>Problem Deconstruction: - Atomic unit identification with justification - Explicit statement of assumptions with validation plan - Stakeholder mapping and constraint catalog - Current workflow documentation</p> <p>Alternatives Menu: - Minimum 5 distinct alternatives spanning different archetypes - Include at least one non-ML alternative - Brief description of each approach (2-3 sentences) - Explanation of why each is viable</p> <p>Trade-off Analysis: - Comparison matrix with minimum 5 criteria (cost, latency, accuracy, maintenance, organizational fit) - Qualitative or quantitative assessment for each alternative - Clear recommendation with justification - Acknowledgment of what you're giving up with your choice</p> <p>Signal Framework: - 3+ success metrics across model, business, and user dimensions - Leading indicators that predict outcome achievement - Kill signals with explicit thresholds - Instrumentation plan and evaluation timeline</p> <p>Quality indicators: - Each phase builds logically on previous phases - Decisions are evidence-based and transparent - Alternatives are genuinely different (not minor variations) - Trade-offs are honest (acknowledging weaknesses of chosen approach) - Signals are measurable and actionable</p>"},{"location":"chapters/06-application/#3-anticipated-failure-modes-and-pivot-plan-500-words","title":"3. Anticipated Failure Modes and Pivot Plan (500 words)","text":"<p>Demonstrate proactive thinking about what could go wrong:</p> <ul> <li>Most likely failure mode: What's the highest-probability way this framing could be wrong?</li> <li>Pivot pattern match: Which pattern from this chapter (if any) matches your problem? What does that predict about failure modes?</li> <li>Alternative framing: If your initial framing fails, what alternative framing would you pivot to?</li> <li>Signal timeline: When would you expect to see signals of success or failure? What's your decision point for persist/pivot/stop?</li> </ul> <p>Quality indicators: - Realistic failure modes (not strawman scenarios) - Clear connection to course patterns - Thoughtful alternative framing (not just \"try harder\") - Defined decision criteria (avoiding indefinite persistence)</p>"},{"location":"chapters/06-application/#4-stakeholder-communication-plan-300-words","title":"4. Stakeholder Communication Plan (300 words)","text":"<p>How would you present this framing to secure buy-in?</p> <ul> <li>Executive summary: 3-4 sentences capturing the opportunity, approach, and expected outcome</li> <li>Key trade-offs: What are you optimizing for and what are you accepting?</li> <li>Risk mitigation: What's the biggest risk and how are you managing it?</li> <li>Decision points: When will you update stakeholders on progress and what criteria determine success?</li> </ul> <p>Quality indicators: - Accessible to non-technical audience - Honest about limitations and risks - Clear ask (resources, timeline, decision authority) - Realistic expectations</p>"},{"location":"chapters/06-application/#submission-format","title":"Submission Format","text":"<ul> <li>Length: 2800-3300 words total (excluding references)</li> <li>Structure: Use the four sections above as headings</li> <li>Visuals: Include at least one diagram (trade-off matrix, workflow diagram, or alternatives comparison)</li> <li>Citations: Reference course concepts explicitly (e.g., \"Using The Loop framework from Chapter 3...\")</li> </ul>"},{"location":"chapters/06-application/#evaluation-rubric","title":"Evaluation Rubric","text":"<p>Your portfolio will be evaluated on:</p>"},{"location":"chapters/06-application/#systematic-application-40","title":"Systematic Application (40%)","text":"<ul> <li>Exemplary (36-40 points): Every Loop phase executed thoroughly with clear reasoning; decisions build logically; framework applied correctly throughout</li> <li>Proficient (32-35 points): Loop phases mostly complete with minor gaps; generally sound reasoning; correct framework application</li> <li>Developing (28-31 points): Some Loop phases incomplete or superficial; occasional logical gaps; framework applied inconsistently</li> <li>Beginning (0-27 points): Missing critical Loop phases; unclear reasoning; framework misapplied or ignored</li> </ul>"},{"location":"chapters/06-application/#depth-of-analysis-30","title":"Depth of Analysis (30%)","text":"<ul> <li>Exemplary (27-30 points): Explores second-order effects, hidden assumptions, edge cases; considers multiple perspectives; demonstrates sophisticated understanding</li> <li>Proficient (24-26 points): Addresses main considerations; identifies key assumptions; shows solid understanding</li> <li>Developing (21-23 points): Surface-level analysis; misses important considerations; limited perspective-taking</li> <li>Beginning (0-20 points): Minimal analysis; obvious gaps; single-perspective thinking</li> </ul>"},{"location":"chapters/06-application/#evidence-based-reasoning-20","title":"Evidence-Based Reasoning (20%)","text":"<ul> <li>Exemplary (18-20 points): Claims grounded in data, examples, or case study patterns; acknowledges uncertainty appropriately; transparent about assumptions</li> <li>Proficient (16-17 points): Most claims supported; reasonable uncertainty handling; generally transparent</li> <li>Developing (14-15 points): Some unsupported claims; treats assumptions as facts; limited transparency</li> <li>Beginning (0-13 points): Mostly unsupported assertions; ignores uncertainty; opaque reasoning</li> </ul>"},{"location":"chapters/06-application/#communication-clarity-10","title":"Communication Clarity (10%)","text":"<ul> <li>Exemplary (9-10 points): Clear structure; compelling narrative; appropriate visuals; accessible to diverse audiences</li> <li>Proficient (8 points): Organized; mostly clear; adequate visuals; generally accessible</li> <li>Developing (7 points): Somewhat disorganized; clarity issues; minimal visuals; assumes too much context</li> <li>Beginning (0-6 points): Disorganized; unclear writing; no visuals; inaccessible</li> </ul> <p>Total: 100 points</p> <p>Passing threshold: 70 points (demonstrates competence in problem framing for professional practice)</p>"},{"location":"chapters/06-application/#peer-review-process-optional-but-recommended","title":"Peer Review Process (Optional but Recommended)","text":"<p>While not required for portfolio completion, peer feedback accelerates learning:</p>"},{"location":"chapters/06-application/#how-peer-review-works","title":"How Peer Review Works","text":"<ol> <li>Pairing: After submitting your portfolio, you'll be matched with 2-3 peers</li> <li>Review Period: 1 week to read peer portfolios and provide written feedback</li> <li>Feedback Format: Structured questions guide your review (provided separately)</li> <li>Discussion: Optional live discussion session to deepen understanding</li> </ol>"},{"location":"chapters/06-application/#what-to-focus-on","title":"What to Focus On","text":"<p>When reviewing peer work:</p> <ul> <li>Completeness: Are all Loop phases addressed?</li> <li>Alternatives: Are alternatives genuinely different or minor variations?</li> <li>Trade-offs: Are trade-offs realistic or overly optimistic?</li> <li>Signals: Are signals measurable and actionable or vague aspirations?</li> <li>Failure modes: Has the person anticipated realistic failure modes?</li> </ul>"},{"location":"chapters/06-application/#how-to-give-useful-feedback","title":"How to Give Useful Feedback","text":"<ul> <li>Be specific: \"The trade-off analysis ignores maintenance burden\" &gt; \"Trade-offs need work\"</li> <li>Be constructive: Suggest alternatives, don't just criticize</li> <li>Be curious: Ask questions to understand reasoning rather than assuming it's wrong</li> <li>Be humble: Your framing might have gaps too\u2014peer review is collaborative learning</li> </ul>"},{"location":"chapters/06-application/#benefits-of-peer-review","title":"Benefits of Peer Review","text":"<ul> <li>Pattern recognition: See how others frame similar problems differently</li> <li>Blind spot detection: Peers catch assumptions you didn't notice</li> <li>Alternative approaches: Learn solution archetypes outside your expertise</li> <li>Critical thinking: Evaluating others sharpens your self-evaluation</li> </ul>"},{"location":"chapters/06-application/#course-conclusion-from-frameworks-to-practice","title":"Course Conclusion: From Frameworks to Practice","text":"<p>You've completed a graduate-level course in AI problem framing\u2014a skill that will compound in value throughout your career.</p>"},{"location":"chapters/06-application/#what-youve-learned","title":"What You've Learned","text":"<p>Chapter 1 taught you to recognize the solution-first trap, embrace ambiguity, and activate System 2 thinking at critical moments.</p> <p>Chapter 2 built your mental model of the AI solution space\u201413 archetypes spanning classic ML and GenAI, with trade-offs and application contexts.</p> <p>Chapter 3 gave you The Loop, a systematic framework for moving from vague objectives to actionable plans through outcome clarity, deconstruction, alternatives, trade-offs, and signals.</p> <p>Chapter 4 developed your ability to read signals from live systems\u2014distinguishing meaningful patterns from noise, identifying leading indicators, and designing instrumentation.</p> <p>Chapter 5 taught you when and how to pivot based on evidence, avoiding both premature abandonment and sunk-cost persistence.</p> <p>Chapter 6 synthesized these concepts through case studies, pattern recognition, and the Monday Morning Checklist\u2014moving from theory to professional practice.</p>"},{"location":"chapters/06-application/#how-this-changes-your-work","title":"How This Changes Your Work","text":"<p>Before this course, you might have approached AI problems reactively: someone asks for a solution, you build it, you deploy it, you hope it works.</p> <p>After this course, you approach AI problems systematically:</p> <ul> <li>You ask upstream questions before committing to solutions</li> <li>You explore diverse alternatives rather than defaulting to the obvious approach</li> <li>You make trade-offs explicit rather than discovering them post-deployment</li> <li>You design signals to detect misalignment early</li> <li>You pivot based on evidence rather than intuition or stubbornness</li> </ul> <p>This systematic approach has three benefits:</p> <ol> <li>Fewer failures: You catch misaligned framings before investing months of engineering</li> <li>Faster pivots: You recognize failure modes early and adjust course deliberately</li> <li>Better stakeholder relationships: You communicate trade-offs transparently and manage expectations realistically</li> </ol>"},{"location":"chapters/06-application/#what-to-learn-next","title":"What to Learn Next","text":"<p>This course focused on problem framing\u2014the upstream decisions that determine what to build. To translate framings into deployed systems, consider deepening expertise in:</p> <p>For Classic ML Practitioners: - MLOps and deployment: Moving models from notebooks to production reliably - Causal inference: Moving beyond correlation to understand intervention effects - Fairness and interpretability: Ensuring models work equitably and transparently - Data engineering: Building pipelines that provide high-quality training data at scale</p> <p>For GenAI Practitioners: - Prompt engineering and evaluation: Systematic approaches to optimizing LLM behavior - RAG architectures: Building retrieval-augmented systems that ground generation - Fine-tuning and alignment: Adapting models to domain-specific needs safely - LLM evaluation frameworks: Measuring quality beyond toy examples</p> <p>For Everyone: - Organizational change: Implementing AI in real organizations with real politics - Product management: Translating technical capabilities into user value - Ethics and policy: Navigating the societal implications of AI systems - Domain expertise: Deepening knowledge in your application area (healthcare, finance, education, etc.)</p> <p>Recommended Next Courses: - Production Machine Learning Systems (focuses on deployment and operations) - Causal Inference for Data Science (teaches intervention analysis) - Designing Human-AI Collaboration (explores organizational and UX challenges)</p>"},{"location":"chapters/06-application/#continuing-the-practice","title":"Continuing the Practice","text":"<p>Problem framing is a skill that improves with deliberate practice. To continue developing:</p> <ol> <li>Use the Monday Morning Checklist: Apply it to every new AI problem you encounter</li> <li>Study failures: When AI projects fail (yours or others'), diagnose the framing gaps</li> <li>Build a pattern library: Document pivot patterns you encounter in your domain</li> <li>Teach others: Explaining The Loop to colleagues deepens your understanding</li> <li>Stay current: AI capabilities evolve\u2014new solution archetypes emerge that expand the alternatives menu</li> </ol>"},{"location":"chapters/06-application/#a-final-note-on-humility","title":"A Final Note on Humility","text":"<p>The Loop framework and Monday Morning Checklist are tools, not magic. They help you think systematically, but they don't guarantee success. You will still frame problems incorrectly. You will still miss signals. You will still build things that don't work.</p> <p>The difference is that you'll recognize these failures earlier, diagnose them more accurately, and pivot more deliberately. You'll waste fewer resources on doomed approaches and more quickly find framings that deliver value.</p> <p>Problem framing is not about perfection. It's about developing the habit of asking \"Are we solving the right problem?\" before asking \"How do we solve it?\" That habit\u2014practiced consistently across projects and years\u2014is what separates effective AI practitioners from those who build impressive solutions to irrelevant problems.</p> <p>You now have the frameworks, patterns, and checklists to practice that habit. The rest is up to you.</p> <p>Welcome to the upstream work of AI. The work that happens before the code, before the models, before the deployment. The work that determines whether everything that follows creates value or waste.</p> <p>Go build something worth building.</p>"},{"location":"chapters/06-application/#additional-resources","title":"Additional Resources","text":""},{"location":"chapters/06-application/#case-study-sources","title":"Case Study Sources","text":"<p>The case studies in this chapter are synthesized from real projects, with details anonymized and composited to protect confidentiality. For deeper investigation of similar patterns:</p> <ul> <li>Churn prediction: \"Why Customer Analytics Fails: And How to Fix It\" (Harvard Business Review)</li> <li>Fraud detection: \"Machine Learning for Fraud Detection: Lessons from the Field\" (Stripe Engineering Blog)</li> <li>Recommendations: \"Exploration vs. Exploitation in Recommender Systems\" (RecSys Conference Papers)</li> <li>Chatbots: \"The ROI of Conversational AI\" (Gartner Research)</li> <li>Summarization: \"What Makes a Good Summary? Perspectives from Legal Tech\" (LegalTech News)</li> </ul>"},{"location":"chapters/06-application/#pattern-recognition","title":"Pattern Recognition","text":"<ul> <li>Christensen, C. (1997). The Innovator's Dilemma - discusses pattern recognition across industry pivots</li> <li>Gabriel, R. (2019). Patterns of Software - explores patterns in software architecture that parallel problem framing</li> </ul>"},{"location":"chapters/06-application/#frameworks-and-toolkits","title":"Frameworks and Toolkits","text":"<ul> <li>Project retrospective templates: [add link to course resources]</li> <li>Trade-off analysis worksheets: [add link to course resources]</li> <li>Signal design canvas: [add link to course resources]</li> </ul> <p>Congratulations on completing AI Problem Framing for AI Practitioners. You're now equipped to make better upstream decisions about AI projects. Go practice.</p>"},{"location":"chapters/06-application/quiz/","title":"Chapter 6: Application - Quiz","text":"<p>Course: AI Problem Framing for AI Practitioners Chapter Type: Advanced (Synthesis) Focus: Case study analysis, pattern recognition across pivots, and practical application of the Monday Morning Checklist</p>"},{"location":"chapters/06-application/quiz/#instructions","title":"Instructions","text":"<p>This quiz contains 10 questions designed to assess your understanding of the Application chapter. Questions range from basic recall to complex synthesis tasks. For each question, select the best answer or provide your response as indicated.</p> <p>Time Estimate: 30-45 minutes</p>"},{"location":"chapters/06-application/quiz/#questions","title":"Questions","text":""},{"location":"chapters/06-application/quiz/#1-remember-classic-ml-pivot-recognition","title":"1. Remember | Classic ML Pivot Recognition","text":"<p>A team has spent 3 months building a rules-based invoice processing system that achieved 78% extraction accuracy. They realize accuracy is insufficient for their use case. Which of the following is the most direct Classic ML Pivot opportunity?</p> <p>A) Switch to a completely different problem domain B) Implement a fine-tuned language model for invoice extraction C) Add more manual rules to improve the heuristic system D) Hire more people to manually process invoices</p> <p>Correct Answer: B</p> <p>Explanation: A Classic ML Pivot involves moving from simpler approaches (rules-based) to machine learning when the simpler approach cannot achieve sufficient performance. Fine-tuning a language model represents a clear ML Pivot from rule-based extraction.</p>"},{"location":"chapters/06-application/quiz/#2-remember-genai-pivot-identification","title":"2. Remember | GenAI Pivot Identification","text":"<p>Which scenario best exemplifies a GenAI Pivot decision?</p> <p>A) A team improves their existing classification model's hyperparameters B) A team switches from traditional NLP pipelines to using a large language model's generation capabilities for text summarization C) A team adds more training data to their existing regression model D) A team changes their feature engineering approach</p> <p>Correct Answer: B</p> <p>Explanation: A GenAI Pivot represents a shift toward generative AI approaches when traditional models or simpler methods are insufficient. Moving to LLM-based generation for summarization is a classic GenAI Pivot.</p>"},{"location":"chapters/06-application/quiz/#3-understand-pattern-bridge-application","title":"3. Understand | Pattern Bridge Application","text":"<p>A healthcare organization successfully deployed a machine learning system to predict patient readmission risk, achieving high accuracy. They now want to apply the same pattern to predict no-show rates for appointments. What is this an example of?</p> <p>A) Problem Pivot B) Pattern Bridge C) Scope Pivot D) Tool Pivot</p> <p>Correct Answer: B</p> <p>Explanation: Pattern Bridging is the structured transfer of a successful problem-solving approach from one context (readmission prediction) to a similar but distinct problem (no-show prediction). It recognizes that the underlying pattern\u2014predicting binary user behavior from historical data\u2014transfers across domains.</p>"},{"location":"chapters/06-application/quiz/#4-understand-problem-portfolio-structure","title":"4. Understand | Problem Portfolio Structure","text":"<p>A fintech company has identified 12 potential AI improvement opportunities across their platform. Before beginning development, they should use a Problem Portfolio to accomplish which of the following?</p> <p>A) Immediately start implementation on all 12 problems in parallel B) Rank and prioritize problems by business impact and solution feasibility C) Assign each problem to a different engineer D) Guarantee that all solutions will use the same ML architecture</p> <p>Correct Answer: B</p> <p>Explanation: A Problem Portfolio provides a structured inventory of related problems prioritized by business impact and solution feasibility. This enables strategic focus rather than chaotic parallel development.</p>"},{"location":"chapters/06-application/quiz/#5-apply-monday-morning-checklist-in-practice","title":"5. Apply | Monday Morning Checklist in Practice","text":"<p>It's Monday morning, and your team is resuming work on a customer churn prediction model you started 2 weeks ago. Which question should NOT be part of your Monday Morning Checklist evaluation?</p> <p>A) Has the business context changed (new competitive threat, market shift, organizational priorities)? B) Are the data quality assumptions still valid? C) What is the exact model architecture we used for our initial prototype? D) Is the original problem we're solving still the most critical priority?</p> <p>Correct Answer: C</p> <p>Explanation: The Monday Morning Checklist is designed to validate whether your original framing remains sound. It focuses on business context, data validity, and strategic alignment\u2014not implementation details like model architecture. Knowing the exact architecture doesn't help assess if you're solving the right problem.</p>"},{"location":"chapters/06-application/quiz/#6-apply-synthesis-of-pivots-and-signals","title":"6. Apply | Synthesis of Pivots and Signals","text":"<p>A team built a GenAI-powered customer service system using prompt engineering. After 2 weeks in production, they observe: high user satisfaction (90%) but increasing latency (5s \u2192 12s response times) and rising API costs ($2K \u2192 $12K monthly). What decision does your Monday Morning Checklist suggest?</p> <p>A) Continue with GenAI Pivot\u2014the signals don't matter B) Persist with current approach; user satisfaction is the only metric that matters C) Evaluate whether the current approach is sustainable despite positive sentiment signals D) Immediately switch to a Classic ML approach without analysis</p> <p>Correct Answer: C</p> <p>Explanation: The Monday Morning Checklist synthesizes multiple signals. While user satisfaction is a success signal, increasing latency and costs are leading indicators of operational strain. A responsible evaluation must weigh whether the current solution is sustainable given its cost and performance trajectory, even with positive user feedback. This might lead to an Approach Pivot toward a hybrid solution.</p>"},{"location":"chapters/06-application/quiz/#7-analyze-pattern-recognition-across-problem-domains","title":"7. Analyze | Pattern Recognition Across Problem Domains","text":"<p>You're reviewing three completed projects: (1) a recommendation system using collaborative filtering, (2) a fraud detection system using anomaly detection, and (3) a demand forecasting system using time series prediction. Which Pattern Bridge opportunity exists?</p> <p>A) All three should use identical algorithms B) The anomaly detection pattern from fraud detection could apply to demand forecasting for identifying unusual market conditions C) Recommendation systems and forecasting have no shared patterns D) All three problems are fundamentally different and cannot share approaches</p> <p>Correct Answer: B</p> <p>Explanation: Pattern Bridging involves recognizing structural similarities across surface-level differences. Both fraud detection (identifying anomalies) and demand forecasting (modeling normal behavior) operate on the pattern of distinguishing expected from unexpected observations. The anomaly detection approach used in fraud could transfer to identifying unusual demand patterns caused by market disruptions.</p>"},{"location":"chapters/06-application/quiz/#8-analyze-evaluating-pivot-necessity-through-systematic-analysis","title":"8. Analyze | Evaluating Pivot Necessity Through Systematic Analysis","text":"<p>A logistics company tried a rules-based route optimization system. After 6 months, they achieved 85% of theoretical optimal routes. Their stakeholders ask: \"Should we pivot to an ML-based approach?\" What framework should guide this analysis?</p> <p>A) Immediately pivot because 85% isn't perfect B) Apply the Loop framework to diagnose: Does 85% solve the business problem? What are the constraints? What would ML realistically achieve? What are the costs? C) Always use ML for optimization problems D) Never pivot from rules-based systems</p> <p>Correct Answer: B</p> <p>Explanation: The decision to pivot is not binary \"good vs. perfect\" but strategic. The Loop framework guides analysis: Is the current solution adequate for business outcomes? Is the problem likely solvable better with ML (not just theoretically)? What are realistic improvements vs. implementation costs? What would the operational impact be?</p>"},{"location":"chapters/06-application/quiz/#9-evaluate-trade-off-analysis-in-application-decisions","title":"9. Evaluate | Trade-off Analysis in Application Decisions","text":"<p>Your team must choose between two approaches for a new recommendation system:</p> <p>Approach A (Classic ML Pivot): Train a collaborative filtering model on historical user-item interactions. Estimated accuracy: 88%, cost: $50K development, latency: 200ms, interpretability: Low</p> <p>Approach B (GenAI Pivot): Use an LLM with prompt engineering to generate personalized recommendations. Estimated quality: 92% (subjective), cost: $15K development + $5K/month API fees, latency: 2-5s, interpretability: High</p> <p>Which is not a legitimate evaluation criterion for deciding between these approaches?</p> <p>A) Total cost of ownership over 12 months B) Whether latency impact will degrade user experience C) Whether A or B has the cooler sounding name D) Whether interpretability is critical for stakeholder trust</p> <p>Correct Answer: C</p> <p>Explanation: Trade-off analysis must consider relevant business and technical dimensions: cost, performance, latency, interpretability, organizational fit, etc. The \"coolness\" of an approach name has zero bearing on whether it solves the problem effectively and sustainably.</p>"},{"location":"chapters/06-application/quiz/#10-create-comprehensive-problem-portfolio-decision","title":"10. Create | Comprehensive Problem Portfolio Decision","text":"<p>A healthcare organization has identified three AI opportunities in their Problem Portfolio:</p> <ol> <li>Readmission Prediction: High business impact, moderate solution complexity, known domain patterns</li> <li>Drug Interaction Detection: Medium business impact, high solution complexity (regulatory constraints), limited existing patterns</li> <li>Patient Communication Optimization: High business impact, low solution complexity, strong Pattern Bridge from marketing domain</li> </ol> <p>Using the concepts of Problem Portfolio, Pattern Bridge, and the Loop framework, rank these by implementation priority and justify your ranking in 2-3 sentences.</p> <p>Evaluation Criteria: - Does the ranking reflect sound strategic thinking? - Are Pattern Bridge opportunities recognized and valued? - Is the justification grounded in course concepts (Portfolio prioritization, Pattern Bridge value, Loop framework considerations)? - Does the response acknowledge trade-offs or constraints?</p> <p>Model Answer:</p> <p>The ranking should be: (1) Patient Communication Optimization, (2) Readmission Prediction, (3) Drug Interaction Detection.</p> <p>Justification: Patient Communication Optimization offers the best combination of high impact and low complexity, with immediate Pattern Bridge opportunity from marketing (proven transferable knowledge), enabling quick wins that build organizational AI capability. Readmission Prediction follows as a medium-risk, high-impact core healthcare application with known patterns. Drug Interaction Detection, while important, should be deferred until the team has built capability and tribal knowledge through earlier successes\u2014the high regulatory and technical complexity requires deeper domain expertise that later Pattern Bridges could provide from completed projects.</p>"},{"location":"chapters/06-application/quiz/#blooms-distribution-summary","title":"Bloom's Distribution Summary","text":"Level Count Questions Remember 2 Q1, Q2 Understand 2 Q3, Q4 Apply 2 Q5, Q6 Analyze 2 Q7, Q8 Evaluate 1 Q9 Create 1 Q10 Total 10"},{"location":"chapters/06-application/quiz/#key-concepts-assessed","title":"Key Concepts Assessed","text":"<ul> <li>ML Pivot &amp; GenAI Pivot: Recognition and application of when and why to transition from simpler to more complex approaches (Q1, Q2, Q6)</li> <li>Pattern Bridge: Understanding how successful patterns transfer across problem contexts (Q3, Q7)</li> <li>Problem Portfolio: Strategic prioritization of multiple AI opportunities (Q4, Q10)</li> <li>Monday Morning Checklist: Systematic re-validation of problem frames and decisions (Q5, Q6)</li> <li>Signal Recognition &amp; Synthesis: Integrating multiple signals to make persist/pivot decisions (Q6, Q8)</li> <li>Trade-off Analysis: Evaluating competing priorities across multiple dimensions (Q9)</li> <li>Systematic Decision-Making: Applying frameworks over intuition (Q8, Q10)</li> </ul>"},{"location":"chapters/06-application/quiz/#answer-key-scoring","title":"Answer Key &amp; Scoring","text":"<p>Multiple Choice Questions (Q1-Q9): 1 point each = 9 points Essay Question (Q10): 1 point = 1 point Total Possible: 10 points</p> <p>Passing Score: 7/10 (70%) Mastery: 9/10 (90%)</p>"},{"location":"chapters/06-application/quiz/#scoring-rubric-for-q10-create","title":"Scoring Rubric for Q10 (Create)","text":"<p>1 point: Ranking is provided with brief justification that references at least one course concept (Portfolio, Pattern Bridge, or Loop framework).</p> <p>0.5 points: Ranking shows strategic thinking but justification lacks clear connection to course concepts.</p> <p>0 points: Ranking is provided without justification, or justification is inaccurate.</p>"},{"location":"chapters/06-application/quiz/#discussion-prompts-optional","title":"Discussion Prompts (Optional)","text":"<p>For deeper learning, consider these reflection questions:</p> <ol> <li> <p>Pattern Bridge Analysis: In your organization or field, what successful problem-solving patterns could be \"bridged\" to new contexts? What would make those bridges successful or risky?</p> </li> <li> <p>Monday Morning Checklist in Your Work: Describe a project where the Monday Morning Checklist would have prevented or caught a strategic misalignment. What changed between the initial framing and when the issue was discovered?</p> </li> <li> <p>Problem Portfolio Trade-offs: If you had to choose between solving a high-impact, high-complexity problem immediately versus deferring it to build capability through lower-complexity problems first, how would you justify that choice to a stakeholder?</p> </li> <li> <p>Pivot Recognition: Think of a project where a pivot was needed but didn't happen (or vice versa). What signals should have been stronger? How would applying The Loop framework have changed the outcome?</p> </li> </ol>"},{"location":"learning-graph/","title":"Learning Graph","text":"<p>This course is built on a structured learning graph of 200 concepts organized into a directed acyclic graph (DAG). The learning graph ensures proper prerequisite sequencing and enables adaptive learning paths.</p>"},{"location":"learning-graph/#concept-categories","title":"Concept Categories","text":"Category Description Count Foundation (FOUND) Core concepts with no prerequisites ~25 Basic (BASIC) Build on foundations ~45 Intermediate (INTER) Build on basics ~60 Advanced (ADV) Build on intermediate ~45 Application (APP) Synthesize multiple concepts ~25"},{"location":"learning-graph/#viewing-the-graph","title":"Viewing the Graph","text":"<p>The learning graph data is stored in JSON format:</p> <ul> <li>learning-graph.json - Full concept graph with dependencies</li> </ul>"},{"location":"learning-graph/#using-the-learning-graph","title":"Using the Learning Graph","text":"<p>The learning graph supports:</p> <ol> <li>Prerequisite tracking - Each concept lists its dependencies</li> <li>Chapter alignment - Concepts are mapped to chapters</li> <li>Bloom's levels - Each concept indicates its cognitive level</li> <li>Adaptive paths - Skip concepts you already know, focus on gaps</li> </ol>"},{"location":"learning-graph/#sample-concept-entry","title":"Sample Concept Entry","text":"<pre><code>{\n  \"id\": 42,\n  \"name\": \"The Loop Framework\",\n  \"category\": \"INTER\",\n  \"depends_on\": [15, 23, 31],\n  \"chapter\": 3,\n  \"bloom_level\": \"apply\",\n  \"description\": \"5-step systematic process for AI problem framing\"\n}\n</code></pre>"},{"location":"sims/ai-alternatives-decision-tree/","title":"AI Alternatives Decision Tree","text":"<p>Navigate problem characteristics systematically to discover whether AI, traditional software, or hybrid approaches best fit your needs.</p>"},{"location":"sims/ai-alternatives-decision-tree/#interactive-simulation","title":"Interactive Simulation","text":""},{"location":"sims/ai-alternatives-decision-tree/#how-to-use-this-microsim","title":"How to Use This MicroSim","text":""},{"location":"sims/ai-alternatives-decision-tree/#quick-start","title":"Quick Start","text":"<ol> <li>Adjust the controls on the right panel to describe your problem characteristics</li> <li>Watch the tree dynamically update as decision paths illuminate based on your inputs</li> <li>Follow the highlighted path from root to leaf to see the recommended approach</li> <li>Read the recommendation to understand why this solution fits your constraints</li> <li>Explore alternatives by tweaking controls to see how recommendations change</li> </ol>"},{"location":"sims/ai-alternatives-decision-tree/#understanding-the-tree","title":"Understanding the Tree","text":"<p>Node Types: - Hexagons (Decision Points): Questions about your problem that guide the recommendation - Rectangles (Recommendations): Specific solution approaches at the end of each path - Arrows (Paths): Labeled with conditions that lead from one decision to the next</p> <p>Color Meanings: - Green: Simple/non-AI solutions (rules, heuristics, traditional algorithms) - Yellow: Hybrid approaches (combining rules with ML) - Blue: Traditional machine learning (regression, decision trees, random forests) - Purple: Deep learning (neural networks, transformers) - Red: Not recommended given your constraints</p> <p>Confidence Meter: The bar chart shows how well your inputs align with the recommendation. High confidence (&gt;80%) means strong fit; low confidence (&lt;60%) suggests edge case or conflicting requirements.</p>"},{"location":"sims/ai-alternatives-decision-tree/#key-controls-to-understand","title":"Key Controls to Understand","text":"<p>Data Availability: How much labeled training data do you have? AI needs data to learn patterns.</p> <p>Pattern Complexity: Can you write simple if-then rules, or are relationships too complex/unknown?</p> <p>Latency Requirements: How fast must the system respond? Real-time needs favor simpler models.</p> <p>Interpretability: Do stakeholders need to understand why decisions were made?</p> <p>Budget Constraint: Development, infrastructure, and maintenance costs vary dramatically by approach.</p>"},{"location":"sims/ai-alternatives-decision-tree/#learning-objectives","title":"Learning Objectives","text":"<p>By using this MicroSim, you will:</p> <ol> <li>Evaluate problem characteristics systematically rather than jumping to solutions</li> <li>Apply decision criteria to match technology to need, not hype</li> <li>Analyze trade-offs between different solution approaches</li> <li>Recognize when simpler non-AI solutions outperform complex ML systems</li> <li>Justify technology choices based on evidence and constraints</li> </ol>"},{"location":"sims/ai-alternatives-decision-tree/#connection-to-course-content","title":"Connection to Course Content","text":"<ul> <li>Chapter 2: AI vs. Non-AI Alternatives\u2014when is AI the right tool?</li> <li>Chapter 3: Trade-off Analysis\u2014balancing accuracy, cost, speed, interpretability</li> <li>Chapter 5: Pivoting Decisions\u2014recognizing when to switch approaches</li> <li>Chapter 6: Implementation Constraints\u2014budget, timeline, team expertise</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/#worked-examples","title":"Worked Examples","text":""},{"location":"sims/ai-alternatives-decision-tree/#example-1-customer-support-routing","title":"Example 1: Customer Support Routing","text":"<p>Problem: Route customer emails to correct department - Data: 10,000 labeled historical emails - Pattern: Multi-factor (keywords, sentiment, urgency) - Latency: 1 second acceptable - Interpretability: Helpful for quality assurance</p> <p>Recommendation: Traditional ML (Naive Bayes or Logistic Regression) - Fast training and inference - Interpretable feature weights - Sufficient data for good accuracy - Low infrastructure costs</p>"},{"location":"sims/ai-alternatives-decision-tree/#example-2-fraud-detection","title":"Example 2: Fraud Detection","text":"<p>Problem: Flag suspicious financial transactions in real-time - Data: Millions of transactions, rare fraud cases - Pattern: Complex, evolving fraud patterns - Latency: &lt;50ms required - Interpretability: Critical for regulatory compliance</p> <p>Recommendation: Hybrid (Rules + Lightweight ML) - Known fraud patterns \u2192 Rules (instant, explainable) - Novel patterns \u2192 Gradient Boosting (fast inference, feature importance) - Handles class imbalance with sampling techniques</p>"},{"location":"sims/ai-alternatives-decision-tree/#example-3-content-moderation","title":"Example 3: Content Moderation","text":"<p>Problem: Detect harmful content in user posts - Data: Abundant, diverse dataset - Pattern: Subtle context-dependent violations - Latency: Batch processing acceptable (review within 5 minutes) - Interpretability: Helpful for appeals process</p> <p>Recommendation: Deep Learning (Fine-tuned Transformer) - Understands context and nuance - Transfer learning from pre-trained models - High accuracy on complex cases - Generate explanations via attention weights</p>"},{"location":"sims/ai-alternatives-decision-tree/#common-insights","title":"Common Insights","text":""},{"location":"sims/ai-alternatives-decision-tree/#when-simple-beats-complex","title":"When Simple Beats Complex","text":"<ul> <li>Clear business rules exist: Don't use ML to learn what you already know</li> <li>Small datasets: Rules or transfer learning outperform custom models</li> <li>High-stakes decisions: Interpretability often trumps marginal accuracy gains</li> <li>Tight budgets: Open-source rules-based systems cost far less than ML infrastructure</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/#when-to-choose-hybrid","title":"When to Choose Hybrid","text":"<ul> <li>Known patterns + edge cases: Rules for 80%, ML for 20%</li> <li>Gradual rollout: Start with rules, add ML incrementally</li> <li>Regulatory constraints: Rules for compliance-critical paths, ML for optimization</li> <li>Team expertise: Leverage domain knowledge via rules, ML for unknowns</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/#when-ai-makes-sense","title":"When AI Makes Sense","text":"<ul> <li>Abundant data: You have 10K+ labeled examples</li> <li>Complex patterns: Relationships are non-linear or unknown</li> <li>Performance matters: Accuracy gains justify costs</li> <li>Evolving domains: Patterns change, requiring adaptive models</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/#reflection-questions","title":"Reflection Questions","text":"<p>After exploring the decision tree, consider:</p> <ol> <li>What surprised you? Did simpler solutions work for problems you assumed needed AI?</li> <li>What constraints matter most? Which inputs most dramatically changed recommendations?</li> <li>Where are you biased? Do you default to complex solutions when simple ones suffice?</li> <li>How would you explain this? Could you justify your recommendation to a non-technical stakeholder?</li> </ol>"},{"location":"sims/ai-alternatives-decision-tree/#assessment-applications","title":"Assessment Applications","text":"<p>This MicroSim supports: - Homework 3: Technology Selection Justification - Midterm Question: \"Given these constraints, recommend and defend a solution approach\" - Final Project: Technology choice documentation and trade-off analysis - Peer Review: Critique classmates' algorithm selections using this framework</p>"},{"location":"sims/ai-alternatives-decision-tree/spec/","title":"MicroSim: AI Alternatives Decision Tree","text":""},{"location":"sims/ai-alternatives-decision-tree/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Decision criteria for selecting AI vs. non-AI solutions</li> <li>Learning Goal: Students will evaluate problem characteristics systematically to identify optimal solution approaches, understanding when AI adds value versus when simpler alternatives suffice</li> <li>Difficulty: Advanced (graduate level)</li> <li>Bloom's Level: Evaluate/Apply</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range Default Effect Data Availability Slider None/Limited/Moderate/Abundant Moderate Influences AI feasibility scoring Data Quality Slider Poor/Fair/Good/Excellent Good Affects confidence in AI recommendations Pattern Complexity Dropdown [Simple Rules, Multi-factor, Non-linear, Unknown] Multi-factor Determines whether rules suffice vs. ML needed Latency Requirements Slider (ms) 1-10000 100 Real-time vs. batch processing Accuracy Needs Slider 50-99.9% 90% Precision requirements Interpretability Dropdown [Not Important, Helpful, Critical] Helpful Explainability requirements Budget Constraint Slider \\(0-\\)1M $50K Development/operational costs Risk Tolerance Dropdown [Low, Medium, High] Medium Acceptable failure modes Regulatory Checkbox group [GDPR, HIPAA, Financial, None] None Compliance constraints Team Expertise Multi-select [ML, Rules, Stats, Domain] Domain Available skills Time to Deploy Slider (weeks) 1-52 12 Development timeline Scale Dropdown [&lt;1K, 1K-100K, 100K-1M, &gt;1M] 1K-100K Expected usage volume Reset Button - - Clear all selections Show Reasoning Toggle On/Off On Display decision rationale"},{"location":"sims/ai-alternatives-decision-tree/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":""},{"location":"sims/ai-alternatives-decision-tree/spec/#tree-structure","title":"Tree Structure","text":"<ul> <li>Root node: \"What solution approach fits best?\"</li> <li>Decision nodes: Questions about problem characteristics (hexagons)</li> <li>Branch paths: User responses leading to next question (arrows with labels)</li> <li>Leaf nodes: Recommended approaches (rounded rectangles)</li> <li>Color coding:</li> <li>Green: Non-AI/Simple solutions</li> <li>Yellow: Hybrid approaches</li> <li>Blue: Traditional ML</li> <li>Purple: Deep learning</li> <li>Red: Not recommended</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#interactive-flow","title":"Interactive Flow","text":"<ul> <li>Active path highlighted: Bold lines show current decision path</li> <li>Dimmed alternatives: Grayed out paths not taken</li> <li>Progress indicator: Breadcrumb trail of decisions made</li> <li>Animated transitions: Smooth pan/zoom to next question</li> <li>Confidence meter: Bar chart showing certainty of recommendation (based on how clear-cut the criteria are)</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#recommendation-display","title":"Recommendation Display","text":"<p>When a leaf node is reached: - Primary recommendation: Large, prominent display - Specific approach: E.g., \"Rule-based system\" or \"Random Forest classifier\" - Confidence score: Percentage based on input alignment - Why this fits: Bullet points explaining the match - Trade-offs: Key advantages and limitations - Alternative to consider: Second-best option with rationale - Chapter links: Related course content sections</p>"},{"location":"sims/ai-alternatives-decision-tree/spec/#tree-layout","title":"Tree Layout","text":"<ul> <li>Top-down orientation: Root at top, recommendations at bottom</li> <li>Collapsible branches: Click to expand/collapse subtrees</li> <li>Minimap: Small overview in corner for navigation</li> <li>Zoom controls: +/- buttons and scroll-to-zoom</li> <li>Path replay: Animation showing decision flow from root to recommendation</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#decision-logic","title":"Decision Logic","text":""},{"location":"sims/ai-alternatives-decision-tree/spec/#key-decision-points","title":"Key Decision Points","text":"<ol> <li>Data Volume Check</li> <li>&lt;1K samples \u2192 Simple rules or transfer learning</li> <li>1K-100K \u2192 Traditional ML</li> <li> <p>100K \u2192 Deep learning viable</p> </li> <li> <p>Pattern Complexity</p> </li> <li>Simple rules \u2192 If-then logic, decision trees</li> <li>Multi-factor \u2192 Regression, ensemble methods</li> <li>Non-linear \u2192 Neural networks, SVM</li> <li> <p>Unknown \u2192 Exploratory data analysis first</p> </li> <li> <p>Real-time Requirements</p> </li> <li>&lt;10ms \u2192 Pre-computed rules, lookup tables</li> <li>10-100ms \u2192 Lightweight models, edge deployment</li> <li> <p>100ms \u2192 Cloud-based ML acceptable</p> </li> <li> <p>Interpretability Needs</p> </li> <li>Critical \u2192 Linear models, decision trees, rules</li> <li>Helpful \u2192 SHAP-enabled models, attention mechanisms</li> <li> <p>Not important \u2192 Black-box models acceptable</p> </li> <li> <p>Budget Constraints</p> </li> <li>&lt;$10K \u2192 Rules-based, open-source ML</li> <li>\\(10K-\\)100K \u2192 Custom ML with standard frameworks</li> <li> <p>$100K \u2192 Deep learning, specialized infrastructure</p> </li> </ol>"},{"location":"sims/ai-alternatives-decision-tree/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>Students discover that most problems don't need cutting-edge AI\u2014simpler solutions often win on speed, cost, interpretability, and maintainability. The tree reveals how problem characteristics, not AI hype, should drive technology choices. They learn to ask \"Does this problem warrant AI?\" before jumping to complex solutions.</p>"},{"location":"sims/ai-alternatives-decision-tree/spec/#technical-notes","title":"Technical Notes","text":"<ul> <li>Canvas: Responsive SVG, minimum 700px width, 600px height</li> <li>Library: D3.js for tree layout and transitions</li> <li>Layout Algorithm: Reingold-Tilford tree with collision detection</li> <li>State Management: Decision history stack for back navigation</li> <li>Mobile: Horizontal scrolling on small screens, touch gestures for pan/zoom</li> <li>Accessibility: Keyboard navigation (arrow keys), ARIA labels for screen readers</li> <li>Performance: Lazy rendering for large trees, virtual scrolling</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#assessment-integration","title":"Assessment Integration","text":"<p>This MicroSim prepares students for: - Quiz Question 1: \"When is a rule-based system preferable to machine learning?\" - Quiz Question 2: \"What problem characteristics favor deep learning over traditional ML?\" - Quiz Question 3: \"How do regulatory constraints affect algorithm selection?\" - Case Study: \"Evaluate whether AI is appropriate for [scenario]\" - Design Exercise: \"Justify your technology choice for the capstone project\"</p>"},{"location":"sims/ai-alternatives-decision-tree/spec/#example-pathways","title":"Example Pathways","text":""},{"location":"sims/ai-alternatives-decision-tree/spec/#path-1-simple-rule-based-solution","title":"Path 1: Simple Rule-Based Solution","text":"<ul> <li>Data: Limited (&lt;1K samples)</li> <li>Pattern: Simple rules</li> <li>Latency: &lt;10ms</li> <li>Interpretability: Critical</li> <li>Recommendation: If-then logic with lookup tables</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#path-2-traditional-ml","title":"Path 2: Traditional ML","text":"<ul> <li>Data: Moderate (10K samples)</li> <li>Pattern: Multi-factor</li> <li>Latency: 100ms acceptable</li> <li>Interpretability: Helpful</li> <li>Recommendation: Random Forest with SHAP explanations</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#path-3-deep-learning","title":"Path 3: Deep Learning","text":"<ul> <li>Data: Abundant (&gt;1M samples)</li> <li>Pattern: Non-linear, unknown</li> <li>Latency: &gt;500ms acceptable</li> <li>Interpretability: Not critical</li> <li>Budget: High</li> <li>Recommendation: Neural network (CNN/LSTM depending on data type)</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#path-4-hybrid-approach","title":"Path 4: Hybrid Approach","text":"<ul> <li>Data: Moderate</li> <li>Pattern: Partially known rules + edge cases</li> <li>Interpretability: Critical for some decisions</li> <li>Recommendation: Rules + ML fallback (rules handle common cases, ML for edge cases)</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#edge-cases","title":"Edge Cases","text":"<ul> <li>Conflicting constraints: Show warning when requirements are incompatible (e.g., real-time + high accuracy + limited data)</li> <li>No clear winner: Display multiple options with tie-breaker questions</li> <li>Insufficient information: Prompt for critical missing inputs before recommendation</li> <li>Overspecified: Detect when user is making arbitrary distinctions, suggest simplification</li> </ul>"},{"location":"sims/ai-alternatives-decision-tree/spec/#extension-opportunities","title":"Extension Opportunities","text":"<ul> <li>Cost calculator: Estimate total cost of ownership for each recommendation</li> <li>Case library: Real-world examples mapped to tree paths</li> <li>Comparative mode: Run multiple scenarios side-by-side</li> <li>Team discussion: Shared sessions with comment threads on decisions</li> <li>Custom trees: Students build their own decision trees for domain-specific problems</li> </ul>"},{"location":"sims/loop-framework-visualizer/","title":"LOOP Framework Visualizer","text":"<p>Master systematic problem framing by walking through the five-stage LOOP framework with interactive guidance and visual feedback.</p>"},{"location":"sims/loop-framework-visualizer/#interactive-simulation","title":"Interactive Simulation","text":""},{"location":"sims/loop-framework-visualizer/#how-to-use-this-microsim","title":"How to Use This MicroSim","text":""},{"location":"sims/loop-framework-visualizer/#getting-started","title":"Getting Started","text":"<ol> <li>Enter your problem statement in the text area (or select a sample problem)</li> <li>Progress through each step sequentially\u2014each stage builds on the previous</li> <li>Complete all required inputs before advancing to the next stage</li> <li>Toggle to Canvas View at any time to see the big picture</li> <li>Export your framework when complete for documentation or presentation</li> </ol>"},{"location":"sims/loop-framework-visualizer/#the-five-stages","title":"The Five Stages","text":"<p>Step 1: Outcome Define what success looks like. Be specific about whose outcome matters (user, business, or technical) and what measurable state you're optimizing for.</p> <p>Step 2: Deconstruction Break the problem into key components. Assign priority levels to focus your analysis. The visualization shows how components relate to your outcome.</p> <p>Step 3: Alternatives Generate multiple solution approaches. Tag each as AI, non-AI, or hybrid. The visualization helps you see the solution landscape.</p> <p>Step 4: Trade-offs Position alternatives on two axes (e.g., Cost vs. Accuracy). Drag points to reflect realistic trade-offs. The Pareto frontier highlights optimal solutions.</p> <p>Step 5: Signals Define success metrics (green light), kill criteria (red light), and leading indicators (yellow light). Set thresholds for decision-making.</p>"},{"location":"sims/loop-framework-visualizer/#tips-for-effective-use","title":"Tips for Effective Use","text":"<ul> <li>Start broad, then narrow: Your first pass can be rough\u2014refine as you progress</li> <li>Revisit earlier steps: If Step 4 reveals issues, go back and adjust components</li> <li>Use the Canvas View: This holistic perspective often reveals gaps or conflicts</li> <li>Save frequently: The MicroSim auto-saves, but export JSON for version control</li> <li>Compare approaches: Try framing the same problem multiple ways</li> </ul>"},{"location":"sims/loop-framework-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>By completing this MicroSim, you will:</p> <ol> <li>Apply the LOOP framework to authentic problem scenarios</li> <li>Analyze how problem components, alternatives, and trade-offs interconnect</li> <li>Evaluate the completeness and coherence of your problem framing</li> <li>Create a comprehensive problem canvas suitable for stakeholder communication</li> <li>Recognize when iteration is needed based on emerging insights</li> </ol>"},{"location":"sims/loop-framework-visualizer/#connection-to-course-content","title":"Connection to Course Content","text":"<ul> <li>Chapter 1: Introduction to LOOP framework fundamentals</li> <li>Chapter 2: Deep dive into alternatives and AI vs. non-AI solutions</li> <li>Chapter 3: Trade-off analysis and decision matrices</li> <li>Chapter 4: Signal design and success metrics</li> <li>Chapter 5: Iterative problem framing and pivoting</li> </ul>"},{"location":"sims/loop-framework-visualizer/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>Vague outcomes: \"Improve user experience\" is too broad\u2014specify what aspect and how much</li> <li>Too few alternatives: Generate at least 3-5 to reveal meaningful trade-offs</li> <li>Ignoring non-AI options: Always consider if AI is truly necessary</li> <li>Unrealistic trade-off positions: Be honest about constraints\u2014no solution is perfect</li> <li>Signals without thresholds: Quantify what \"success\" or \"failure\" actually means</li> </ul>"},{"location":"sims/loop-framework-visualizer/#assessment-applications","title":"Assessment Applications","text":"<p>This MicroSim directly supports: - Homework Assignment 2: LOOP Framework Application - Midterm Case Study: Problem framing under time constraints - Final Project: Comprehensive problem canvas for capstone work - Peer Review Exercise: Critique classmates' frameworks using this structure</p>"},{"location":"sims/loop-framework-visualizer/spec/","title":"MicroSim: LOOP Framework Visualizer","text":""},{"location":"sims/loop-framework-visualizer/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: LOOP Framework (Outcome \u2192 Deconstruction \u2192 Alternatives \u2192 Trade-offs \u2192 Signals)</li> <li>Learning Goal: Students will understand systematic problem framing by completing each stage of the LOOP framework, seeing connections between stages, and generating a holistic problem canvas</li> <li>Difficulty: Advanced (graduate level)</li> <li>Bloom's Level: Analyze/Evaluate</li> </ul>"},{"location":"sims/loop-framework-visualizer/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range Default Effect Problem Statement Text area 500 chars Empty Initial problem description to frame Current Step Button group 1-5 1 Advances through LOOP stages Outcome Input Text field 200 chars Empty Desired end state (Step 1) Outcome Type Dropdown [User, Business, Technical] User Categorizes outcome Components Multi-input 10 max Empty Key problem elements (Step 2) Component Priority Slider (per component) 1-5 3 Importance weighting Alternatives Multi-input 8 max Empty Solution approaches (Step 3) Alternative Type Tag selector [AI, Non-AI, Hybrid] - Categorizes each alternative Trade-off Axes Dropdown (2) [Cost, Speed, Accuracy, Complexity, Data, Risk] Cost/Accuracy X/Y axes for trade-off plot Alternative Position Drag point Canvas space Center Position on trade-off matrix Signal Type Checkbox group Success/Kill/Leading None Signal categories (Step 5) Signal Threshold Number input 0-100 50 Trigger value for each signal View Mode Toggle [Linear, Canvas] Linear Layout style Export Canvas Button - - Download summary as JSON/PNG"},{"location":"sims/loop-framework-visualizer/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":""},{"location":"sims/loop-framework-visualizer/spec/#step-1-outcome","title":"Step 1: Outcome","text":"<ul> <li>Large text display: \"What outcome are we optimizing for?\"</li> <li>Visual icon representing outcome type (user/business/technical)</li> <li>Sticky note aesthetic with outcome statement</li> <li>Color coding: Blue (user), Green (business), Orange (technical)</li> </ul>"},{"location":"sims/loop-framework-visualizer/spec/#step-2-deconstruction","title":"Step 2: Deconstruction","text":"<ul> <li>Tree diagram radiating from outcome</li> <li>Each component as a node with size proportional to priority</li> <li>Connecting lines showing relationships</li> <li>Interactive: hover to see component details</li> <li>Visual weight: Larger circles for higher priority</li> </ul>"},{"location":"sims/loop-framework-visualizer/spec/#step-3-alternatives","title":"Step 3: Alternatives","text":"<ul> <li>Horizontal swimlanes showing different approaches</li> <li>Color coded by type: Purple (AI), Gray (Non-AI), Gradient (Hybrid)</li> <li>Icons representing approach category</li> <li>Expandable cards with brief descriptions</li> <li>Connection lines back to components they address</li> </ul>"},{"location":"sims/loop-framework-visualizer/spec/#step-4-trade-offs","title":"Step 4: Trade-offs","text":"<ul> <li>2D scatter plot with selected axes</li> <li>Each alternative as a draggable point</li> <li>Quadrant labels (e.g., \"High Cost/High Accuracy\")</li> <li>Pareto frontier line highlighting optimal solutions</li> <li>Hover: tooltip with alternative name and scores</li> <li>Visual feedback: optimal region shaded green</li> </ul>"},{"location":"sims/loop-framework-visualizer/spec/#step-5-signals","title":"Step 5: Signals","text":"<ul> <li>Dashboard with three sections: Success/Kill/Leading</li> <li>Gauge visualizations for each signal</li> <li>Threshold lines clearly marked</li> <li>Traffic light colors (green/yellow/red zones)</li> <li>Historical trend sparklines if revisiting</li> </ul>"},{"location":"sims/loop-framework-visualizer/spec/#canvas-view","title":"Canvas View","text":"<ul> <li>All-in-one visual summary</li> <li>Circular layout: Outcome (center) \u2192 Components \u2192 Alternatives \u2192 Trade-offs \u2192 Signals (outer ring)</li> <li>Connection lines showing information flow</li> <li>Printable/exportable format</li> <li>Annotations enabled for note-taking</li> </ul>"},{"location":"sims/loop-framework-visualizer/spec/#interaction-flow","title":"Interaction Flow","text":"<ol> <li>Enter Problem \u2192 Text area activates, sample problems available</li> <li>Define Outcome \u2192 Step 1 unlocks, outcome visualizes as central node</li> <li>Deconstruct \u2192 Step 2 unlocks, components branch from outcome</li> <li>Explore Alternatives \u2192 Step 3 unlocks, approaches populate lanes</li> <li>Assess Trade-offs \u2192 Step 4 unlocks, scatter plot becomes interactive</li> <li>Set Signals \u2192 Step 5 unlocks, dashboard populates</li> <li>Review Canvas \u2192 Toggle to see complete framework</li> <li>Export \u2192 Download summary for external use</li> </ol>"},{"location":"sims/loop-framework-visualizer/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>Students realize that problem framing is not linear but iterative\u2014defining signals may reveal gaps in alternatives, trade-off analysis may expose missing outcome dimensions, and deconstruction may highlight conflicting priorities. The canvas view crystallizes how all elements interconnect into a coherent problem frame.</p>"},{"location":"sims/loop-framework-visualizer/spec/#technical-notes","title":"Technical Notes","text":"<ul> <li>Canvas: Responsive, minimum 600px width, 800px height</li> <li>Library: p5.js for visualization, custom state management</li> <li>Data Structure: JSON object storing all framework elements</li> <li>Persistence: LocalStorage auto-save every 30 seconds</li> <li>Mobile: Touch-optimized, vertical layout below 768px</li> <li>Accessibility: Keyboard navigation through steps, screen reader annotations</li> <li>Export: PNG canvas snapshot + JSON data download</li> </ul>"},{"location":"sims/loop-framework-visualizer/spec/#assessment-integration","title":"Assessment Integration","text":"<p>This MicroSim prepares students for: - Quiz Question 1: \"Which LOOP stage identifies success/kill metrics?\" - Quiz Question 2: \"Why must outcomes be defined before alternatives?\" - Quiz Question 3: \"How do trade-offs inform signal selection?\" - Assignment: \"Apply LOOP framework to your capstone project problem\"</p>"},{"location":"sims/loop-framework-visualizer/spec/#edge-cases","title":"Edge Cases","text":"<ul> <li>Empty components: Warning message, cannot advance to Step 3</li> <li>Single alternative: Prompt to add at least 3 for comparison</li> <li>Overlapping trade-off points: Slight jitter to separate visually</li> <li>No signals defined: Cannot complete framework, Step 5 required</li> </ul>"},{"location":"sims/loop-framework-visualizer/spec/#extension-opportunities","title":"Extension Opportunities","text":"<ul> <li>Collaborative mode: Multiple users frame same problem, compare canvases</li> <li>Template library: Pre-populated examples from industry cases</li> <li>AI suggestions: GPT-powered alternative generation or component identification</li> <li>Version history: Track how problem framing evolves over iterations</li> </ul>"},{"location":"sims/pivot-decision-matrix/","title":"Pivot Decision Matrix","text":"<p>Visualize multi-dimensional trade-offs across pivot alternatives using 2x2 matrices, bubble charts, radar plots, and weighted scoring to make defensible decisions when options have conflicting strengths.</p>"},{"location":"sims/pivot-decision-matrix/#interactive-simulation","title":"Interactive Simulation","text":""},{"location":"sims/pivot-decision-matrix/#how-to-use-this-microsim","title":"How to Use This MicroSim","text":""},{"location":"sims/pivot-decision-matrix/#quick-start","title":"Quick Start","text":"<ol> <li>Select a template (Feature Pivot, Model Change, Market Shift, or Resource Reallocation) or start with \"Custom\"</li> <li>Choose visualization type: 2x2 Matrix (default), Bubble Chart, Radar Chart, or Weighted Table</li> <li>Add your pivot options if starting custom (aim for 4-8 alternatives to compare)</li> <li>Score each option on relevant dimensions using sliders</li> <li>Adjust dimension weights to reflect what matters most to your decision</li> <li>Analyze the visualization to identify quick wins, money pits, and optimal trade-offs</li> <li>Use Compare Mode to examine top contenders side-by-side</li> </ol>"},{"location":"sims/pivot-decision-matrix/#visualization-types","title":"Visualization Types","text":"<p>2x2 Matrix (Impact vs. Effort): Best for quick, high-level comparison of alternatives when two dimensions dominate the decision.</p> <ul> <li>Quick Wins (Bottom-right): High impact, low effort\u2014prioritize these</li> <li>Big Bets (Top-right): High impact, high effort\u2014strategic investments requiring resources</li> <li>Money Pits (Top-left): Low impact, high effort\u2014avoid these</li> <li>Low Priority (Bottom-left): Low impact, low effort\u2014defer or skip</li> </ul> <p>Bubble Chart (3 Dimensions): Best when three dimensions matter (X-axis, Y-axis, bubble size). For example: Impact (X) vs. Risk (Y), sized by Confidence.</p> <ul> <li>Larger bubbles are more prominent on the third dimension</li> <li>Position shows two-dimensional trade-off</li> <li>Pareto frontier highlights non-dominated options (can't improve one dimension without worsening another)</li> </ul> <p>Radar Chart (Multi-dimensional Profile): Best for holistic comparison across 4-8 dimensions simultaneously.</p> <ul> <li>Each axis represents one dimension</li> <li>Larger polygons indicate stronger options overall</li> <li>Shape reveals strength/weakness patterns (e.g., technically strong but risky)</li> </ul> <p>Weighted Table (Comprehensive Scoring): Best for systematic, defensible decisions with many dimensions.</p> <ul> <li>Adjust dimension weights to reflect priorities</li> <li>Weighted score auto-calculates: \u03a3(dimension_score \u00d7 dimension_weight) / \u03a3(weights)</li> <li>Sort by rank to see recommended options</li> <li>Export to CSV for stakeholder review</li> </ul>"},{"location":"sims/pivot-decision-matrix/#understanding-dimension-types","title":"Understanding Dimension Types","text":"<p>Benefit Dimensions (higher is better): - Impact: User/business value created - Value: Revenue or cost savings - Speed: Time to market - Certainty: Confidence in estimates - Quality: Output excellence</p> <p>Cost Dimensions (lower is better): - Effort: Development time - Cost: Budget required - Risk: Uncertainty/downside - Complexity: Maintenance burden</p> <p>Note: The MicroSim automatically inverts cost dimensions in calculations (low effort = high score).</p>"},{"location":"sims/pivot-decision-matrix/#using-the-pareto-frontier","title":"Using the Pareto Frontier","text":"<p>The Pareto frontier connects all non-dominated options\u2014those where you can't improve one dimension without sacrificing another.</p> <p>Key insight: Options not on the frontier are suboptimal\u2014there exists another option that's better on all dimensions (or equal on some, better on others).</p> <p>Decision strategy: 1. Eliminate options below the frontier (dominated alternatives) 2. Choose among frontier options based on your priorities (dimension weights) 3. If undecided, run sensitivity analysis (adjust weights to see if recommendation changes)</p>"},{"location":"sims/pivot-decision-matrix/#compare-mode","title":"Compare Mode","text":"<p>Select 2-3 options to see a side-by-side comparison table: - Row per dimension with scores for each option - Difference column shows gaps - Visual bars for quick scanning - \"Winner\" indicator per dimension - Overall recommendation based on weighted scores</p> <p>Use this when: - Narrowing from 5+ options to final 2-3 - Explaining trade-offs to stakeholders - Debugging why scores are similar despite different profiles</p>"},{"location":"sims/pivot-decision-matrix/#learning-objectives","title":"Learning Objectives","text":"<p>By using this MicroSim, you will:</p> <ol> <li>Evaluate pivot options across multiple competing dimensions systematically</li> <li>Apply the Pareto frontier concept to eliminate dominated alternatives</li> <li>Analyze how dimension weights affect recommendations and rankings</li> <li>Create defensible decision frameworks for stakeholder communication</li> <li>Recognize that \"best\" is context-dependent\u2014optimal choice depends on priorities</li> </ol>"},{"location":"sims/pivot-decision-matrix/#connection-to-course-content","title":"Connection to Course Content","text":"<ul> <li>Chapter 3: Trade-off Analysis\u2014balancing competing objectives</li> <li>Chapter 5: Pivot Decisions\u2014when and how to change course</li> <li>Chapter 6: Stakeholder Alignment\u2014making decisions transparent and defensible</li> <li>Chapter 7: Resource Constraints\u2014working within budget, time, and team limitations</li> </ul>"},{"location":"sims/pivot-decision-matrix/#worked-examples","title":"Worked Examples","text":""},{"location":"sims/pivot-decision-matrix/#example-1-feature-pivot-product-development","title":"Example 1: Feature Pivot (Product Development)","text":"<p>Scenario: Your AI-powered shopping app has low engagement. You need to pivot the recommendation feature.</p> <p>Options: 1. AI Recommendation Engine: High impact (8), High effort (7), High cost (8) 2. Manual Curation: Medium impact (6), Medium effort (5), Medium cost (4) 3. Hybrid (AI + Human): High impact (8), High effort (6), High cost (6) 4. User-Generated: Medium impact (5), Low effort (3), Low cost (2) 5. No Personalization: Low impact (2), Low effort (1), Low cost (1)</p> <p>2x2 Matrix (Impact vs. Effort): - Quick Win: User-Generated (medium impact, low effort) - Big Bet: Hybrid (high impact, medium-high effort) - Money Pit: Pure AI (high impact, but highest effort/cost) - Low Priority: No Personalization</p> <p>Weighted Score Analysis: - Dimensions: Impact (weight: 10), Effort (weight: 8, inverted), Cost (weight: 6, inverted), Speed (weight: 7, inverted) - Winner: Hybrid approach (balances impact with manageable effort/cost) - Runner-up: User-Generated (fastest to market, decent impact)</p> <p>Decision: Start with User-Generated for quick validation, then layer in Hybrid AI if initial results promising.</p>"},{"location":"sims/pivot-decision-matrix/#example-2-model-change-technical-pivot","title":"Example 2: Model Change (Technical Pivot)","text":"<p>Scenario: Your image classifier has 75% accuracy but is too slow (500ms inference). Stakeholders demand improvement.</p> <p>Options: 1. Keep Current (Baseline): Accuracy (5), Speed (2), Complexity (4), Interpretability (6) 2. Transformer Model: Accuracy (9), Speed (1), Complexity (9), Interpretability (3) 3. Ensemble: Accuracy (8), Speed (3), Complexity (8), Interpretability (4) 4. Simplify to Linear: Accuracy (3), Speed (10), Complexity (2), Interpretability (10) 5. Third-Party API: Accuracy (7), Speed (9), Complexity (1), Interpretability (5)</p> <p>Bubble Chart (Accuracy vs. Speed, sized by Interpretability): - Pareto Frontier: Third-Party API, Ensemble, Transformer - Dominated: Keep Current (Third-Party API beats it on all dimensions) - Trade-off: Transformer (best accuracy, worst speed) vs. Third-Party API (good accuracy, great speed)</p> <p>Weighted Score Analysis (Stakeholder: Product Manager): - Dimensions: Accuracy (weight: 7), Speed (weight: 9), Complexity (weight: 5, inverted), Interpretability (weight: 4) - Winner: Third-Party API (balances accuracy and speed with low complexity)</p> <p>Weighted Score Analysis (Stakeholder: Data Scientist): - Dimensions: Accuracy (weight: 10), Speed (weight: 6), Complexity (weight: 3, inverted), Interpretability (weight: 5) - Winner: Ensemble (maximizes accuracy while keeping reasonable speed)</p> <p>Decision: Stakeholder priorities differ\u2014Product Manager prioritizes speed/simplicity, Data Scientist prioritizes accuracy. Negotiation needed, or run A/B test comparing Third-Party API vs. Ensemble.</p>"},{"location":"sims/pivot-decision-matrix/#example-3-market-shift-strategic-pivot","title":"Example 3: Market Shift (Strategic Pivot)","text":"<p>Scenario: Your AI tool has 500 SMB customers but growth plateaued. Consider pivoting to enterprise or freemium.</p> <p>Options: 1. Enterprise B2B: Revenue Potential (9), CAC (8, inverted), Time to Revenue (7, inverted), Market Size (6), Competition (7, inverted), Team Fit (5) 2. Consumer B2C: Revenue Potential (7), CAC (4, inverted), Time to Revenue (5, inverted), Market Size (10), Competition (9, inverted), Team Fit (4) 3. Freemium + Premium: Revenue Potential (8), CAC (3, inverted), Time to Revenue (4, inverted), Market Size (8), Competition (8, inverted), Team Fit (7) 4. White-Label Licensing: Revenue Potential (6), CAC (5, inverted), Time to Revenue (6, inverted), Market Size (5), Competition (4, inverted), Team Fit (6)</p> <p>Radar Chart: - Enterprise B2B: Strong on revenue potential, weak on team fit and competition - Consumer B2C: Huge market size, but fierce competition and CAC challenges - Freemium: Balanced profile, strongest team fit - White-Label: Most differentiated (least competition), but limited upside</p> <p>Weighted Score (Board Priorities): - Dimensions: Revenue Potential (weight: 10), Time to Revenue (weight: 8), Market Size (weight: 6), Competition (weight: 7), Team Fit (weight: 5) - Winner: Freemium (scores well across all dimensions, no critical weaknesses) - Runner-up: Enterprise B2B (highest revenue potential, but execution risk on team fit)</p> <p>Decision: Pursue Freemium with option to upsell SMB customers to Premium tier. Monitor Enterprise demand and revisit in 12 months if traction.</p>"},{"location":"sims/pivot-decision-matrix/#strategic-insights","title":"Strategic Insights","text":""},{"location":"sims/pivot-decision-matrix/#when-options-have-similar-weighted-scores","title":"When Options Have Similar Weighted Scores","text":"<p>Scenario: Top two options within 5% of each other</p> <p>Strategies: 1. Sensitivity analysis: Adjust dimension weights to see if recommendation changes 2. De-risking: Choose option with fewer critical weaknesses (no dimension &lt; 3/10) 3. Experimentation: Run limited pilot/A/B test to gather real data 4. Stakeholder tiebreaker: Let key decision-maker prioritize based on strategic vision</p>"},{"location":"sims/pivot-decision-matrix/#when-no-clear-quick-wins-exist","title":"When No Clear Quick Wins Exist","text":"<p>All options are high effort or low impact</p> <p>Strategies: 1. Question the framing: Are these really the only options? Generate more alternatives 2. Phased approach: Break big bets into smaller milestones 3. Hybrid solutions: Combine elements of multiple options 4. Defer decision: Gather more data before committing</p>"},{"location":"sims/pivot-decision-matrix/#when-stakeholders-disagree-on-weights","title":"When Stakeholders Disagree on Weights","text":"<p>Example: Engineering wants low complexity, Sales wants high impact, Finance wants low cost</p> <p>Strategies: 1. Show multiple weighted views: One per stakeholder role 2. Find consensus option: Option that scores well across all weighting schemes 3. Sequential optimization: Satisfy critical constraint first (e.g., cost &lt; $50K), then optimize impact 4. Escalate to executive: Let leadership set priority order</p>"},{"location":"sims/pivot-decision-matrix/#design-principles-for-good-decision-matrices","title":"Design Principles for Good Decision Matrices","text":""},{"location":"sims/pivot-decision-matrix/#characteristics-of-effective-dimensions","title":"Characteristics of Effective Dimensions","text":"<p>Independent: Dimensions shouldn't be redundant - Bad: \"Impact\" and \"Value\" (highly correlated) - Good: \"Impact\" (user outcomes) and \"Cost\" (budget)</p> <p>Measurable: Scores should be based on evidence, not just intuition - Bad: \"Strategic fit\" (vague) - Good: \"Time to first customer\" (concrete, measurable)</p> <p>Actionable: Dimensions should matter to the decision - Bad: \"Technology coolness\" (doesn't affect success) - Good: \"Regulatory compliance risk\" (critical for healthcare AI)</p> <p>Balanced: Mix of benefit and cost dimensions - Bad: All benefit dimensions \u2192 Every option looks good - Good: Impact, Speed (benefits) vs. Cost, Risk (costs)</p>"},{"location":"sims/pivot-decision-matrix/#how-many-options-to-compare","title":"How Many Options to Compare","text":"<ul> <li>Too few (&lt; 3): Limited exploration, may miss better alternatives</li> <li>Sweet spot (4-8): Enough diversity without overwhelming analysis</li> <li>Too many (&gt; 10): Decision paralysis, visualization clutter</li> </ul> <p>Strategy: Start with brainstorm (10+ options), eliminate obvious non-starters, analyze remaining 4-8 in depth.</p>"},{"location":"sims/pivot-decision-matrix/#scoring-calibration","title":"Scoring Calibration","text":"<p>Common mistakes: - Grade inflation: Everything scores 7-10, no differentiation - Sandbagging: Everything scores 3-6, loses aspirational options - Relative vs. absolute: Mixing \"best in this set\" with \"best imaginable\"</p> <p>Best practice: - Anchor the scale: Define what 0, 5, and 10 mean concretely   - Example (Impact): 0 = no user value, 5 = 10% engagement lift, 10 = 2x growth - Use evidence: Base scores on data, user research, benchmarks - Document assumptions: Note uncertainties in option descriptions</p>"},{"location":"sims/pivot-decision-matrix/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Ignoring the Pareto frontier: Wasting time analyzing dominated options</li> <li> <p>Solution: Eliminate non-frontier options early</p> </li> <li> <p>Overweighting easy-to-measure dimensions: Effort/cost are concrete, impact is uncertain</p> </li> <li> <p>Solution: Resist bias toward known quantities, force rigorous impact estimation</p> </li> <li> <p>Analysis paralysis: Adding more dimensions indefinitely</p> </li> <li> <p>Solution: Limit to 5-8 dimensions, focus on decision-critical factors</p> </li> <li> <p>Forgetting opportunity cost: Choosing Option A means NOT choosing Option B</p> </li> <li> <p>Solution: Compare mode highlights what you're giving up</p> </li> <li> <p>Treating weights as static: Priorities change as project progresses</p> </li> <li>Solution: Revisit weights quarterly, re-run analysis with updated priorities</li> </ol>"},{"location":"sims/pivot-decision-matrix/#reflection-questions","title":"Reflection Questions","text":"<p>After completing your pivot decision matrix:</p> <ol> <li>Robustness: If your top dimension weight changed by 50%, would the recommended option still win?</li> <li>Regret minimization: Which option would you least regret choosing if you're wrong about assumptions?</li> <li>Reversibility: How hard is it to switch from your chosen option if it fails?</li> <li>Evidence gaps: Which dimension scores have the highest uncertainty? Can you de-risk with experiments?</li> <li>Team alignment: Would your team agree with these weights and scores, or do you need negotiation?</li> </ol>"},{"location":"sims/pivot-decision-matrix/#assessment-applications","title":"Assessment Applications","text":"<p>This MicroSim supports: - Homework 5: Pivot Decision Analysis - Midterm Case Study: Evaluate pivot options for a failing AI product - Final Project: Create and defend pivot decision matrix with 5+ alternatives - Peer Review: Critique classmates' dimension selection, scoring, and weighting - Team Exercise: Negotiate stakeholder weights to reach consensus recommendation - Written Report: Document decision rationale with sensitivity analysis</p>"},{"location":"sims/pivot-decision-matrix/spec/","title":"MicroSim: Pivot Decision Matrix","text":""},{"location":"sims/pivot-decision-matrix/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Multi-dimensional trade-off analysis for pivot decisions using weighted scoring matrices</li> <li>Learning Goal: Students will evaluate pivot options across competing dimensions, visualize trade-offs spatially, apply weighted scoring, and make defensible decisions when alternatives have conflicting strengths</li> <li>Difficulty: Advanced (graduate level)</li> <li>Bloom's Level: Evaluate/Create</li> </ul>"},{"location":"sims/pivot-decision-matrix/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range Default Effect Matrix Type Dropdown [2x2, Bubble Chart, Radar, Weighted Table] 2x2 Visualization style X-Axis Dimension Dropdown [Impact, Effort, Cost, Risk, Speed, Complexity, Value, Certainty] Impact Horizontal axis Y-Axis Dimension Dropdown [Impact, Effort, Cost, Risk, Speed, Complexity, Value, Certainty] Effort Vertical axis Bubble Size (if bubble) Dropdown [Confidence, ROI, Team Buy-in, Data Quality] Confidence Third dimension Add Option Button + Form - - Create new pivot option Option Name Text input 50 chars Empty Label for alternative X-Axis Score Slider 0-10 5 Position on horizontal Y-Axis Score Slider 0-10 5 Position on vertical Bubble Value Slider 0-10 5 Size (if bubble chart) Option Color Color picker - Random Visual identification Option Notes Text area 200 chars Empty Rationale, risks, assumptions Remove Option Button (per option) - - Delete alternative Add Dimension Button + Form - - Custom scoring criteria Dimension Name Text input 30 chars Empty Custom dimension label Dimension Weight Slider 1-10 5 Importance in overall score Score Options Button - - Rate all options on this dimension Scoring Mode Toggle [Manual, Auto-calculate] Manual Weighted sum or direct input Pareto Toggle Checkbox On/Off On Highlight Pareto frontier Grid Lines Checkbox On/Off On Show quadrant dividers Labels Checkbox On/Off On Show option names on chart Zoom/Pan Mouse/Touch - - Navigate large matrices Compare Mode Checkbox On/Off Off Side-by-side option comparison Selected Options Multi-select - None Compare subset of options Export Button - - Download as PNG/PDF/CSV Load Template Dropdown [Feature Pivot, Model Change, Market Shift, Resource Reallocate] Custom Pre-configured scenarios"},{"location":"sims/pivot-decision-matrix/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":""},{"location":"sims/pivot-decision-matrix/spec/#2x2-matrix-view-default","title":"2x2 Matrix View (Default)","text":"<p>Layout: - X-axis: Selected dimension (e.g., Impact), 0-10 scale - Y-axis: Selected dimension (e.g., Effort), 0-10 scale - Four quadrants with labels:   - Top-right (High Impact, High Effort): \"Big Bets\" (yellow)   - Top-left (Low Impact, High Effort): \"Money Pits\" (red)   - Bottom-right (High Impact, Low Effort): \"Quick Wins\" (green)   - Bottom-left (Low Impact, Low Effort): \"Low Priority\" (gray)</p> <p>Option Representation: - Colored circles placed at (X-score, Y-score) coordinates - Circle size: Fixed or proportional to third dimension if enabled - Option name as label (togglable) - Draggable: Click and drag to reposition - Hover: Tooltip with all dimension scores and notes</p> <p>Pareto Frontier: - Dashed line connecting non-dominated options - Options below/left of frontier are suboptimal - Highlighted in bold or with star icon</p>"},{"location":"sims/pivot-decision-matrix/spec/#bubble-chart-view","title":"Bubble Chart View","text":"<p>3D Representation: - X-axis: Dimension 1 (e.g., Impact) - Y-axis: Dimension 2 (e.g., Risk) - Bubble size: Dimension 3 (e.g., Confidence) - Color: Option category or custom grouping - Legend: Bubble size scale (small = low, large = high)</p> <p>Interaction: - Click bubble: Expand detail panel with all scores - Drag bubble: Adjust X/Y scores, size auto-updates if linked to formula - Overlap detection: Slight jitter to separate overlapping bubbles</p>"},{"location":"sims/pivot-decision-matrix/spec/#radar-chart-view","title":"Radar Chart View","text":"<p>Multi-dimensional Profile: - Polygon per option overlaid on same axes - Each vertex: One dimension (up to 8 dimensions supported) - Shaded area: Represents option's overall strength profile - Color-coded by option - Hover: Highlight single option, dim others</p> <p>Interaction: - Click vertex: Edit score for that dimension - Toggle options on/off to reduce visual clutter - Rotate chart to prioritize different dimensions</p>"},{"location":"sims/pivot-decision-matrix/spec/#weighted-table-view","title":"Weighted Table View","text":"<p>Spreadsheet-style Matrix: - Rows: Pivot options - Columns: Dimensions + Weighted Score + Rank - Each cell: Editable score (0-10) - Dimension column headers: Weight slider - Weighted score formula: \u03a3(dimension_score \u00d7 dimension_weight) / \u03a3(weights) - Auto-sort by rank or any column - Conditional formatting: Green (high scores), Red (low scores)</p> <p>Interaction: - Click cell: Edit score - Click header: Adjust weight - Click option row: Expand notes section - Export to CSV for external analysis</p>"},{"location":"sims/pivot-decision-matrix/spec/#compare-mode-side-panel","title":"Compare Mode (Side Panel)","text":"<p>When 2+ options selected: - Split-screen comparison table - Row per dimension showing side-by-side scores - Difference column: Option A - Option B - Visual bar charts for quick comparison - \"Winner\" indicator per dimension - Overall recommendation based on weighted scores</p>"},{"location":"sims/pivot-decision-matrix/spec/#decision-logic","title":"Decision Logic","text":""},{"location":"sims/pivot-decision-matrix/spec/#weighted-scoring-algorithm","title":"Weighted Scoring Algorithm","text":"<pre><code>For each option:\n  WeightedScore = \u03a3(DimensionScore_i \u00d7 DimensionWeight_i) / \u03a3(DimensionWeight_i)\n  Rank = Sort options by WeightedScore descending\n</code></pre>"},{"location":"sims/pivot-decision-matrix/spec/#pareto-frontier-detection","title":"Pareto Frontier Detection","text":"<pre><code>Option A dominates Option B if:\n  - A scores \u2265 B on all dimensions (for benefit dimensions like Impact)\n  - At least one dimension where A &gt; B\n\nPareto Frontier = All non-dominated options\n</code></pre>"},{"location":"sims/pivot-decision-matrix/spec/#recommendation-engine","title":"Recommendation Engine","text":"<p>Quick Win Detection (2x2 Matrix): - Bottom-right quadrant (High Impact, Low Effort) - Recommendation: \"Prioritize these\u2014high ROI\"</p> <p>Money Pit Detection (2x2 Matrix): - Top-left quadrant (Low Impact, High Effort) - Recommendation: \"Avoid these\u2014poor ROI\"</p> <p>Big Bet Decision (2x2 Matrix): - Top-right quadrant (High Impact, High Effort) - Recommendation: \"Strategic choice\u2014high risk/reward, ensure resources available\"</p> <p>Weighted Score Recommendation: - Top 3 options by weighted score - Flag ties (&lt; 5% difference) - Warn if top option has critical weakness (any dimension &lt; 3/10)</p>"},{"location":"sims/pivot-decision-matrix/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>Students discover that the \"best\" pivot depends on what you optimize for\u2014changing dimension weights or axes completely reorders options. They see that optimal choices lie on the Pareto frontier (can't improve one dimension without worsening another), and that dominated options should be eliminated immediately. The visualization makes trade-offs visceral: you can't have high impact, low effort, low risk, and low cost all at once. Decisions require accepting trade-offs.</p>"},{"location":"sims/pivot-decision-matrix/spec/#technical-notes","title":"Technical Notes","text":"<ul> <li>Canvas: Responsive SVG, minimum 600px \u00d7 600px for 2x2, scalable for bubble/radar</li> <li>Library: D3.js for charts, custom drag-and-drop handlers</li> <li>Layout Algorithm: Force-directed for bubble overlap prevention, automatic label positioning</li> <li>State Management: Option array with {name, scores[], notes, color} objects</li> <li>Persistence: LocalStorage for autosave, export to JSON for versioning</li> <li>Mobile: Touch gestures for drag, pinch-to-zoom, tap for tooltips</li> <li>Accessibility: Keyboard navigation (tab through options, arrow keys to adjust scores), screen reader table fallback</li> <li>Performance: Canvas rendering for &gt;50 options, SVG for &lt;50</li> </ul>"},{"location":"sims/pivot-decision-matrix/spec/#assessment-integration","title":"Assessment Integration","text":"<p>This MicroSim prepares students for: - Quiz Question 1: \"What is a Pareto frontier and why does it matter for pivot decisions?\" - Quiz Question 2: \"Why might different stakeholders choose different pivots from the same matrix?\" - Quiz Question 3: \"How does changing dimension weights affect the recommended pivot?\" - Case Study: \"Analyze this pivot decision matrix and defend your recommended choice\" - Design Exercise: \"Create a pivot decision matrix for your capstone project with 5+ alternatives\"</p>"},{"location":"sims/pivot-decision-matrix/spec/#pre-configured-templates","title":"Pre-configured Templates","text":""},{"location":"sims/pivot-decision-matrix/spec/#feature-pivot-product-development","title":"Feature Pivot (Product Development)","text":"<p>Options: 1. AI-powered recommendation engine 2. Manual curation by experts 3. Hybrid (AI suggestions + human override) 4. User-generated content (crowdsourced) 5. No personalization (one-size-fits-all)</p> <p>Dimensions: - Impact: User engagement lift (0 = none, 10 = 2x growth) - Effort: Development time in months (0 = 1 month, 10 = 12+ months) - Cost: Budget required (0 = $10K, 10 = $500K+) - Risk: Uncertainty in outcome (0 = proven, 10 = experimental) - Speed to market: Time to first user value (0 = 1 week, 10 = 6+ months)</p> <p>Default Axis: Impact (X) vs. Effort (Y)</p>"},{"location":"sims/pivot-decision-matrix/spec/#model-change-technical-pivot","title":"Model Change (Technical Pivot)","text":"<p>Options: 1. Keep current model architecture 2. Switch to transformer-based model 3. Ensemble of multiple models 4. Simplify to linear regression 5. Outsource to third-party API</p> <p>Dimensions: - Accuracy: Expected performance improvement (0 = worse, 10 = 50%+ better) - Complexity: Model maintenance burden (0 = simple, 10 = very complex) - Inference Speed: Latency impact (0 = 10x slower, 10 = 10x faster) - Data Requirements: Labeled data needed (0 = 100 samples, 10 = 1M+ samples) - Interpretability: Explainability for stakeholders (0 = black box, 10 = fully transparent)</p> <p>Default Axis: Accuracy (X) vs. Complexity (Y), Bubble size = Interpretability</p>"},{"location":"sims/pivot-decision-matrix/spec/#market-shift-strategic-pivot","title":"Market Shift (Strategic Pivot)","text":"<p>Options: 1. Enterprise B2B focus 2. Consumer B2C focus 3. Freemium + Premium tiers 4. White-label licensing 5. Open-source + Consulting</p> <p>Dimensions: - Revenue Potential: 3-year projection (0 = &lt;$1M, 10 = $50M+) - Customer Acquisition Cost: CAC ratio (0 = low, 10 = high) - Time to Revenue: Months to first paying customer (0 = 1 month, 10 = 12+ months) - Market Size: Total addressable market (0 = niche, 10 = mass market) - Competitive Intensity: Existing player strength (0 = blue ocean, 10 = red ocean) - Team Fit: Expertise alignment (0 = no experience, 10 = perfect fit)</p> <p>Default Axis: Revenue Potential (X) vs. Competitive Intensity (Y)</p>"},{"location":"sims/pivot-decision-matrix/spec/#resource-reallocation-operational-pivot","title":"Resource Reallocation (Operational Pivot)","text":"<p>Options: 1. Hire 3 ML engineers 2. Hire 1 data scientist + outsource labeling 3. Buy third-party tools ($100K/year) 4. Retrain existing team (6-month bootcamp) 5. Partner with university research lab</p> <p>Dimensions: - Capability Gain: New skills acquired (0 = minimal, 10 = transformative) - Cost: Total expenditure (0 = $0, 10 = $500K+) - Speed: Time to operational (0 = 1 week, 10 = 12+ months) - Sustainability: Long-term viability (0 = temporary fix, 10 = permanent solution) - Quality: Output excellence (0 = adequate, 10 = world-class)</p> <p>Default Axis: Capability Gain (X) vs. Cost (Y)</p>"},{"location":"sims/pivot-decision-matrix/spec/#edge-cases","title":"Edge Cases","text":"<ul> <li>All options in same quadrant: Suggest adjusting scales or choosing different dimensions</li> <li>No clear winner: Display top 3 with recommendation to gather more data or run experiments</li> <li>Extreme outlier: Option with one 10/10 score and rest 1/10 \u2192 Flag as \"high risk specialist choice\"</li> <li>Ties in weighted score: Show sensitivity analysis (how much would weights need to change to break tie?)</li> <li>Contradictory dimensions: E.g., \"Impact\" and \"Value\" are redundant \u2192 Warn user to diversify dimensions</li> </ul>"},{"location":"sims/pivot-decision-matrix/spec/#extension-opportunities","title":"Extension Opportunities","text":"<ul> <li>Sensitivity analysis: Slider to adjust all weights, watch ranking change in real-time</li> <li>Monte Carlo simulation: Add uncertainty ranges to scores, run 1000 simulations, show probability of each option being best</li> <li>Stakeholder views: Different stakeholder roles (CEO, Engineer, User) with preset dimension weights</li> <li>Time dimension: Animate how option scores evolve over project timeline</li> <li>Collaborative scoring: Team members vote on scores independently, consensus view with outlier detection</li> <li>AI suggestions: GPT recommends dimension scores based on option descriptions, human can override</li> </ul>"},{"location":"sims/signal-dashboard/","title":"Signal Dashboard","text":"<p>Design and monitor success, kill, and leading indicators to make objective persist/pivot/stop decisions for AI projects based on threshold-driven signals.</p>"},{"location":"sims/signal-dashboard/#interactive-simulation","title":"Interactive Simulation","text":""},{"location":"sims/signal-dashboard/#how-to-use-this-microsim","title":"How to Use This MicroSim","text":""},{"location":"sims/signal-dashboard/#getting-started","title":"Getting Started","text":"<ol> <li>Observe the dashboard with 6 pre-configured signals across three types (success, kill, leading)</li> <li>Adjust current values using the sliders on the right panel to simulate different scenarios</li> <li>Watch the gauges update with color-coded zones (green/yellow/red)</li> <li>Monitor the recommendation banner that shows PERSIST, PIVOT, or STOP based on signal health</li> <li>Click \"Simulate Time\" to watch signals drift over time and practice real-time decision-making</li> </ol>"},{"location":"sims/signal-dashboard/#signal-types-explained","title":"Signal Types Explained","text":"<p>Success Signals (Green border): These measure positive outcomes you're trying to maximize. Examples: - Model accuracy on validation set - User engagement rate - Revenue generated - Customer satisfaction score</p> <p>Kill Signals (Red border): These measure negative outcomes that could force project termination. Examples: - Error rate or bugs per release - Cost overrun percentage - Customer churn rate - Regulatory compliance violations</p> <p>Leading Indicators (Blue border): These predict future success or failure before it happens. Examples: - Data labeling progress (predicts when model training can start) - Developer velocity (predicts delivery timeline) - Early user adoption rate (predicts long-term growth) - Model drift detection (predicts accuracy degradation)</p>"},{"location":"sims/signal-dashboard/#setting-good-thresholds","title":"Setting Good Thresholds","text":"<p>Green Threshold: The target you're aiming for. Reaching this means you're succeeding.</p> <p>Yellow Threshold: The warning zone. Not failing yet, but attention needed.</p> <p>Red Threshold: The danger zone. Below this (for success) or above this (for kill) signals serious problems.</p> <p>Example: Model Accuracy (Success Signal) - Green: \u2265 90% (excellent performance) - Yellow: 75-89% (acceptable but needs improvement) - Red: &lt; 75% (unacceptable, requires pivot or stop)</p> <p>Example: API Latency (Kill Signal) - Green: &lt; 100ms (fast response) - Yellow: 100-500ms (sluggish but tolerable) - Red: \u2265 500ms (unacceptable user experience)</p>"},{"location":"sims/signal-dashboard/#using-the-time-simulation","title":"Using the Time Simulation","text":"<p>Click \"Simulate Time\" to watch signals evolve over time. This helps you: - See how trends develop before thresholds are crossed - Test different scenarios (what if accuracy declines slowly vs. suddenly?) - Practice decision-making under changing conditions - Understand leading indicators that predict future state</p> <p>Click \"Reset\" to return to the default values and start fresh.</p>"},{"location":"sims/signal-dashboard/#interpreting-the-decision-recommendation","title":"Interpreting the Decision Recommendation","text":"<p>The dashboard combines all signals into a weighted recommendation:</p> <p>Persist: Green light\u2014continue with current approach - Most success signals green - Kill signals under control - Leading indicators positive</p> <p>Pivot: Yellow light\u2014change strategy but don't abandon - Mixed signals (some yellow/red) - Recovery is possible with course correction - Leading indicators suggest potential</p> <p>Stop: Red light\u2014terminate the project - Critical kill signals in red zone - Success signals consistently failing - Leading indicators predict continued failure - Sunk cost fallacy alert\u2014cut losses</p> <p>Decision Confidence shows how clear-cut the recommendation is. Low confidence (&lt; 60%) means you're in a gray area requiring deeper analysis.</p>"},{"location":"sims/signal-dashboard/#learning-objectives","title":"Learning Objectives","text":"<p>By using this MicroSim, you will:</p> <ol> <li>Design comprehensive signal systems covering success, kill, and leading indicators</li> <li>Evaluate project health objectively using threshold-based criteria</li> <li>Create actionable dashboards that drive persist/pivot/stop decisions</li> <li>Analyze trade-offs in threshold selection (too strict vs. too lenient)</li> <li>Apply weighted decision logic to prioritize critical signals</li> </ol>"},{"location":"sims/signal-dashboard/#connection-to-course-content","title":"Connection to Course Content","text":"<ul> <li>Chapter 4: Signal Design\u2014defining measurable success and failure criteria</li> <li>Chapter 5: Pivot Decisions\u2014when to change course vs. stay the course vs. quit</li> <li>Chapter 6: Implementation Monitoring\u2014tracking real-world AI system performance</li> <li>Chapter 7: Stakeholder Communication\u2014dashboards as shared truth</li> </ul>"},{"location":"sims/signal-dashboard/#worked-examples","title":"Worked Examples","text":""},{"location":"sims/signal-dashboard/#example-1-e-commerce-personalization-engine","title":"Example 1: E-commerce Personalization Engine","text":"<p>Success Signals: - Click-through rate: Current 18% (Green \u2265 15%, Yellow 10-14%, Red &lt; 10%) - Conversion rate: Current 6% (Green \u2265 5%, Yellow 3-4%, Red &lt; 3%) - Revenue lift: Current 12% (Green \u2265 10%, Yellow 5-9%, Red &lt; 5%)</p> <p>Kill Signals: - API latency: Current 200ms (Green &lt; 100ms, Yellow 100-500ms, Red \u2265 500ms) - Cost per conversion: Current $8 (Green &lt; $10, Yellow $10-20, Red \u2265 $20)</p> <p>Leading Indicators: - A/B test sample size: Current 8,000 users (Target 10,000) - Model retrain frequency: Current 2 days since last train (Target &lt; 7 days)</p> <p>Dashboard Recommendation: PERSIST - Confidence: 85% - All success signals green, kill signals yellow (manageable) - Leading indicators on track - Key action: Monitor latency, optimize if trends toward red</p>"},{"location":"sims/signal-dashboard/#example-2-healthcare-diagnostic-ai-warning-state","title":"Example 2: Healthcare Diagnostic AI (Warning State)","text":"<p>Success Signals: - Sensitivity: Current 92% (Green \u2265 95%, Yellow 90-94%, Red &lt; 90%) \u2192 YELLOW - Specificity: Current 88% (Green \u2265 90%, Yellow 85-89%, Red &lt; 85%) \u2192 YELLOW - Physician trust: Current 3.5/5 (Green \u2265 4/5, Yellow 3-3.9/5, Red &lt; 3/5) \u2192 YELLOW</p> <p>Kill Signals: - False negative rate: Current 8% (Green &lt; 2%, Yellow 2-5%, Red \u2265 5%) \u2192 RED - Patient complaints: Current 3/month (Green 0, Yellow 1-5, Red &gt; 5) \u2192 YELLOW</p> <p>Leading Indicators: - Dataset diversity: Current 60% (Target 80%) \u2192 Behind - Expert agreement: Current 75% (Target 90%) \u2192 Behind</p> <p>Dashboard Recommendation: PIVOT - Confidence: 72% - Critical issue: False negative rate in red (life-threatening) - Success signals underwhelming, all in yellow - Leading indicators suggest data quality problems - Recommended pivot: Expand dataset diversity, retrain with stricter false negative penalty, increase expert review</p>"},{"location":"sims/signal-dashboard/#example-3-fintech-fraud-detection-stop-signal","title":"Example 3: FinTech Fraud Detection (Stop Signal)","text":"<p>Success Signals: - Fraud catch rate: Current 70% (Green \u2265 85%, Yellow 75-84%, Red &lt; 75%) \u2192 RED - Precision: Current 65% (Green \u2265 80%, Yellow 70-79%, Red &lt; 70%) \u2192 RED</p> <p>Kill Signals: - False positive rate: Current 8% (Green &lt; 1%, Yellow 1-5%, Red \u2265 5%) \u2192 RED - Customer friction: Current 4.2/5 (Green &lt; 2/5, Yellow 2-3/5, Red \u2265 3/5) \u2192 RED - Compliance violations: Current 2 this quarter (Green 0, Yellow 0, Red &gt; 0) \u2192 RED</p> <p>Leading Indicators: - Data drift: Current HIGH (models trained on outdated patterns) - Transaction volume: Growing 20% monthly (models can't keep up)</p> <p>Dashboard Recommendation: STOP - Confidence: 95% - Multiple critical kill signals in red, including compliance violations (existential risk) - Success signals also failing - Leading indicators predict worsening performance - Recommended action: Stop deployment, redesign approach, address data drift before retry</p>"},{"location":"sims/signal-dashboard/#design-principles-for-good-signals","title":"Design Principles for Good Signals","text":""},{"location":"sims/signal-dashboard/#characteristics-of-effective-signals","title":"Characteristics of Effective Signals","text":"<p>Actionable: When the signal changes, you know what to do - Bad: \"User satisfaction\" (vague) - Good: \"% users rating 4+ stars on feature X\" (specific action possible)</p> <p>Measurable: Objective, quantifiable data - Bad: \"Team morale\" (subjective) - Good: \"Sprint velocity (story points/week)\" (objective)</p> <p>Timely: Updates frequently enough to enable decisions - Bad: Annual survey results (too slow) - Good: Daily active users (real-time feedback)</p> <p>Owned: Someone is responsible for the signal - Bad: Generic \"system health\" (no owner) - Good: \"API uptime (owned by infrastructure team)\"</p>"},{"location":"sims/signal-dashboard/#balanced-signal-portfolio","title":"Balanced Signal Portfolio","text":"<p>A good dashboard includes: - 3-5 success signals: Not too many or focus diffuses - 2-3 kill signals: Critical risks that could end the project - 2-4 leading indicators: Early warnings for proactive management - Mix of types: User metrics, business metrics, technical metrics</p>"},{"location":"sims/signal-dashboard/#threshold-calibration","title":"Threshold Calibration","text":"<p>Too strict: Everything is red, team becomes demoralized, dashboard ignored Too lenient: Everything is green, dashboard provides no value, surprises happen</p> <p>Right balance: - Green: Achievable but challenging (70-80% of time if performing well) - Yellow: Warning state that prompts investigation (15-25% of time) - Red: Genuine crisis requiring immediate action (&lt; 10% of time)</p> <p>Recalibrate thresholds quarterly based on actual performance.</p>"},{"location":"sims/signal-dashboard/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Vanity metrics: Tracking numbers that look good but don't predict success</li> <li>Example: Total users (instead of active users)</li> <li> <p>Example: Model complexity (instead of accuracy)</p> </li> <li> <p>Lagging indicators only: By the time you see the problem, it's too late</p> </li> <li> <p>Solution: Include leading indicators that predict future state</p> </li> <li> <p>No kill signals: Only tracking positives means no objective stop criteria</p> </li> <li> <p>Result: Sunk cost fallacy, projects drag on indefinitely</p> </li> <li> <p>Ignoring yellow signals: Waiting until red to act</p> </li> <li> <p>Result: Problems escalate, harder to fix</p> </li> <li> <p>Signal overload: Dashboards with 20+ signals become noise</p> </li> <li>Solution: Focus on 8-12 critical signals, archive the rest</li> </ol>"},{"location":"sims/signal-dashboard/#reflection-questions","title":"Reflection Questions","text":"<p>After designing your signal dashboard:</p> <ol> <li>Coverage: Do your signals cover user impact, business value, and technical health?</li> <li>Early warning: Could you predict failure 4+ weeks before it happens?</li> <li>Objectivity: Could two people look at this dashboard and reach the same decision?</li> <li>Actionability: For each red signal, do you know what corrective action to take?</li> <li>Stakeholder alignment: Would your team and leadership agree with these thresholds?</li> </ol>"},{"location":"sims/signal-dashboard/#assessment-applications","title":"Assessment Applications","text":"<p>This MicroSim supports: - Homework 4: Signal Design Assignment - Midterm Case Study: Given a scenario, design appropriate signals and thresholds - Final Project: Create and justify signal dashboard for capstone project - Peer Review: Critique classmates' signal selection and threshold choices - Group Exercise: Team debate on persist/pivot/stop decisions</p>"},{"location":"sims/signal-dashboard/spec/","title":"MicroSim: Signal Dashboard","text":""},{"location":"sims/signal-dashboard/spec/#concept-visualized","title":"Concept Visualized","text":"<ul> <li>Concept: Success/Kill/Leading Indicators for AI Project Monitoring</li> <li>Learning Goal: Students will design, configure, and interpret signal dashboards that drive persist/pivot/stop decisions based on threshold-based monitoring of project health indicators</li> <li>Difficulty: Advanced (graduate level)</li> <li>Bloom's Level: Evaluate/Create</li> </ul>"},{"location":"sims/signal-dashboard/spec/#controls-right-panel","title":"Controls (Right Panel)","text":"Control Type Range Default Effect Add Signal Button + Form - - Create new signal with name, type, thresholds Signal Name Text input 50 chars Empty Label for dashboard tile Signal Type Dropdown [Success, Kill, Leading] Success Category and color Current Value Slider 0-100 50 Real-time metric value Green Threshold Number input 0-100 70 Success zone (\u2265 this value) Yellow Threshold Number input 0-100 40 Warning zone Red Threshold Number input 0-100 0 Danger zone (&lt; yellow) Unit Text input 20 chars \"%\" Display unit (%, ms, $, etc.) Trend Period Dropdown [1 week, 1 month, 3 months, 6 months] 1 month Historical data window Update Frequency Dropdown [Real-time, Hourly, Daily, Weekly] Daily Refresh rate Alert Enabled Checkbox On/Off Off Notification when thresholds crossed Weight Slider 1-10 5 Importance in overall decision Remove Signal Button (per signal) - - Delete signal from dashboard Load Template Dropdown [E-commerce, Healthcare, FinTech, Custom] Custom Pre-configured signal sets Time Simulation Play/Pause - Paused Animate values changing over time Simulation Speed Slider 0.5x-5x 1x Playback speed for time series Export Dashboard Button - - Download config as JSON Import Dashboard File upload JSON - Load saved configuration"},{"location":"sims/signal-dashboard/spec/#visualization-left-panel","title":"Visualization (Left Panel)","text":""},{"location":"sims/signal-dashboard/spec/#dashboard-layout","title":"Dashboard Layout","text":"<ul> <li>Grid of tiles: 2-4 columns, responsive</li> <li>Each tile shows:</li> <li>Signal name (bold, top)</li> <li>Large gauge/dial visualization</li> <li>Current value (prominent number + unit)</li> <li>Threshold markers on gauge</li> <li>Color background (green/yellow/red zone)</li> <li>Mini trend line (sparkline showing recent history)</li> <li>Status icon (\u2713, \u26a0, \u2717)</li> <li>Last updated timestamp</li> </ul>"},{"location":"sims/signal-dashboard/spec/#gauge-styles-by-signal-type","title":"Gauge Styles by Signal Type","text":"<p>Success Signals (Green border): - Semi-circular gauge, 0-100 scale - Higher is better - Green zone (\u2265 green threshold) - Yellow zone (yellow to green threshold) - Red zone (&lt; yellow threshold) - Example: User satisfaction, model accuracy, engagement rate</p> <p>Kill Signals (Red border): - Semi-circular gauge, inverted color scheme - Lower is better - Red zone (\u2265 red threshold) \u2192 Project should stop - Yellow zone (yellow to red threshold) - Green zone (&lt; yellow threshold) - Example: Error rate, customer churn, cost overrun</p> <p>Leading Indicators (Blue border): - Linear progress bar - Predicts future success/kill signals - Directional arrow showing trend - Example: Data labeling progress, team velocity, user adoption rate</p>"},{"location":"sims/signal-dashboard/spec/#overall-decision-panel","title":"Overall Decision Panel","text":"<ul> <li>Top banner summarizing dashboard state:</li> <li>Persist: All critical signals green, majority positive</li> <li>Pivot: Mixed signals, some yellows/reds but recoverable</li> <li>Stop: Critical kill signals in red, low recovery probability</li> <li>Decision confidence: Percentage based on weighted signals</li> <li>Recommendation text: Human-readable guidance</li> <li>Key concerns: Bullet list of red/yellow signals needing attention</li> </ul>"},{"location":"sims/signal-dashboard/spec/#historical-trends-section","title":"Historical Trends Section","text":"<ul> <li>Time series line chart below gauge tiles</li> <li>Select any signal to expand full trend view</li> <li>Overlay multiple signals for correlation analysis</li> <li>Threshold lines visible on timeline</li> <li>Annotations for key events (pivots, launches, etc.)</li> <li>Date range selector for zoom</li> </ul>"},{"location":"sims/signal-dashboard/spec/#alert-feed","title":"Alert Feed","text":"<ul> <li>Sidebar showing recent threshold crossings</li> <li>\"Accuracy dropped below 85% (yellow threshold)\"</li> <li>Timestamp and severity icon</li> <li>Clickable to jump to relevant signal tile</li> </ul>"},{"location":"sims/signal-dashboard/spec/#interaction-flow","title":"Interaction Flow","text":"<ol> <li>Set up signals:</li> <li>Add 3-8 signals covering success, kill, and leading indicators</li> <li>Configure thresholds based on project requirements</li> <li> <p>Assign weights to critical signals</p> </li> <li> <p>Monitor dashboard:</p> </li> <li>View real-time or simulated values</li> <li>Watch gauges change color as thresholds crossed</li> <li> <p>Check overall decision recommendation</p> </li> <li> <p>Analyze trends:</p> </li> <li>Expand individual signals to see historical patterns</li> <li>Identify correlations (e.g., leading indicators predict success signals)</li> <li> <p>Spot early warnings before kill signals trigger</p> </li> <li> <p>Simulate scenarios:</p> </li> <li>Use time simulation to test \"what if\" situations</li> <li>Adjust thresholds to see impact on recommendations</li> <li> <p>Explore how different weighting affects decisions</p> </li> <li> <p>Export and share:</p> </li> <li>Save dashboard configuration for team use</li> <li>Generate reports for stakeholder updates</li> </ol>"},{"location":"sims/signal-dashboard/spec/#the-aha-moment","title":"The \"Aha\" Moment","text":"<p>Students realize that defining signals upfront prevents sunk cost fallacy\u2014when kill thresholds are clear and agreed upon, teams can objectively decide to stop failing projects rather than throwing good money after bad. They also see how leading indicators provide early warning, enabling pivots before irreversible failure. The weighted decision logic shows that not all signals are equal\u2014some are critical, others informative.</p>"},{"location":"sims/signal-dashboard/spec/#technical-notes","title":"Technical Notes","text":"<ul> <li>Canvas: Responsive grid, minimum 800px width, variable height</li> <li>Library: Chart.js for gauges and trends, custom gauge component</li> <li>Data Structure: Signal array with metadata (name, type, thresholds, values, history)</li> <li>Time Series: Array of {timestamp, value} pairs per signal</li> <li>Persistence: LocalStorage for dashboard config, IndexedDB for time series</li> <li>Mobile: Stacked tiles, swipe to switch signals, collapsible trend section</li> <li>Accessibility: High contrast mode, text alternatives for gauges, keyboard navigation</li> <li>Performance: Virtualized rendering for &gt;20 signals, debounced updates</li> </ul>"},{"location":"sims/signal-dashboard/spec/#assessment-integration","title":"Assessment Integration","text":"<p>This MicroSim prepares students for: - Quiz Question 1: \"What's the difference between success and leading indicators?\" - Quiz Question 2: \"Why are kill signals essential for project governance?\" - Quiz Question 3: \"How do thresholds convert metrics into actionable decisions?\" - Case Study: \"Analyze this signal dashboard and recommend persist/pivot/stop\" - Design Exercise: \"Create a signal dashboard for your capstone project\"</p>"},{"location":"sims/signal-dashboard/spec/#pre-configured-templates","title":"Pre-configured Templates","text":""},{"location":"sims/signal-dashboard/spec/#e-commerce-recommendation-system","title":"E-commerce Recommendation System","text":"<ul> <li>Success: Click-through rate (&gt;15% green), conversion rate (&gt;5% green), revenue lift (&gt;10% green)</li> <li>Kill: API latency (&gt;500ms red), model staleness (&gt;7 days red), cost per conversion (&gt;$20 red)</li> <li>Leading: Training data freshness, A/B test enrollment, user feedback score</li> </ul>"},{"location":"sims/signal-dashboard/spec/#healthcare-diagnostic-ai","title":"Healthcare Diagnostic AI","text":"<ul> <li>Success: Sensitivity (&gt;95% green), specificity (&gt;90% green), physician trust score (&gt;4/5 green)</li> <li>Kill: False negative rate (&gt;2% red), regulatory audit failures (&gt;0 red), patient complaints (&gt;5/month red)</li> <li>Leading: Dataset diversity, expert annotation agreement, deployment pipeline health</li> </ul>"},{"location":"sims/signal-dashboard/spec/#fintech-fraud-detection","title":"FinTech Fraud Detection","text":"<ul> <li>Success: Fraud catch rate (&gt;85% green), precision (&gt;80% green), processing speed (&lt;100ms green)</li> <li>Kill: False positive rate (&gt;5% red), customer friction score (&gt;3/5 red), compliance violations (&gt;0 red)</li> <li>Leading: Model retrain frequency, data drift detection, transaction volume growth</li> </ul>"},{"location":"sims/signal-dashboard/spec/#edge-cases","title":"Edge Cases","text":"<ul> <li>Conflicting signals: Success and kill both in red zones \u2192 Show \"Mixed Evidence, Manual Review Required\"</li> <li>No data: Gray gauge with \"Awaiting Data\" message</li> <li>Threshold overlap: Validate yellow between red and green, prevent invalid configs</li> <li>All signals green: Celebrate but warn against complacency, suggest stretch goals</li> <li>Rapid oscillation: Dampen updates if value crosses thresholds too frequently (noise vs. signal)</li> </ul>"},{"location":"sims/signal-dashboard/spec/#extension-opportunities","title":"Extension Opportunities","text":"<ul> <li>Forecasting: ML model predicts signal values 1-4 weeks ahead</li> <li>Anomaly detection: Alert when signal deviates from expected pattern</li> <li>Team collaboration: Comments and annotations on signals</li> <li>Integration APIs: Pull real-time data from Google Analytics, Datadog, etc.</li> <li>Automated reports: Weekly email summaries with dashboard snapshots</li> <li>Decision history: Log persist/pivot/stop decisions with rationale for retrospectives</li> </ul>"}]}